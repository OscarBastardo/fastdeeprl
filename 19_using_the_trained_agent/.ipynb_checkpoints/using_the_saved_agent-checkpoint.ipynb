{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8532d434-a069-4abd-b6fb-f08296a34678",
   "metadata": {},
   "source": [
    "# Using the saved agent\n",
    "\n",
    "<img src=\"images/restore/restore.png\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032e646-dc33-443d-a83e-a0a6d69c5220",
   "metadata": {},
   "source": [
    "## Step 1: Restoring the agent from the checkpoint\n",
    "\n",
    "1. **Algorithm trainer class**: Find it in the algorithm implementation (linked in the [`rllib` algorithms page](https://docs.ray.io/en/master/rllib-algorithms.html))\n",
    "2. Import the trainer class.\n",
    "3. Create an empty agent by initializing the trainer class. **Use the same configuration as the experiment**.\n",
    "4. Restore the agent from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c675ea4-37b2-47be-857c-f4e476782761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_18-04-33_808683_54502/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_18-04-33_808683_54502/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_18-04-33_808683_54502',\n",
       " 'metrics_export_port': 55980,\n",
       " 'gcs_address': '127.0.0.1:59277',\n",
       " 'address': '127.0.0.1:59277',\n",
       " 'node_id': 'b0df5d58a9eb5816e8e96c7ed45be21b46a0c8b0a2c4ed6fc1a54633'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc4e887-4063-4061-abbe-b7f802810faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 18:05:28,398\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-12-23 18:05:28,404\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "2022-12-23 18:05:28,405\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-12-23 18:05:28,405\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-12-23 18:05:38,458\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-12-23 18:05:39,367\tINFO trainable.py:127 -- Trainable.setup took 10.974 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-12-23 18:05:39,368\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-12-23 18:05:39,439\tINFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: ../18_saving_the_trained_agent/cartpole_v1/PPO/PPO_CartPole-v1_60b6a_00000_0_2022-12-23_17-36-50/checkpoint_000016/checkpoint-16\n",
      "2022-12-23 18:05:39,440\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 16, '_timesteps_total': 64000, '_time_total': 117.12063884735107, '_episodes_total': 437}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "agent = PPOTrainer(config={\"env\": \"CartPole-v1\",\n",
    "                           \"evaluation_interval\": 2,\n",
    "                           \"evaluation_num_episodes\": 20\n",
    "                           }\n",
    "                   )\n",
    "agent.restore(\"../18_saving_the_trained_agent/cartpole_v1/PPO/PPO_CartPole-v1_60b6a_00000_0_2022-12-23_17-36-50/checkpoint_000016/checkpoint-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af115db8-ef89-444a-af2d-c7d643527db1",
   "metadata": {},
   "source": [
    "## Step 2: Use the agent\n",
    "\n",
    "- Compute the action (according to the **trained policy**) using the `agent.compute_action()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bcc939-eda1-4d99-97b1-5dc1b4e4a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 18:05:39,451\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f1d9e-dfd3-49cb-92c0-20d379286dc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making videos of the agent in action\n",
    "\n",
    "- Wrap the `env` in the `gym.wrappers.RecordVideo` class.\n",
    "    - Supply the directory to write the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b97fba-2db0-482b-b213-3505ce98760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscar/miniconda3/envs/fastdeeprl/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/oscar/Projects/Python/fastdeeprl/19_using_the_trained_agent/ppo_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\"), \"ppo_video\")\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b0206-1676-4a7a-9a15-9b82f4825aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
