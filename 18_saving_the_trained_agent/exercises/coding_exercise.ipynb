{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d192556-0e8b-4ee7-aff7-f161a8c67a2c",
   "metadata": {},
   "source": [
    "# Save the trained robot\n",
    "\n",
    "Run the same experiment (using PPO in the `BipedalWalker-v3` environment) again. But this time, store the results in the relative path `bipedal_walker_v3` and save the agent at every evaluation.  \n",
    "\n",
    "Please stop any previous experiments (if they are still running) before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e03d10-671e-4b98-85ff-f8669ae3a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,618\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:52,094\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:52 (running for 00:00:06.22)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:52,806\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:53 (running for 00:00:07.23)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:54,960\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 480.375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -103.69946023898075\n",
      "  episode_reward_mean: -114.8549389703056\n",
      "  episode_reward_min: -124.98623578542097\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 8\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6258463859558105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012235086411237717\n",
      "          model: {}\n",
      "          policy_loss: -0.01618567667901516\n",
      "          total_loss: 623.4373779296875\n",
      "          vf_explained_var: -0.006057567894458771\n",
      "          vf_loss: 623.4510498046875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.214285714285715\n",
      "    ram_util_percent: 21.342857142857145\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09166318616052081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.331095222947837\n",
      "    mean_inference_ms: 0.552110333016132\n",
      "    mean_raw_obs_processing_ms: 0.07754194921162771\n",
      "  time_since_restore: 4.269518852233887\n",
      "  time_this_iter_s: 4.269518852233887\n",
      "  time_total_s: 4.269518852233887\n",
      "  timers:\n",
      "    learn_throughput: 1891.704\n",
      "    learn_time_ms: 2114.495\n",
      "    load_throughput: 34379540.984\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1395.154\n",
      "    sample_time_ms: 2867.068\n",
      "    update_time_ms: 1.819\n",
      "  timestamp: 1671817557\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:59 (running for 00:00:12.51)<br>Memory usage on this node: 6.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.26952</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-114.855</td><td style=\"text-align: right;\">            -103.699</td><td style=\"text-align: right;\">            -124.986</td><td style=\"text-align: right;\">           480.375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:04 (running for 00:00:17.52)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.26381</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -115.2</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           447.143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 566.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.31078195821432\n",
      "  episode_reward_mean: -115.73399798446101\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 20\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.721611022949219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020989786833524704\n",
      "          model: {}\n",
      "          policy_loss: -0.028235970064997673\n",
      "          total_loss: 209.29212951660156\n",
      "          vf_explained_var: -0.003048080950975418\n",
      "          vf_loss: 209.316162109375\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105358281828876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32962731473015977\n",
      "    mean_inference_ms: 0.5419344102446331\n",
      "    mean_raw_obs_processing_ms: 0.07582435689889211\n",
      "  time_since_restore: 12.235905647277832\n",
      "  time_this_iter_s: 3.9720983505249023\n",
      "  time_total_s: 12.235905647277832\n",
      "  timers:\n",
      "    learn_throughput: 2018.524\n",
      "    learn_time_ms: 1981.646\n",
      "    load_throughput: 31165107.121\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1085.207\n",
      "    sample_time_ms: 3685.931\n",
      "    update_time_ms: 1.53\n",
      "  timestamp: 1671817565\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:10 (running for 00:00:23.48)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         16.1943</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-116.154</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           570.462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 483.43589743589746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.31078195821432\n",
      "  episode_reward_mean: -115.04463941887997\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 39\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.342600345611572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014165914617478848\n",
      "          model: {}\n",
      "          policy_loss: -0.024110201746225357\n",
      "          total_loss: 699.7661743164062\n",
      "          vf_explained_var: -0.31886932253837585\n",
      "          vf_loss: 699.7860107421875\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.28\n",
      "    ram_util_percent: 20.9\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904261450275846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.327223236207678\n",
      "    mean_inference_ms: 0.5336058620450683\n",
      "    mean_raw_obs_processing_ms: 0.0751198450935775\n",
      "  time_since_restore: 20.16917085647583\n",
      "  time_this_iter_s: 3.9748873710632324\n",
      "  time_total_s: 20.16917085647583\n",
      "  timers:\n",
      "    learn_throughput: 2038.794\n",
      "    learn_time_ms: 1961.944\n",
      "    load_throughput: 28083722.799\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 1051.148\n",
      "    sample_time_ms: 3805.365\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1671817573\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:16 (running for 00:00:29.48)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.1692</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-115.045</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           483.436</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 533.1041666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -99.16475845614572\n",
      "  episode_reward_mean: -114.00788426825024\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 48\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.1425371170043945\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012110638432204723\n",
      "          model: {}\n",
      "          policy_loss: -0.027374638244509697\n",
      "          total_loss: 211.97628784179688\n",
      "          vf_explained_var: -0.3948518633842468\n",
      "          vf_loss: 212.00003051757812\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.86666666666667\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09023781161029043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3265487691918853\n",
      "    mean_inference_ms: 0.5313916133439255\n",
      "    mean_raw_obs_processing_ms: 0.07477130896606463\n",
      "  time_since_restore: 28.067641496658325\n",
      "  time_this_iter_s: 3.969888687133789\n",
      "  time_total_s: 28.067641496658325\n",
      "  timers:\n",
      "    learn_throughput: 2051.306\n",
      "    learn_time_ms: 1949.977\n",
      "    load_throughput: 29913528.273\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 1038.491\n",
      "    sample_time_ms: 3851.743\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671817580\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:22 (running for 00:00:35.42)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         28.0676</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-114.008</td><td style=\"text-align: right;\">            -99.1648</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           533.104</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:28 (running for 00:00:41.41)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         32.0483</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-114.134</td><td style=\"text-align: right;\">            -99.1648</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           575.528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 559.5081967213115\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -93.3107473169906\n",
      "  episode_reward_mean: -112.97977898940303\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 61\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.340518951416016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01272121723741293\n",
      "          model: {}\n",
      "          policy_loss: -0.03178858757019043\n",
      "          total_loss: 276.122314453125\n",
      "          vf_explained_var: -0.20616589486598969\n",
      "          vf_loss: 276.1502990722656\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.9\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0900462336512511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32602352169803495\n",
      "    mean_inference_ms: 0.5289747059336626\n",
      "    mean_raw_obs_processing_ms: 0.07437019078630032\n",
      "  time_since_restore: 35.98486828804016\n",
      "  time_this_iter_s: 3.9365463256835938\n",
      "  time_total_s: 35.98486828804016\n",
      "  timers:\n",
      "    learn_throughput: 2057.903\n",
      "    learn_time_ms: 1943.726\n",
      "    load_throughput: 30935247.695\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1030.921\n",
      "    sample_time_ms: 3880.024\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671817588\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:33 (running for 00:00:46.71)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         40.3081</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-112.976</td><td style=\"text-align: right;\">            -93.3107</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           583.328</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 590.1805555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -93.3107473169906\n",
      "  episode_reward_mean: -112.58386121206621\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.826778411865234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010720446705818176\n",
      "          model: {}\n",
      "          policy_loss: -0.03029380924999714\n",
      "          total_loss: 118.45962524414062\n",
      "          vf_explained_var: -0.14337755739688873\n",
      "          vf_loss: 118.48670959472656\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.93333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09001495503187394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259977529623879\n",
      "    mean_inference_ms: 0.5279252355991926\n",
      "    mean_raw_obs_processing_ms: 0.07420907827664361\n",
      "  time_since_restore: 44.264172077178955\n",
      "  time_this_iter_s: 3.9560396671295166\n",
      "  time_total_s: 44.264172077178955\n",
      "  timers:\n",
      "    learn_throughput: 2058.69\n",
      "    learn_time_ms: 1942.984\n",
      "    load_throughput: 28841698.47\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 990.724\n",
      "    sample_time_ms: 4037.45\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671817597\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:39 (running for 00:00:52.70)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         44.2642</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-112.584</td><td style=\"text-align: right;\">            -93.3107</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           590.181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:45 (running for 00:00:58.67)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         48.2402</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -112.28</td><td style=\"text-align: right;\">            -92.8687</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           617.473</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 648.0512820512821\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -90.51237783390522\n",
      "  episode_reward_mean: -111.7828716902311\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 78\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.84559965133667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010079648345708847\n",
      "          model: {}\n",
      "          policy_loss: -0.026890454813838005\n",
      "          total_loss: 72.36813354492188\n",
      "          vf_explained_var: -0.16960875689983368\n",
      "          vf_loss: 72.39200592041016\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.46\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08998623904449796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32595240769321676\n",
      "    mean_inference_ms: 0.527495865632787\n",
      "    mean_raw_obs_processing_ms: 0.07407096810865428\n",
      "  time_since_restore: 52.27972197532654\n",
      "  time_this_iter_s: 4.039555311203003\n",
      "  time_total_s: 52.27972197532654\n",
      "  timers:\n",
      "    learn_throughput: 2054.079\n",
      "    learn_time_ms: 1947.344\n",
      "    load_throughput: 28916263.357\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 995.233\n",
      "    sample_time_ms: 4019.158\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671817605\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:50 (running for 00:01:03.80)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         56.3326</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-111.289</td><td style=\"text-align: right;\">            -90.5124</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           650.337</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 683.4651162790698\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -82.5397765114735\n",
      "  episode_reward_mean: -110.52060518893026\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 86\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.670735836029053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009962263517081738\n",
      "          model: {}\n",
      "          policy_loss: -0.026227496564388275\n",
      "          total_loss: 7.788515090942383\n",
      "          vf_explained_var: -0.025038449093699455\n",
      "          vf_loss: 7.811753273010254\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08997941050632202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3260383715740126\n",
      "    mean_inference_ms: 0.5270875035551403\n",
      "    mean_raw_obs_processing_ms: 0.07392809940167157\n",
      "  time_since_restore: 60.36208891868591\n",
      "  time_this_iter_s: 4.02950382232666\n",
      "  time_total_s: 60.36208891868591\n",
      "  timers:\n",
      "    learn_throughput: 2049.261\n",
      "    learn_time_ms: 1951.923\n",
      "    load_throughput: 28455251.018\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 992.732\n",
      "    sample_time_ms: 4029.286\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671817613\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:55 (running for 00:01:08.84)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         60.3621</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-110.521</td><td style=\"text-align: right;\">            -82.5398</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           683.465</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:00 (running for 00:01:13.85)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         64.3173</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-109.985</td><td style=\"text-align: right;\">            -77.1683</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           697.258</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-01\n",
      "  done: false\n",
      "  episode_len_mean: 712.7446808510638\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -76.71557696290687\n",
      "  episode_reward_mean: -109.27937672076816\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 94\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.699250221252441\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010482219979166985\n",
      "          model: {}\n",
      "          policy_loss: -0.028056735172867775\n",
      "          total_loss: 144.89393615722656\n",
      "          vf_explained_var: -0.05199733003973961\n",
      "          vf_loss: 144.91885375976562\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.92\n",
      "    ram_util_percent: 21.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08994597713552237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259962971744285\n",
      "    mean_inference_ms: 0.526639405827228\n",
      "    mean_raw_obs_processing_ms: 0.07375674821276192\n",
      "  time_since_restore: 68.20912528038025\n",
      "  time_this_iter_s: 3.8917877674102783\n",
      "  time_total_s: 68.20912528038025\n",
      "  timers:\n",
      "    learn_throughput: 2049.472\n",
      "    learn_time_ms: 1951.722\n",
      "    load_throughput: 27003405.762\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 992.254\n",
      "    sample_time_ms: 4031.227\n",
      "    update_time_ms: 1.35\n",
      "  timestamp: 1671817621\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:06 (running for 00:01:19.63)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         72.0997</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-108.575</td><td style=\"text-align: right;\">            -73.4689</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           731.229</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-09\n",
      "  done: false\n",
      "  episode_len_mean: 757.5555555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -73.46885569225505\n",
      "  episode_reward_mean: -107.87515568687103\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6524786949157715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01055772602558136\n",
      "          model: {}\n",
      "          policy_loss: -0.020433904603123665\n",
      "          total_loss: 2.827256441116333\n",
      "          vf_explained_var: -0.11335103213787079\n",
      "          vf_loss: 2.8445234298706055\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.4\n",
      "    ram_util_percent: 21.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08991644429045768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259298430147222\n",
      "    mean_inference_ms: 0.526272945707899\n",
      "    mean_raw_obs_processing_ms: 0.07364049724696722\n",
      "  time_since_restore: 76.01431083679199\n",
      "  time_this_iter_s: 3.9145617485046387\n",
      "  time_total_s: 76.01431083679199\n",
      "  timers:\n",
      "    learn_throughput: 2051.69\n",
      "    learn_time_ms: 1949.612\n",
      "    load_throughput: 27064391.031\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 994.731\n",
      "    sample_time_ms: 4021.186\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1671817629\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:12 (running for 00:01:25.59)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         76.0143</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-107.875</td><td style=\"text-align: right;\">            -73.4689</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           757.556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-16\n",
      "  done: false\n",
      "  episode_len_mean: 796.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -66.33643454601743\n",
      "  episode_reward_mean: -106.2590584493743\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 104\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.63258171081543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007472531404346228\n",
      "          model: {}\n",
      "          policy_loss: -0.02253890037536621\n",
      "          total_loss: 66.96393585205078\n",
      "          vf_explained_var: 0.011790256947278976\n",
      "          vf_loss: 66.9842300415039\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.333333333333336\n",
      "    ram_util_percent: 21.483333333333334\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08982756920547182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3256879623858853\n",
      "    mean_inference_ms: 0.5248371639839405\n",
      "    mean_raw_obs_processing_ms: 0.07328906827995751\n",
      "  time_since_restore: 83.82912540435791\n",
      "  time_this_iter_s: 3.9347105026245117\n",
      "  time_total_s: 83.82912540435791\n",
      "  timers:\n",
      "    learn_throughput: 2076.352\n",
      "    learn_time_ms: 1926.456\n",
      "    load_throughput: 29335925.861\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1006.658\n",
      "    sample_time_ms: 3973.543\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671817636\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:18 (running for 00:01:31.42)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         83.8291</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-106.259</td><td style=\"text-align: right;\">            -66.3364</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">            796.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:23 (running for 00:01:36.43)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         87.7982</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-104.719</td><td style=\"text-align: right;\">            -63.9533</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">            809.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-24\n",
      "  done: false\n",
      "  episode_len_mean: 853.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -54.738792526859264\n",
      "  episode_reward_mean: -103.31683409865802\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 114\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.570556163787842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011751503683626652\n",
      "          model: {}\n",
      "          policy_loss: -0.01598619855940342\n",
      "          total_loss: 131.1350860595703\n",
      "          vf_explained_var: -0.06928105652332306\n",
      "          vf_loss: 131.1475372314453\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08957001720598605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.324797932973258\n",
      "    mean_inference_ms: 0.5222932823635901\n",
      "    mean_raw_obs_processing_ms: 0.07284877008509741\n",
      "  time_since_restore: 91.70651292800903\n",
      "  time_this_iter_s: 3.9082701206207275\n",
      "  time_total_s: 91.70651292800903\n",
      "  timers:\n",
      "    learn_throughput: 2079.373\n",
      "    learn_time_ms: 1923.656\n",
      "    load_throughput: 30229218.018\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1009.691\n",
      "    sample_time_ms: 3961.607\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671817644\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:28 (running for 00:01:42.24)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         95.6095</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-102.142</td><td style=\"text-align: right;\">            -54.7388</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            868.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 898.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -54.738792526859264\n",
      "  episode_reward_mean: -101.13744371609971\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 118\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.51967191696167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010870070196688175\n",
      "          model: {}\n",
      "          policy_loss: -0.018528630957007408\n",
      "          total_loss: 0.8644792437553406\n",
      "          vf_explained_var: -0.0007182357367128134\n",
      "          vf_loss: 0.8797469735145569\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.88333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08952775586723945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32463043660276375\n",
      "    mean_inference_ms: 0.5216518547757183\n",
      "    mean_raw_obs_processing_ms: 0.07266434132603063\n",
      "  time_since_restore: 99.49541115760803\n",
      "  time_this_iter_s: 3.885935068130493\n",
      "  time_total_s: 99.49541115760803\n",
      "  timers:\n",
      "    learn_throughput: 2088.43\n",
      "    learn_time_ms: 1915.314\n",
      "    load_throughput: 33281523.507\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1015.565\n",
      "    sample_time_ms: 3938.693\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1671817652\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:34 (running for 00:01:48.15)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         99.4954</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-101.137</td><td style=\"text-align: right;\">            -54.7388</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            898.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 913.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -97.66475634889761\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 127\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6290669441223145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009882863610982895\n",
      "          model: {}\n",
      "          policy_loss: -0.024132315069437027\n",
      "          total_loss: 137.83746337890625\n",
      "          vf_explained_var: -0.16362427175045013\n",
      "          vf_loss: 137.858642578125\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.44\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08942282676685015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.324236563520408\n",
      "    mean_inference_ms: 0.5206684116487595\n",
      "    mean_raw_obs_processing_ms: 0.07233281319634166\n",
      "  time_since_restore: 107.3671875\n",
      "  time_this_iter_s: 3.9565773010253906\n",
      "  time_total_s: 107.3671875\n",
      "  timers:\n",
      "    learn_throughput: 2087.898\n",
      "    learn_time_ms: 1915.802\n",
      "    load_throughput: 35454809.806\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1016.84\n",
      "    sample_time_ms: 3933.754\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671817660\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:40 (running for 00:01:54.06)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         107.367</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-97.6648</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            913.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:46 (running for 00:01:59.97)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         111.265</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-97.4661</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            944.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 959.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -96.38850418733541\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 132\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.67781925201416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009757702238857746\n",
      "          model: {}\n",
      "          policy_loss: -0.026699598878622055\n",
      "          total_loss: 71.39501953125\n",
      "          vf_explained_var: -0.3221835196018219\n",
      "          vf_loss: 71.4188003540039\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08937594270584129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32405772704503827\n",
      "    mean_inference_ms: 0.5201632334391408\n",
      "    mean_raw_obs_processing_ms: 0.07210594386749487\n",
      "  time_since_restore: 115.14185738563538\n",
      "  time_this_iter_s: 3.877049684524536\n",
      "  time_total_s: 115.14185738563538\n",
      "  timers:\n",
      "    learn_throughput: 2086.093\n",
      "    learn_time_ms: 1917.46\n",
      "    load_throughput: 35454809.806\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1018.064\n",
      "    sample_time_ms: 3929.027\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671817668\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:52 (running for 00:02:05.82)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         119.076</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-95.6204</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">             990.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 1050.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -93.48034450222569\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 138\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.588151931762695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007226522546261549\n",
      "          model: {}\n",
      "          policy_loss: -0.023820700123906136\n",
      "          total_loss: 3.5000650882720947\n",
      "          vf_explained_var: -0.04026513174176216\n",
      "          vf_loss: 3.5217180252075195\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.50000000000001\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08928981283802045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3236918162188872\n",
      "    mean_inference_ms: 0.5197541715239529\n",
      "    mean_raw_obs_processing_ms: 0.07186835872635619\n",
      "  time_since_restore: 123.01969838142395\n",
      "  time_this_iter_s: 3.943251132965088\n",
      "  time_total_s: 123.01969838142395\n",
      "  timers:\n",
      "    learn_throughput: 2085.252\n",
      "    learn_time_ms: 1918.233\n",
      "    load_throughput: 33156553.36\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1016.128\n",
      "    sample_time_ms: 3936.512\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1671817676\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:58 (running for 00:02:11.78)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">          123.02</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-93.4803</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1050.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 1066.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -91.50706391165427\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 143\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.71390438079834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01629435084760189\n",
      "          model: {}\n",
      "          policy_loss: -0.03438657894730568\n",
      "          total_loss: 3.7106854915618896\n",
      "          vf_explained_var: -0.3511037528514862\n",
      "          vf_loss: 3.7401835918426514\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08923912781463027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32346552319924354\n",
      "    mean_inference_ms: 0.5194010221102583\n",
      "    mean_raw_obs_processing_ms: 0.07169286151383675\n",
      "  time_since_restore: 130.8518283367157\n",
      "  time_this_iter_s: 3.8995859622955322\n",
      "  time_total_s: 130.8518283367157\n",
      "  timers:\n",
      "    learn_throughput: 2084.7\n",
      "    learn_time_ms: 1918.741\n",
      "    load_throughput: 33195916.106\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1016.879\n",
      "    sample_time_ms: 3933.606\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1671817684\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:04 (running for 00:02:17.68)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         130.852</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-91.5071</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1066.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:10 (running for 00:02:23.59)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         134.775</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-90.5504</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1097.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 1121.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -89.6620458623347\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 148\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.524649143218994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007157592568546534\n",
      "          model: {}\n",
      "          policy_loss: -0.024165576323866844\n",
      "          total_loss: 36.132713317871094\n",
      "          vf_explained_var: 0.06165395677089691\n",
      "          vf_loss: 36.154727935791016\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.419999999999995\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08919230764789511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3232424652685692\n",
      "    mean_inference_ms: 0.5190583501604972\n",
      "    mean_raw_obs_processing_ms: 0.07152345890939914\n",
      "  time_since_restore: 138.71032857894897\n",
      "  time_this_iter_s: 3.935013771057129\n",
      "  time_total_s: 138.71032857894897\n",
      "  timers:\n",
      "    learn_throughput: 2084.489\n",
      "    learn_time_ms: 1918.936\n",
      "    load_throughput: 30778235.186\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1015.466\n",
      "    sample_time_ms: 3939.078\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1671817692\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:16 (running for 00:02:29.52)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         142.671</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-87.5761</td><td style=\"text-align: right;\">            -37.5283</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1122.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 1106.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.52829230689759\n",
      "  episode_reward_mean: -86.90324109318887\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 162\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.785221576690674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014871382154524326\n",
      "          model: {}\n",
      "          policy_loss: -0.03858889639377594\n",
      "          total_loss: 173.766357421875\n",
      "          vf_explained_var: -0.44147369265556335\n",
      "          vf_loss: 173.8004913330078\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.43333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08908871398688774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32263050751480454\n",
      "    mean_inference_ms: 0.5182105222694524\n",
      "    mean_raw_obs_processing_ms: 0.07113127237885306\n",
      "  time_since_restore: 146.62599563598633\n",
      "  time_this_iter_s: 3.954572916030884\n",
      "  time_total_s: 146.62599563598633\n",
      "  timers:\n",
      "    learn_throughput: 2083.495\n",
      "    learn_time_ms: 1919.851\n",
      "    load_throughput: 30834802.426\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1014.697\n",
      "    sample_time_ms: 3942.065\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1671817700\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:22 (running for 00:02:35.49)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         146.626</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-86.9032</td><td style=\"text-align: right;\">            -37.5283</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1106.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 1136.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.75034602557369\n",
      "  episode_reward_mean: -85.76247162809035\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.453378677368164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009026079438626766\n",
      "          model: {}\n",
      "          policy_loss: -0.02339906617999077\n",
      "          total_loss: 38.5586051940918\n",
      "          vf_explained_var: -0.2259003221988678\n",
      "          vf_loss: 38.57929611206055\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08900630550338098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32219427990818816\n",
      "    mean_inference_ms: 0.5177766772928224\n",
      "    mean_raw_obs_processing_ms: 0.07092894870937845\n",
      "  time_since_restore: 154.49257493019104\n",
      "  time_this_iter_s: 3.918593645095825\n",
      "  time_total_s: 154.49257493019104\n",
      "  timers:\n",
      "    learn_throughput: 2084.948\n",
      "    learn_time_ms: 1918.513\n",
      "    load_throughput: 29157483.49\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 1011.88\n",
      "    sample_time_ms: 3953.037\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1671817707\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:27 (running for 00:02:41.39)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         154.493</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-85.7625</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">              1136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:33 (running for 00:02:47.31)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         158.398</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-84.5074</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1151.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 1167.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.75034602557369\n",
      "  episode_reward_mean: -82.82083281104674\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 174\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.46420955657959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007359570357948542\n",
      "          model: {}\n",
      "          policy_loss: -0.02395291067659855\n",
      "          total_loss: 35.11384963989258\n",
      "          vf_explained_var: -0.09342267364263535\n",
      "          vf_loss: 35.13559341430664\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.51666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08892590505601758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32175675528393244\n",
      "    mean_inference_ms: 0.5173427051614724\n",
      "    mean_raw_obs_processing_ms: 0.07072740040323032\n",
      "  time_since_restore: 162.32040929794312\n",
      "  time_this_iter_s: 3.9223368167877197\n",
      "  time_total_s: 162.32040929794312\n",
      "  timers:\n",
      "    learn_throughput: 2081.768\n",
      "    learn_time_ms: 1921.444\n",
      "    load_throughput: 30891577.978\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1014.081\n",
      "    sample_time_ms: 3944.459\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1671817715\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:39 (running for 00:02:53.18)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         166.227</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-82.2369</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1145.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-43\n",
      "  done: false\n",
      "  episode_len_mean: 1176.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.765560157990087\n",
      "  episode_reward_mean: -80.81851151701557\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 181\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.274076461791992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014437778852880001\n",
      "          model: {}\n",
      "          policy_loss: -0.03310661390423775\n",
      "          total_loss: 5.485393047332764\n",
      "          vf_explained_var: -0.14664390683174133\n",
      "          vf_loss: 5.514168739318848\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.42\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08883766908698117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32121919434773416\n",
      "    mean_inference_ms: 0.5167200889090546\n",
      "    mean_raw_obs_processing_ms: 0.0705433666350596\n",
      "  time_since_restore: 170.19679284095764\n",
      "  time_this_iter_s: 3.9697389602661133\n",
      "  time_total_s: 170.19679284095764\n",
      "  timers:\n",
      "    learn_throughput: 2084.631\n",
      "    learn_time_ms: 1918.805\n",
      "    load_throughput: 30800837.158\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1012.047\n",
      "    sample_time_ms: 3952.387\n",
      "    update_time_ms: 1.421\n",
      "  timestamp: 1671817723\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:45 (running for 00:02:59.17)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         170.197</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-80.8185</td><td style=\"text-align: right;\">            -27.7656</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1176.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 1191.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.091330872762892\n",
      "  episode_reward_mean: -77.73011896537314\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 186\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.159714221954346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012011077255010605\n",
      "          model: {}\n",
      "          policy_loss: -0.02221444621682167\n",
      "          total_loss: 1.5564600229263306\n",
      "          vf_explained_var: -0.041677411645650864\n",
      "          vf_loss: 1.575071096420288\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08876312931674915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3207692774913312\n",
      "    mean_inference_ms: 0.5163245799791509\n",
      "    mean_raw_obs_processing_ms: 0.07040139326959698\n",
      "  time_since_restore: 178.09393501281738\n",
      "  time_this_iter_s: 3.946103096008301\n",
      "  time_total_s: 178.09393501281738\n",
      "  timers:\n",
      "    learn_throughput: 2082.499\n",
      "    learn_time_ms: 1920.769\n",
      "    load_throughput: 33354306.163\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.508\n",
      "    sample_time_ms: 3954.492\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671817731\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:51 (running for 00:03:05.10)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         178.094</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-77.7301</td><td style=\"text-align: right;\">            -23.0913</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1191.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:57 (running for 00:03:11.00)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         181.985</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-75.8421</td><td style=\"text-align: right;\">            -23.0913</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1206.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 1221.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -19.62076479933533\n",
      "  episode_reward_mean: -74.33538317307747\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 191\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.02078914642334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015727244317531586\n",
      "          model: {}\n",
      "          policy_loss: -0.023728204891085625\n",
      "          total_loss: 1.9430346488952637\n",
      "          vf_explained_var: 0.02868264727294445\n",
      "          vf_loss: 1.9620448350906372\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08870278322258848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203544245783131\n",
      "    mean_inference_ms: 0.5159491320132854\n",
      "    mean_raw_obs_processing_ms: 0.07027817685917336\n",
      "  time_since_restore: 185.90224814414978\n",
      "  time_this_iter_s: 3.917337417602539\n",
      "  time_total_s: 185.90224814414978\n",
      "  timers:\n",
      "    learn_throughput: 2083.988\n",
      "    learn_time_ms: 1919.397\n",
      "    load_throughput: 31312459.873\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1013.576\n",
      "    sample_time_ms: 3946.425\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1671817739\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:03 (running for 00:03:16.90)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         189.849</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-71.8137</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1236.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 1210.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.688280790192389\n",
      "  episode_reward_mean: -70.97316439791015\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 198\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.046761512756348\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01362482737749815\n",
      "          model: {}\n",
      "          policy_loss: -0.03094792552292347\n",
      "          total_loss: 76.05774688720703\n",
      "          vf_explained_var: -0.08447451144456863\n",
      "          vf_loss: 76.08460998535156\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.68\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08864116376106114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31985836957520053\n",
      "    mean_inference_ms: 0.5155530757627463\n",
      "    mean_raw_obs_processing_ms: 0.07013618998221462\n",
      "  time_since_restore: 193.86620354652405\n",
      "  time_this_iter_s: 4.017568349838257\n",
      "  time_total_s: 193.86620354652405\n",
      "  timers:\n",
      "    learn_throughput: 2082.823\n",
      "    learn_time_ms: 1920.47\n",
      "    load_throughput: 33255135.778\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.872\n",
      "    sample_time_ms: 3953.069\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671817747\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:08 (running for 00:03:21.94)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         193.866</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-70.9732</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1210.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:14 (running for 00:03:27.86)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         197.775</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-69.7793</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1210.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 1180.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.427794498312586\n",
      "  episode_reward_mean: -69.06477168415154\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 205\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.995788097381592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014605144038796425\n",
      "          model: {}\n",
      "          policy_loss: -0.0337463915348053\n",
      "          total_loss: 212.52139282226562\n",
      "          vf_explained_var: -0.47533535957336426\n",
      "          vf_loss: 212.55075073242188\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.839999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0885989064624983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31943607316315875\n",
      "    mean_inference_ms: 0.515275465843376\n",
      "    mean_raw_obs_processing_ms: 0.07002861092930286\n",
      "  time_since_restore: 201.66234588623047\n",
      "  time_this_iter_s: 3.887784481048584\n",
      "  time_total_s: 201.66234588623047\n",
      "  timers:\n",
      "    learn_throughput: 2083.689\n",
      "    learn_time_ms: 1919.672\n",
      "    load_throughput: 33255135.778\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.587\n",
      "    sample_time_ms: 3954.184\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671817755\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:20 (running for 00:03:33.75)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         205.594</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-64.7666</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1225.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 1239.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.34062835792951\n",
      "  episode_reward_mean: -63.02381362011634\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 211\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.003353595733643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012453736737370491\n",
      "          model: {}\n",
      "          policy_loss: -0.025424635037779808\n",
      "          total_loss: 2.5029385089874268\n",
      "          vf_explained_var: -0.07964774966239929\n",
      "          vf_loss: 2.524627208709717\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.449999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08857242088906679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31912124674758896\n",
      "    mean_inference_ms: 0.5149969372410405\n",
      "    mean_raw_obs_processing_ms: 0.06994003873600287\n",
      "  time_since_restore: 209.48947954177856\n",
      "  time_this_iter_s: 3.894988536834717\n",
      "  time_total_s: 209.48947954177856\n",
      "  timers:\n",
      "    learn_throughput: 2081.47\n",
      "    learn_time_ms: 1921.719\n",
      "    load_throughput: 33261728.787\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1013.643\n",
      "    sample_time_ms: 3946.161\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671817763\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:26 (running for 00:03:39.64)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         209.489</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-63.0238</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 1239.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.34062835792951\n",
      "  episode_reward_mean: -60.58491780457339\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 215\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.787299633026123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01410067267715931\n",
      "          model: {}\n",
      "          policy_loss: -0.030919622629880905\n",
      "          total_loss: 1.76252019405365\n",
      "          vf_explained_var: 0.20454537868499756\n",
      "          vf_loss: 1.7892097234725952\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08855204311970657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3188956855439976\n",
      "    mean_inference_ms: 0.5148546949569232\n",
      "    mean_raw_obs_processing_ms: 0.06988234613034132\n",
      "  time_since_restore: 217.2959213256836\n",
      "  time_this_iter_s: 3.9238719940185547\n",
      "  time_total_s: 217.2959213256836\n",
      "  timers:\n",
      "    learn_throughput: 2081.736\n",
      "    learn_time_ms: 1921.473\n",
      "    load_throughput: 31920121.766\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1015.232\n",
      "    sample_time_ms: 3939.988\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671817771\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:32 (running for 00:03:45.52)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         217.296</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-60.5849</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:38 (running for 00:03:51.49)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          221.28</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-57.4403</td><td style=\"text-align: right;\">             26.1389</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 1239.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.138924529629477\n",
      "  episode_reward_mean: -55.71052811489122\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 222\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.605030059814453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007656828034669161\n",
      "          model: {}\n",
      "          policy_loss: -0.017413582652807236\n",
      "          total_loss: 12.985599517822266\n",
      "          vf_explained_var: 0.1860601156949997\n",
      "          vf_loss: 13.000716209411621\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.16\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08852847219623758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31858362987500405\n",
      "    mean_inference_ms: 0.5146780290042691\n",
      "    mean_raw_obs_processing_ms: 0.0698057813163448\n",
      "  time_since_restore: 225.16500544548035\n",
      "  time_this_iter_s: 3.8851726055145264\n",
      "  time_total_s: 225.16500544548035\n",
      "  timers:\n",
      "    learn_throughput: 2082.267\n",
      "    learn_time_ms: 1920.983\n",
      "    load_throughput: 31871610.942\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 1013.897\n",
      "    sample_time_ms: 3945.173\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671817778\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:43 (running for 00:03:57.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         229.064</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-53.2495</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1254.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 1254.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -51.18141115083986\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 227\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.40944242477417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00880446471273899\n",
      "          model: {}\n",
      "          policy_loss: -0.024036606773734093\n",
      "          total_loss: 47.472225189208984\n",
      "          vf_explained_var: 0.09751544147729874\n",
      "          vf_loss: 47.49362564086914\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08851333151689315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3183620317009693\n",
      "    mean_inference_ms: 0.5145564072718624\n",
      "    mean_raw_obs_processing_ms: 0.06975176211422006\n",
      "  time_since_restore: 233.02151489257812\n",
      "  time_this_iter_s: 3.957714796066284\n",
      "  time_total_s: 233.02151489257812\n",
      "  timers:\n",
      "    learn_throughput: 2082.039\n",
      "    learn_time_ms: 1921.194\n",
      "    load_throughput: 31213425.116\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1016.384\n",
      "    sample_time_ms: 3935.522\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1671817786\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:49 (running for 00:04:03.29)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         233.022</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">-51.1814</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1254.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-54\n",
      "  done: false\n",
      "  episode_len_mean: 1253.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -44.46430689765319\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 234\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.540538787841797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015991752967238426\n",
      "          model: {}\n",
      "          policy_loss: -0.02988414466381073\n",
      "          total_loss: 4.635056018829346\n",
      "          vf_explained_var: 0.07719099521636963\n",
      "          vf_loss: 4.660142421722412\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.016666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884995270456065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31807091755187616\n",
      "    mean_inference_ms: 0.5144658583548604\n",
      "    mean_raw_obs_processing_ms: 0.06969026710827342\n",
      "  time_since_restore: 240.8767638206482\n",
      "  time_this_iter_s: 3.9263012409210205\n",
      "  time_total_s: 240.8767638206482\n",
      "  timers:\n",
      "    learn_throughput: 2081.789\n",
      "    learn_time_ms: 1921.424\n",
      "    load_throughput: 31236671.011\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1014.999\n",
      "    sample_time_ms: 3940.891\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1671817794\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:55 (running for 00:04:09.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         240.877</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-44.4643</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1253.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:01 (running for 00:04:15.13)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         244.802</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">-42.6506</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1253.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 1253.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -40.60776168155492\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 238\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.174491882324219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015352150425314903\n",
      "          model: {}\n",
      "          policy_loss: -0.029460906982421875\n",
      "          total_loss: 2.613389492034912\n",
      "          vf_explained_var: 0.3502749800682068\n",
      "          vf_loss: 2.638245105743408\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.56666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849483272913952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31792097871504493\n",
      "    mean_inference_ms: 0.5144068310671248\n",
      "    mean_raw_obs_processing_ms: 0.06965775927938185\n",
      "  time_since_restore: 248.67386150360107\n",
      "  time_this_iter_s: 3.8721771240234375\n",
      "  time_total_s: 248.67386150360107\n",
      "  timers:\n",
      "    learn_throughput: 2082.564\n",
      "    learn_time_ms: 1920.709\n",
      "    load_throughput: 31283266.828\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1015.623\n",
      "    sample_time_ms: 3938.471\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1671817802\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:07 (running for 00:04:20.94)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         252.571</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">-36.0504</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1268.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-10\n",
      "  done: false\n",
      "  episode_len_mean: 1259.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 59.349831344597646\n",
      "  episode_reward_mean: -35.56800696829644\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 244\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.148664474487305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014093538746237755\n",
      "          model: {}\n",
      "          policy_loss: -0.02851599082350731\n",
      "          total_loss: 103.91250610351562\n",
      "          vf_explained_var: -0.198480486869812\n",
      "          vf_loss: 103.93679809570312\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.81666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848786639045167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31769911668767564\n",
      "    mean_inference_ms: 0.5143226511169634\n",
      "    mean_raw_obs_processing_ms: 0.06961498084197597\n",
      "  time_since_restore: 256.45938372612\n",
      "  time_this_iter_s: 3.888091802597046\n",
      "  time_total_s: 256.45938372612\n",
      "  timers:\n",
      "    learn_throughput: 2084.231\n",
      "    learn_time_ms: 1919.173\n",
      "    load_throughput: 30620945.428\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1016.217\n",
      "    sample_time_ms: 3936.166\n",
      "    update_time_ms: 1.383\n",
      "  timestamp: 1671817810\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:13 (running for 00:04:26.85)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         256.459</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -35.568</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1259.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 1266.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 59.349831344597646\n",
      "  episode_reward_mean: -29.967131842990476\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 249\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.075194835662842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012313319370150566\n",
      "          model: {}\n",
      "          policy_loss: -0.02818615362048149\n",
      "          total_loss: 3.5164294242858887\n",
      "          vf_explained_var: 0.21115973591804504\n",
      "          vf_loss: 3.540921688079834\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.96\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848347994318456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31751964413151584\n",
      "    mean_inference_ms: 0.5142606672260608\n",
      "    mean_raw_obs_processing_ms: 0.06958266686539079\n",
      "  time_since_restore: 264.33310055732727\n",
      "  time_this_iter_s: 3.9442903995513916\n",
      "  time_total_s: 264.33310055732727\n",
      "  timers:\n",
      "    learn_throughput: 2085.22\n",
      "    learn_time_ms: 1918.263\n",
      "    load_throughput: 30676935.454\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1015.771\n",
      "    sample_time_ms: 3937.896\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671817818\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:19 (running for 00:04:32.77)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         264.333</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-29.9671</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1266.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:25 (running for 00:04:38.73)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         268.245</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-26.3631</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1281.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 1296.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 63.43249696450801\n",
      "  episode_reward_mean: -23.34879778948653\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 255\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9810478687286377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011981776915490627\n",
      "          model: {}\n",
      "          policy_loss: -0.03165898099541664\n",
      "          total_loss: 69.1121826171875\n",
      "          vf_explained_var: -0.1586046814918518\n",
      "          vf_loss: 69.14025115966797\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847466528183996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172810237653776\n",
      "    mean_inference_ms: 0.514202408463161\n",
      "    mean_raw_obs_processing_ms: 0.06953440993458407\n",
      "  time_since_restore: 272.14142894744873\n",
      "  time_this_iter_s: 3.896162748336792\n",
      "  time_total_s: 272.14142894744873\n",
      "  timers:\n",
      "    learn_throughput: 2084.582\n",
      "    learn_time_ms: 1918.85\n",
      "    load_throughput: 31242487.896\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1017.065\n",
      "    sample_time_ms: 3932.883\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1671817826\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:31 (running for 00:04:44.56)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         276.072</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-18.4308</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1342.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 1357.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 63.43249696450801\n",
      "  episode_reward_mean: -15.801512978616811\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 261\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9935495853424072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017227336764335632\n",
      "          model: {}\n",
      "          policy_loss: -0.03877100348472595\n",
      "          total_loss: 5.365960121154785\n",
      "          vf_explained_var: 0.013474843464791775\n",
      "          vf_loss: 5.399562835693359\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.199999999999996\n",
      "    ram_util_percent: 21.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845637825367775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169992776024903\n",
      "    mean_inference_ms: 0.5141770519209721\n",
      "    mean_raw_obs_processing_ms: 0.06946730010273176\n",
      "  time_since_restore: 280.0361702442169\n",
      "  time_this_iter_s: 3.9646129608154297\n",
      "  time_total_s: 280.0361702442169\n",
      "  timers:\n",
      "    learn_throughput: 2087.077\n",
      "    learn_time_ms: 1916.556\n",
      "    load_throughput: 31225043.737\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1016.139\n",
      "    sample_time_ms: 3936.471\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1671817834\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:36 (running for 00:04:49.58)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         280.036</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">-15.8015</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1357.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 1358.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 64.78455726691469\n",
      "  episode_reward_mean: -9.301243388379138\n",
      "  episode_reward_min: -130.54265340423893\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 268\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9229013919830322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01375757809728384\n",
      "          model: {}\n",
      "          policy_loss: -0.0276393610984087\n",
      "          total_loss: 6.5091071128845215\n",
      "          vf_explained_var: 0.14577075839042664\n",
      "          vf_loss: 6.532619476318359\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.833333333333336\n",
      "    ram_util_percent: 21.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845602328910795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3167876133884678\n",
      "    mean_inference_ms: 0.5140535596516571\n",
      "    mean_raw_obs_processing_ms: 0.06941835891992677\n",
      "  time_since_restore: 287.8343183994293\n",
      "  time_this_iter_s: 3.873603105545044\n",
      "  time_total_s: 287.8343183994293\n",
      "  timers:\n",
      "    learn_throughput: 2086.091\n",
      "    learn_time_ms: 1917.462\n",
      "    load_throughput: 30914346.785\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1015.929\n",
      "    sample_time_ms: 3937.281\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671817841\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:41 (running for 00:04:55.38)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         287.834</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-9.30124</td><td style=\"text-align: right;\">             64.7846</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1358.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:46 (running for 00:05:00.40)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         291.816</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">-7.54279</td><td style=\"text-align: right;\">             65.0833</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1328.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-49\n",
      "  done: false\n",
      "  episode_len_mean: 1343.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 65.08328556086369\n",
      "  episode_reward_mean: -5.204492587578983\n",
      "  episode_reward_min: -130.54265340423893\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 275\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.96517276763916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014771141111850739\n",
      "          model: {}\n",
      "          policy_loss: -0.025464273989200592\n",
      "          total_loss: 4.068432807922363\n",
      "          vf_explained_var: 0.14694848656654358\n",
      "          vf_loss: 4.089465618133545\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.160000000000004\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884485542489842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31653920934179924\n",
      "    mean_inference_ms: 0.514003758603512\n",
      "    mean_raw_obs_processing_ms: 0.06936604577335867\n",
      "  time_since_restore: 295.7117919921875\n",
      "  time_this_iter_s: 3.895420789718628\n",
      "  time_total_s: 295.7117919921875\n",
      "  timers:\n",
      "    learn_throughput: 2085.958\n",
      "    learn_time_ms: 1917.584\n",
      "    load_throughput: 32787211.257\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 1014.037\n",
      "    sample_time_ms: 3944.631\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671817849\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:52 (running for 00:05:06.30)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         295.712</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-5.20449</td><td style=\"text-align: right;\">             65.0833</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1343.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 1350.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 0.6298960534320335\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 282\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9469947814941406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015158119611442089\n",
      "          model: {}\n",
      "          policy_loss: -0.04597778245806694\n",
      "          total_loss: 4.7865309715271\n",
      "          vf_explained_var: -0.027983490377664566\n",
      "          vf_loss: 4.827960968017578\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.38\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08844406642948469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163132207600618\n",
      "    mean_inference_ms: 0.5139360663849644\n",
      "    mean_raw_obs_processing_ms: 0.06932037032196385\n",
      "  time_since_restore: 303.5166642665863\n",
      "  time_this_iter_s: 3.8740456104278564\n",
      "  time_total_s: 303.5166642665863\n",
      "  timers:\n",
      "    learn_throughput: 2085.055\n",
      "    learn_time_ms: 1918.415\n",
      "    load_throughput: 34945253.072\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1015.854\n",
      "    sample_time_ms: 3937.575\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671817857\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:58 (running for 00:05:12.18)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         303.517</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">0.629896</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1350.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:04 (running for 00:05:18.09)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         307.438</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> 1.32542</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1335.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 1320.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 3.11810677126801\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 289\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.762481927871704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011617136187851429\n",
      "          model: {}\n",
      "          policy_loss: -0.02532050758600235\n",
      "          total_loss: 37.34484100341797\n",
      "          vf_explained_var: -0.009955924935638905\n",
      "          vf_loss: 37.366676330566406\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.7\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08843234273027731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31605917471572353\n",
      "    mean_inference_ms: 0.5138717321196407\n",
      "    mean_raw_obs_processing_ms: 0.06927459917992734\n",
      "  time_since_restore: 311.3688507080078\n",
      "  time_this_iter_s: 3.9305403232574463\n",
      "  time_total_s: 311.3688507080078\n",
      "  timers:\n",
      "    learn_throughput: 2085.981\n",
      "    learn_time_ms: 1917.563\n",
      "    load_throughput: 35010884.808\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1014.747\n",
      "    sample_time_ms: 3941.871\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671817865\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:10 (running for 00:05:23.91)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         315.223</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 4.54826</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1320.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 1320.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 7.049778781008302\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.5318403244018555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015694156289100647\n",
      "          model: {}\n",
      "          policy_loss: -0.030311185866594315\n",
      "          total_loss: 3.240161657333374\n",
      "          vf_explained_var: 0.10720180720090866\n",
      "          vf_loss: 3.2657647132873535\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.349999999999994\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08842581872094493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31589680492527566\n",
      "    mean_inference_ms: 0.5138149539048361\n",
      "    mean_raw_obs_processing_ms: 0.06924785043020959\n",
      "  time_since_restore: 319.15712213516235\n",
      "  time_this_iter_s: 3.9341299533843994\n",
      "  time_total_s: 319.15712213516235\n",
      "  timers:\n",
      "    learn_throughput: 2085.828\n",
      "    learn_time_ms: 1917.703\n",
      "    load_throughput: 35157619.447\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1017.308\n",
      "    sample_time_ms: 3931.944\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1671817873\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:16 (running for 00:05:29.86)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         319.157</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> 7.04978</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1320.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 1332.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 12.043038435633903\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 300\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.590075969696045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008626681752502918\n",
      "          model: {}\n",
      "          policy_loss: -0.023494351655244827\n",
      "          total_loss: 66.61802673339844\n",
      "          vf_explained_var: 0.047501690685749054\n",
      "          vf_loss: 66.6389389038086\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.3\n",
      "    ram_util_percent: 21.54\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841714262782928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31570849494346487\n",
      "    mean_inference_ms: 0.5137098393252156\n",
      "    mean_raw_obs_processing_ms: 0.06921905794884363\n",
      "  time_since_restore: 326.9877076148987\n",
      "  time_this_iter_s: 3.9105162620544434\n",
      "  time_total_s: 326.9877076148987\n",
      "  timers:\n",
      "    learn_throughput: 2086.604\n",
      "    learn_time_ms: 1916.991\n",
      "    load_throughput: 35627980.463\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1016.854\n",
      "    sample_time_ms: 3933.701\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671817881\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:22 (running for 00:05:35.73)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         326.988</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  12.043</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1332.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:28 (running for 00:05:41.71)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         330.945</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 14.6468</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1348.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 1363.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 17.279553096828607\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 304\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.371147394180298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01655833050608635\n",
      "          model: {}\n",
      "          policy_loss: -0.037599917501211166\n",
      "          total_loss: 2.0469319820404053\n",
      "          vf_explained_var: 0.3261086642742157\n",
      "          vf_loss: 2.079564332962036\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.28\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841533842160096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31560493901416325\n",
      "    mean_inference_ms: 0.5136512616529868\n",
      "    mean_raw_obs_processing_ms: 0.06920264124797325\n",
      "  time_since_restore: 334.849826335907\n",
      "  time_this_iter_s: 3.905325412750244\n",
      "  time_total_s: 334.849826335907\n",
      "  timers:\n",
      "    learn_throughput: 2086.024\n",
      "    learn_time_ms: 1917.523\n",
      "    load_throughput: 35757067.349\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1016.844\n",
      "    sample_time_ms: 3933.739\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671817889\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:34 (running for 00:05:47.58)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         338.774</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 20.3365</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1378.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 1378.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 84.43436195472016\n",
      "  episode_reward_mean: 22.969029955103295\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 310\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.3551456928253174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015161244198679924\n",
      "          model: {}\n",
      "          policy_loss: -0.031618840992450714\n",
      "          total_loss: 4.154765605926514\n",
      "          vf_explained_var: 0.26732322573661804\n",
      "          vf_loss: 4.181836128234863\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.76666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840910478398245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31541734426445805\n",
      "    mean_inference_ms: 0.5135946601501473\n",
      "    mean_raw_obs_processing_ms: 0.0691714187171764\n",
      "  time_since_restore: 342.72857213020325\n",
      "  time_this_iter_s: 3.95479416847229\n",
      "  time_total_s: 342.72857213020325\n",
      "  timers:\n",
      "    learn_throughput: 2086.158\n",
      "    learn_time_ms: 1917.401\n",
      "    load_throughput: 35711400.596\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1015.223\n",
      "    sample_time_ms: 3940.021\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671817897\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:40 (running for 00:05:53.55)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         342.729</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  22.969</td><td style=\"text-align: right;\">             84.4344</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1378.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-45\n",
      "  done: false\n",
      "  episode_len_mean: 1337.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.55517015708877\n",
      "  episode_reward_mean: 22.741863724270537\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 317\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.1229748725891113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01905512809753418\n",
      "          model: {}\n",
      "          policy_loss: -0.03943061828613281\n",
      "          total_loss: 197.3453369140625\n",
      "          vf_explained_var: 0.09852447360754013\n",
      "          vf_loss: 197.37904357910156\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.0\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841134010196303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31524261421153005\n",
      "    mean_inference_ms: 0.5135497992250143\n",
      "    mean_raw_obs_processing_ms: 0.06914848247613206\n",
      "  time_since_restore: 350.6093156337738\n",
      "  time_this_iter_s: 3.8700342178344727\n",
      "  time_total_s: 350.6093156337738\n",
      "  timers:\n",
      "    learn_throughput: 2084.066\n",
      "    learn_time_ms: 1919.325\n",
      "    load_throughput: 35681020.842\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1013.871\n",
      "    sample_time_ms: 3945.274\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671817905\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:46 (running for 00:05:59.48)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         350.609</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 22.7419</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:52 (running for 00:06:05.47)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         354.555</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 22.6094</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1321.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-52\n",
      "  done: false\n",
      "  episode_len_mean: 1336.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.55517015708877\n",
      "  episode_reward_mean: 26.182590172996548\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 324\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.102459669113159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016271186992526054\n",
      "          model: {}\n",
      "          policy_loss: -0.030102042481303215\n",
      "          total_loss: 3.039541244506836\n",
      "          vf_explained_var: 0.2062022089958191\n",
      "          vf_loss: 3.0647618770599365\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.519999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840423406211702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3150330092649332\n",
      "    mean_inference_ms: 0.5135363178025186\n",
      "    mean_raw_obs_processing_ms: 0.06911885155435174\n",
      "  time_since_restore: 358.4737141132355\n",
      "  time_this_iter_s: 3.9191460609436035\n",
      "  time_total_s: 358.4737141132355\n",
      "  timers:\n",
      "    learn_throughput: 2085.176\n",
      "    learn_time_ms: 1918.303\n",
      "    load_throughput: 35575097.54\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1012.213\n",
      "    sample_time_ms: 3951.737\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671817912\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:57 (running for 00:06:11.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         362.392</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> 27.1643</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-00\n",
      "  done: false\n",
      "  episode_len_mean: 1337.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.87192519142675\n",
      "  episode_reward_mean: 27.973810778232405\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 330\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.965674877166748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025065293535590172\n",
      "          model: {}\n",
      "          policy_loss: -0.03103558160364628\n",
      "          total_loss: 34.40791320800781\n",
      "          vf_explained_var: -0.06598925590515137\n",
      "          vf_loss: 34.43143081665039\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.050000000000004\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840250334760698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31488535179460514\n",
      "    mean_inference_ms: 0.5135181904767774\n",
      "    mean_raw_obs_processing_ms: 0.06909831005593096\n",
      "  time_since_restore: 366.34616017341614\n",
      "  time_this_iter_s: 3.954435348510742\n",
      "  time_total_s: 366.34616017341614\n",
      "  timers:\n",
      "    learn_throughput: 2084.627\n",
      "    learn_time_ms: 1918.808\n",
      "    load_throughput: 34749826.015\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1010.945\n",
      "    sample_time_ms: 3956.693\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671817920\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:02 (running for 00:06:16.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         366.346</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 27.9738</td><td style=\"text-align: right;\">             95.8719</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-08\n",
      "  done: false\n",
      "  episode_len_mean: 1293.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.22397382335117\n",
      "  episode_reward_mean: 26.26615291229657\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 338\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7701303958892822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012452061288058758\n",
      "          model: {}\n",
      "          policy_loss: -0.03107476234436035\n",
      "          total_loss: 92.08829498291016\n",
      "          vf_explained_var: 0.04693411663174629\n",
      "          vf_loss: 92.11376953125\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.016666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839209924567124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31464595871681944\n",
      "    mean_inference_ms: 0.5135337305081521\n",
      "    mean_raw_obs_processing_ms: 0.06906641572182744\n",
      "  time_since_restore: 374.2050111293793\n",
      "  time_this_iter_s: 3.914933919906616\n",
      "  time_total_s: 374.2050111293793\n",
      "  timers:\n",
      "    learn_throughput: 2083.838\n",
      "    learn_time_ms: 1919.535\n",
      "    load_throughput: 34044675.325\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1011.326\n",
      "    sample_time_ms: 3955.203\n",
      "    update_time_ms: 1.347\n",
      "  timestamp: 1671817928\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:08 (running for 00:06:22.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         374.205</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 26.2662</td><td style=\"text-align: right;\">              98.224</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1293.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:14 (running for 00:06:28.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          378.16</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 26.5099</td><td style=\"text-align: right;\">             100.146</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1289.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 1298.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.14576247609915\n",
      "  episode_reward_mean: 28.713717585349382\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 343\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.965895175933838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01864263042807579\n",
      "          model: {}\n",
      "          policy_loss: -0.020977802574634552\n",
      "          total_loss: 5.930751323699951\n",
      "          vf_explained_var: 0.009173193946480751\n",
      "          vf_loss: 5.9433393478393555\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.36\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883943941562514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3145466538529968\n",
      "    mean_inference_ms: 0.5135458459970331\n",
      "    mean_raw_obs_processing_ms: 0.06905561437819846\n",
      "  time_since_restore: 382.090119600296\n",
      "  time_this_iter_s: 3.929793119430542\n",
      "  time_total_s: 382.090119600296\n",
      "  timers:\n",
      "    learn_throughput: 2083.829\n",
      "    learn_time_ms: 1919.544\n",
      "    load_throughput: 34183406.683\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1010.887\n",
      "    sample_time_ms: 3956.92\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671817936\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:20 (running for 00:06:34.12)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          382.09</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 28.7137</td><td style=\"text-align: right;\">             100.146</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">            1298.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 1269.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 26.95440202121816\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 347\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.859990119934082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009421590715646744\n",
      "          model: {}\n",
      "          policy_loss: -0.03246340900659561\n",
      "          total_loss: 120.37698364257812\n",
      "          vf_explained_var: -0.0648389607667923\n",
      "          vf_loss: 120.40519714355469\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.66666666666667\n",
      "    ram_util_percent: 22.522222222222222\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841430288993317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.314506742940488\n",
      "    mean_inference_ms: 0.5137131853452284\n",
      "    mean_raw_obs_processing_ms: 0.06906644618795137\n",
      "  time_since_restore: 388.18377685546875\n",
      "  time_this_iter_s: 6.0936572551727295\n",
      "  time_total_s: 388.18377685546875\n",
      "  timers:\n",
      "    learn_throughput: 1992.88\n",
      "    learn_time_ms: 2007.146\n",
      "    load_throughput: 33608205.128\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 981.068\n",
      "    sample_time_ms: 4077.188\n",
      "    update_time_ms: 1.345\n",
      "  timestamp: 1671817942\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:25 (running for 00:06:39.27)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         388.184</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> 26.9544</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1269.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 1239.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 24.636133543856154\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 351\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.856428623199463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011604920960962772\n",
      "          model: {}\n",
      "          policy_loss: -0.03069290705025196\n",
      "          total_loss: 32.55900955200195\n",
      "          vf_explained_var: -0.23071302473545074\n",
      "          vf_loss: 32.58448028564453\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.3125\n",
      "    ram_util_percent: 23.175\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845102498724286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.314541459659209\n",
      "    mean_inference_ms: 0.5139031788887232\n",
      "    mean_raw_obs_processing_ms: 0.0690889812750087\n",
      "  time_since_restore: 393.62455773353577\n",
      "  time_this_iter_s: 5.440780878067017\n",
      "  time_total_s: 393.62455773353577\n",
      "  timers:\n",
      "    learn_throughput: 1891.266\n",
      "    learn_time_ms: 2114.986\n",
      "    load_throughput: 31254128.167\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 949.337\n",
      "    sample_time_ms: 4213.467\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1671817948\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:31 (running for 00:06:44.75)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:36 (running for 00:06:49.77)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:41 (running for 00:06:54.78)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:46 (running for 00:06:59.78)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:51 (running for 00:07:04.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:56 (running for 00:07:09.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:01 (running for 00:07:14.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:06 (running for 00:07:19.80)<br>Memory usage on this node: 7.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:11 (running for 00:07:24.80)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:16 (running for 00:07:29.81)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:21 (running for 00:07:34.81)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:26 (running for 00:07:39.82)<br>Memory usage on this node: 7.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:31 (running for 00:07:44.82)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:36 (running for 00:07:49.82)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:41 (running for 00:07:54.83)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:46 (running for 00:07:59.83)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:51 (running for 00:08:04.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:56 (running for 00:08:09.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:01 (running for 00:08:14.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:06 (running for 00:08:19.85)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:11 (running for 00:08:24.85)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:16 (running for 00:08:29.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:21 (running for 00:08:34.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:26 (running for 00:08:39.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:31 (running for 00:08:44.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:36 (running for 00:08:49.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:41 (running for 00:08:54.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:46 (running for 00:08:59.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:51 (running for 00:09:04.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-54-53\n",
      "  done: false\n",
      "  episode_len_mean: 1254.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 27.447117710517574\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1452.11\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 118.75265453299521\n",
      "    episode_reward_mean: 74.1810856568662\n",
      "    episode_reward_min: -127.47615840311785\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 153\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 130\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 131\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 143\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 154\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 52\n",
      "      - 112\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 153\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 125\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 58\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 78.42400498591114\n",
      "      - 107.23675819045734\n",
      "      - 90.93516519455292\n",
      "      - 96.29003044224909\n",
      "      - 83.25675050320302\n",
      "      - 98.42055566307404\n",
      "      - -125.67074016297298\n",
      "      - 82.66246388052717\n",
      "      - 73.6163366743246\n",
      "      - 93.14487152264573\n",
      "      - 90.99009892890544\n",
      "      - 101.14873689156472\n",
      "      - 100.37171714548445\n",
      "      - 87.65793576411654\n",
      "      - 102.73661764596413\n",
      "      - 97.48697272011198\n",
      "      - 92.37115546542333\n",
      "      - 71.79903481600685\n",
      "      - 102.69915795307898\n",
      "      - 89.70385368558587\n",
      "      - 108.21168169774998\n",
      "      - 104.58860545074182\n",
      "      - 108.85922607881567\n",
      "      - 93.04850372730013\n",
      "      - 94.46214578143874\n",
      "      - 105.5815265290149\n",
      "      - 103.93626015781228\n",
      "      - 94.46124004799886\n",
      "      - 101.73515394977915\n",
      "      - 91.42912422639503\n",
      "      - 98.32846556951621\n",
      "      - 106.82301206907009\n",
      "      - 104.17844113640936\n",
      "      - -119.38517284958189\n",
      "      - 106.49715388046461\n",
      "      - 90.70044804339733\n",
      "      - 89.89525145823502\n",
      "      - 89.3188674078093\n",
      "      - 104.64142995106285\n",
      "      - 118.75265453299521\n",
      "      - -122.31440798999245\n",
      "      - 89.63611964106177\n",
      "      - 96.73155319788248\n",
      "      - 99.73832102659532\n",
      "      - 102.7526802937179\n",
      "      - -127.47615840311785\n",
      "      - 114.32462048831073\n",
      "      - 103.33425487037945\n",
      "      - 80.08380470716585\n",
      "      - 106.66913633388711\n",
      "      - -99.51674852522524\n",
      "      - 98.91251943954467\n",
      "      - 108.02227857067986\n",
      "      - 101.4076395165392\n",
      "      - 87.68512411140026\n",
      "      - 91.10292231800081\n",
      "      - 93.99477381535439\n",
      "      - 93.76894666821578\n",
      "      - 89.01957975356382\n",
      "      - 91.17493271486687\n",
      "      - 99.326762892497\n",
      "      - 81.75766606957464\n",
      "      - 101.93966538078844\n",
      "      - 110.02411686724314\n",
      "      - 93.79365284988405\n",
      "      - 85.72561475869279\n",
      "      - 84.24794539385721\n",
      "      - 101.73572315276219\n",
      "      - 80.56923031228351\n",
      "      - 107.52205106860306\n",
      "      - 79.44881992745773\n",
      "      - 80.50545993340666\n",
      "      - 94.30111625185607\n",
      "      - 83.57395171995758\n",
      "      - 92.02572160524632\n",
      "      - 93.11395268752918\n",
      "      - 96.04979773938652\n",
      "      - -111.41583665737075\n",
      "      - -121.77145806111768\n",
      "      - 98.76851986086159\n",
      "      - 93.78064463444262\n",
      "      - 91.8520579333385\n",
      "      - 94.70078801878975\n",
      "      - 70.26692126700037\n",
      "      - -126.26975751312017\n",
      "      - 98.54965751751871\n",
      "      - 97.7366344049263\n",
      "      - 106.6287382263062\n",
      "      - 97.37112949296268\n",
      "      - -122.89920833309927\n",
      "      - 98.81055969419086\n",
      "      - 107.42172554441053\n",
      "      - 89.44662875839563\n",
      "      - 108.55835810110347\n",
      "      - 95.82789523768237\n",
      "      - 113.01186725281181\n",
      "      - -112.43619089618822\n",
      "      - 83.7898624140036\n",
      "      - 96.89196559201505\n",
      "      - 93.42842528026061\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08747001897828019\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.31139213375600866\n",
      "      mean_inference_ms: 0.5059533663250226\n",
      "      mean_raw_obs_processing_ms: 0.060765717905211264\n",
      "    timesteps_this_iter: 145211\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7035062313079834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013569644652307034\n",
      "          model: {}\n",
      "          policy_loss: -0.03527693450450897\n",
      "          total_loss: 6.165103435516357\n",
      "          vf_explained_var: -0.015572791919112206\n",
      "          vf_loss: 6.194273948669434\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.889320388349514\n",
      "    ram_util_percent: 26.581067961165047\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848439734840817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31454508925820396\n",
      "    mean_inference_ms: 0.514181550427439\n",
      "    mean_raw_obs_processing_ms: 0.06911087113896594\n",
      "  time_since_restore: 538.7042179107666\n",
      "  time_this_iter_s: 145.07966017723083\n",
      "  time_total_s: 538.7042179107666\n",
      "  timers:\n",
      "    learn_throughput: 1888.195\n",
      "    learn_time_ms: 2118.425\n",
      "    load_throughput: 31277434.75\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 916.341\n",
      "    sample_time_ms: 4365.189\n",
      "    update_time_ms: 1.99\n",
      "  timestamp: 1671818093\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:54:53,423\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:56 (running for 00:09:09.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         538.704</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 27.4471</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1254.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-01\n",
      "  done: false\n",
      "  episode_len_mean: 1255.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.30952588411367\n",
      "  episode_reward_mean: 28.91912529569661\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 360\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.608483076095581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013637454248964787\n",
      "          model: {}\n",
      "          policy_loss: -0.03710879758000374\n",
      "          total_loss: 2.845229387283325\n",
      "          vf_explained_var: 0.2855461537837982\n",
      "          vf_loss: 2.8762013912200928\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.1\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08854210489814651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31463197908933654\n",
      "    mean_inference_ms: 0.5144904492704786\n",
      "    mean_raw_obs_processing_ms: 0.06915064511540976\n",
      "  time_since_restore: 546.6923115253448\n",
      "  time_this_iter_s: 3.964552879333496\n",
      "  time_total_s: 546.6923115253448\n",
      "  timers:\n",
      "    learn_throughput: 1884.059\n",
      "    learn_time_ms: 2123.075\n",
      "    load_throughput: 29579012.694\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 216.732\n",
      "    sample_time_ms: 18455.966\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818101\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:02 (running for 00:09:15.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         546.692</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> 28.9191</td><td style=\"text-align: right;\">              102.31</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1255.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:07 (running for 00:09:20.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         550.685</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> 30.0872</td><td style=\"text-align: right;\">             110.141</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1255.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 1271.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 34.109509613645265\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 366\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.449204206466675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01432492770254612\n",
      "          model: {}\n",
      "          policy_loss: -0.029611118137836456\n",
      "          total_loss: 4.188887119293213\n",
      "          vf_explained_var: 0.3925740122795105\n",
      "          vf_loss: 4.212051868438721\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.0\n",
      "    ram_util_percent: 29.1\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08859707403800515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3146608489513967\n",
      "    mean_inference_ms: 0.5149097180035009\n",
      "    mean_raw_obs_processing_ms: 0.06918873553670771\n",
      "  time_since_restore: 554.6946558952332\n",
      "  time_this_iter_s: 4.010099649429321\n",
      "  time_total_s: 554.6946558952332\n",
      "  timers:\n",
      "    learn_throughput: 1880.836\n",
      "    learn_time_ms: 2126.714\n",
      "    load_throughput: 29526955.297\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 216.619\n",
      "    sample_time_ms: 18465.585\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1671818109\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:12 (running for 00:09:25.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         554.695</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> 34.1095</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1271.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 1288.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 37.64746195318608\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 371\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.6842358112335205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012124155648052692\n",
      "          model: {}\n",
      "          policy_loss: -0.03358088433742523\n",
      "          total_loss: 105.49585723876953\n",
      "          vf_explained_var: -0.054968222975730896\n",
      "          vf_loss: 105.52398681640625\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.85\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08864888389176645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3147146782838911\n",
      "    mean_inference_ms: 0.5152573638326803\n",
      "    mean_raw_obs_processing_ms: 0.06922438712828831\n",
      "  time_since_restore: 562.7095308303833\n",
      "  time_this_iter_s: 3.955777168273926\n",
      "  time_total_s: 562.7095308303833\n",
      "  timers:\n",
      "    learn_throughput: 1877.663\n",
      "    learn_time_ms: 2130.307\n",
      "    load_throughput: 29066555.787\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 216.446\n",
      "    sample_time_ms: 18480.32\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818117\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:17 (running for 00:09:30.98)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">          562.71</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 37.6475</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">            1288.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:22 (running for 00:09:36.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         566.757</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> 40.4139</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1303.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 1303.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 42.662411900685385\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 376\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3780109882354736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0168222114443779\n",
      "          model: {}\n",
      "          policy_loss: -0.03533028066158295\n",
      "          total_loss: 3.2900192737579346\n",
      "          vf_explained_var: 0.43231603503227234\n",
      "          vf_loss: 3.317779541015625\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.02\n",
      "    ram_util_percent: 29.140000000000004\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08870234566630175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31478270995944857\n",
      "    mean_inference_ms: 0.5156129435019607\n",
      "    mean_raw_obs_processing_ms: 0.06926023836408315\n",
      "  time_since_restore: 570.7154223918915\n",
      "  time_this_iter_s: 3.9586386680603027\n",
      "  time_total_s: 570.7154223918915\n",
      "  timers:\n",
      "    learn_throughput: 1956.578\n",
      "    learn_time_ms: 2044.386\n",
      "    load_throughput: 29433712.281\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 217.795\n",
      "    sample_time_ms: 18365.882\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818125\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:27 (running for 00:09:41.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         570.715</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> 42.6624</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1303.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:33 (running for 00:09:47.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         574.721</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> 47.1577</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1333.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 1317.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 46.35326511237408\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 382\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3456103801727295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012088745832443237\n",
      "          model: {}\n",
      "          policy_loss: -0.027218114584684372\n",
      "          total_loss: 15.973329544067383\n",
      "          vf_explained_var: 0.5255213975906372\n",
      "          vf_loss: 15.99510669708252\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.25\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08876904685309162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3148755878973006\n",
      "    mean_inference_ms: 0.5160532088311155\n",
      "    mean_raw_obs_processing_ms: 0.0693070653179218\n",
      "  time_since_restore: 578.8327028751373\n",
      "  time_this_iter_s: 4.1118996143341064\n",
      "  time_total_s: 578.8327028751373\n",
      "  timers:\n",
      "    learn_throughput: 2057.197\n",
      "    learn_time_ms: 1944.393\n",
      "    load_throughput: 30126083.678\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 221.015\n",
      "    sample_time_ms: 18098.314\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671818133\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:38 (running for 00:09:52.28)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         582.919</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  43.367</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1287.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 1288.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 44.204749594958514\n",
      "  episode_reward_min: -126.94262364739367\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 390\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0511093139648438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01803765259683132\n",
      "          model: {}\n",
      "          policy_loss: -0.02746838890016079\n",
      "          total_loss: 25.28322410583496\n",
      "          vf_explained_var: 0.09650327265262604\n",
      "          vf_loss: 25.302576065063477\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.76666666666666\n",
      "    ram_util_percent: 29.266666666666666\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08886839731248371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31505139462702675\n",
      "    mean_inference_ms: 0.516656544803917\n",
      "    mean_raw_obs_processing_ms: 0.06937737613741662\n",
      "  time_since_restore: 586.9022233486176\n",
      "  time_this_iter_s: 3.9833316802978516\n",
      "  time_total_s: 586.9022233486176\n",
      "  timers:\n",
      "    learn_throughput: 2057.613\n",
      "    learn_time_ms: 1944.0\n",
      "    load_throughput: 32183418.377\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 990.281\n",
      "    sample_time_ms: 4039.258\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1671818141\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:43 (running for 00:09:57.29)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         586.902</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> 44.2047</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1288.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:49 (running for 00:10:02.46)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         591.024</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  44.088</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1274.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 1259.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 122.6038127516478\n",
      "  episode_reward_mean: 42.91967497064192\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 398\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.22045636177063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013660403899848461\n",
      "          model: {}\n",
      "          policy_loss: -0.033493686467409134\n",
      "          total_loss: 12.973958969116211\n",
      "          vf_explained_var: 0.16314588487148285\n",
      "          vf_loss: 13.001306533813477\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08897132740821215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3152554224220419\n",
      "    mean_inference_ms: 0.5173227308992535\n",
      "    mean_raw_obs_processing_ms: 0.0694548289151114\n",
      "  time_since_restore: 595.0009732246399\n",
      "  time_this_iter_s: 3.976592540740967\n",
      "  time_total_s: 595.0009732246399\n",
      "  timers:\n",
      "    learn_throughput: 2056.918\n",
      "    learn_time_ms: 1944.657\n",
      "    load_throughput: 33097683.961\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 987.892\n",
      "    sample_time_ms: 4049.026\n",
      "    update_time_ms: 1.366\n",
      "  timestamp: 1671818149\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:54 (running for 00:10:07.49)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">          599.05</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> 43.7369</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 122.6038127516478\n",
      "  episode_reward_mean: 44.44832267207842\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 403\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.321965217590332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023172365501523018\n",
      "          model: {}\n",
      "          policy_loss: -0.024016866460442543\n",
      "          total_loss: 2.4006521701812744\n",
      "          vf_explained_var: 0.4318290650844574\n",
      "          vf_loss: 2.4142415523529053\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.650000000000006\n",
      "    ram_util_percent: 29.483333333333334\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08903426221603805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31537496318243635\n",
      "    mean_inference_ms: 0.5177527855697415\n",
      "    mean_raw_obs_processing_ms: 0.06950315083477063\n",
      "  time_since_restore: 603.1596901416779\n",
      "  time_this_iter_s: 4.109773635864258\n",
      "  time_total_s: 603.1596901416779\n",
      "  timers:\n",
      "    learn_throughput: 2049.79\n",
      "    learn_time_ms: 1951.419\n",
      "    load_throughput: 33635156.375\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 985.031\n",
      "    sample_time_ms: 4060.785\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671818158\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:59 (running for 00:10:12.63)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">          603.16</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> 44.4483</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:04 (running for 00:10:17.68)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         607.197</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> 45.6969</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 126.33096261964698\n",
      "  episode_reward_mean: 46.262064415339076\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 409\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.384523630142212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00953748356550932\n",
      "          model: {}\n",
      "          policy_loss: -0.030215207487344742\n",
      "          total_loss: 2.475980520248413\n",
      "          vf_explained_var: 0.5267147421836853\n",
      "          vf_loss: 2.499758005142212\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.94\n",
      "    ram_util_percent: 29.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08911128745059836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3155261763532507\n",
      "    mean_inference_ms: 0.5182765675322294\n",
      "    mean_raw_obs_processing_ms: 0.0695636082761876\n",
      "  time_since_restore: 611.1544244289398\n",
      "  time_this_iter_s: 3.957780122756958\n",
      "  time_total_s: 611.1544244289398\n",
      "  timers:\n",
      "    learn_throughput: 2049.188\n",
      "    learn_time_ms: 1951.993\n",
      "    load_throughput: 32097218.29\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 984.111\n",
      "    sample_time_ms: 4064.583\n",
      "    update_time_ms: 1.382\n",
      "  timestamp: 1671818166\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:09 (running for 00:10:22.69)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         611.154</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> 46.2621</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:14 (running for 00:10:27.82)<br>Memory usage on this node: 9.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         615.299</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">   46.71</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 47.62177510591871\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 413\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0139198303222656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012564915232360363\n",
      "          model: {}\n",
      "          policy_loss: -0.02764781191945076\n",
      "          total_loss: 1.7638471126556396\n",
      "          vf_explained_var: 0.6364338994026184\n",
      "          vf_loss: 1.7830135822296143\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.62857142857143\n",
      "    ram_util_percent: 30.37142857142857\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08916326587789809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31563540490398123\n",
      "    mean_inference_ms: 0.5186277868653006\n",
      "    mean_raw_obs_processing_ms: 0.06960445057727725\n",
      "  time_since_restore: 619.9019114971161\n",
      "  time_this_iter_s: 4.6027398109436035\n",
      "  time_total_s: 619.9019114971161\n",
      "  timers:\n",
      "    learn_throughput: 1992.444\n",
      "    learn_time_ms: 2007.584\n",
      "    load_throughput: 33614938.89\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 980.603\n",
      "    sample_time_ms: 4079.122\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1671818175\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:19 (running for 00:10:33.34)<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         624.767</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> 52.1212</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1285.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 1301.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 55.581310278733774\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 420\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8927011489868164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023308034986257553\n",
      "          model: {}\n",
      "          policy_loss: -0.02061375416815281\n",
      "          total_loss: 53.472206115722656\n",
      "          vf_explained_var: 0.5795298218727112\n",
      "          vf_loss: 53.47708511352539\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.57142857142858\n",
      "    ram_util_percent: 35.357142857142854\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08927617790716287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31590125253596346\n",
      "    mean_inference_ms: 0.5193653848555152\n",
      "    mean_raw_obs_processing_ms: 0.06969585349552912\n",
      "  time_since_restore: 629.6155297756195\n",
      "  time_this_iter_s: 4.848086595535278\n",
      "  time_total_s: 629.6155297756195\n",
      "  timers:\n",
      "    learn_throughput: 1933.136\n",
      "    learn_time_ms: 2069.177\n",
      "    load_throughput: 31143894.561\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 937.049\n",
      "    sample_time_ms: 4268.718\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1671818184\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:25 (running for 00:10:39.21)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         629.616</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\"> 55.5813</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1301.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:30 (running for 00:10:44.31)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         633.701</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> 54.0808</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1286.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 1286.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 54.77455877721565\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 426\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0714166164398193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008197540417313576\n",
      "          model: {}\n",
      "          policy_loss: -0.034332480281591415\n",
      "          total_loss: 22.94219207763672\n",
      "          vf_explained_var: 0.418192982673645\n",
      "          vf_loss: 22.968225479125977\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.666666666666664\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08938626601966489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31618140360249386\n",
      "    mean_inference_ms: 0.5200287931939909\n",
      "    mean_raw_obs_processing_ms: 0.06978515740545982\n",
      "  time_since_restore: 637.7871990203857\n",
      "  time_this_iter_s: 4.0865397453308105\n",
      "  time_total_s: 637.7871990203857\n",
      "  timers:\n",
      "    learn_throughput: 1931.544\n",
      "    learn_time_ms: 2070.882\n",
      "    load_throughput: 30937149.179\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 930.474\n",
      "    sample_time_ms: 4298.885\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1671818192\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:36 (running for 00:10:49.42)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         637.787</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> 54.7746</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1286.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 1301.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 58.99895573487629\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 431\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0431289672851562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007684572134166956\n",
      "          model: {}\n",
      "          policy_loss: -0.019729336723685265\n",
      "          total_loss: 17.551807403564453\n",
      "          vf_explained_var: 0.5051132440567017\n",
      "          vf_loss: 17.56375503540039\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.483333333333334\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08947606078744702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163998649183084\n",
      "    mean_inference_ms: 0.5205962877244271\n",
      "    mean_raw_obs_processing_ms: 0.0698571013509692\n",
      "  time_since_restore: 645.8360183238983\n",
      "  time_this_iter_s: 3.9911794662475586\n",
      "  time_total_s: 645.8360183238983\n",
      "  timers:\n",
      "    learn_throughput: 1936.012\n",
      "    learn_time_ms: 2066.103\n",
      "    load_throughput: 28986205.943\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 932.262\n",
      "    sample_time_ms: 4290.639\n",
      "    update_time_ms: 1.449\n",
      "  timestamp: 1671818201\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:41 (running for 00:10:54.51)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         645.836</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">  58.999</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1301.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:46 (running for 00:10:59.57)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         649.878</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\"> 61.6011</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">              1316</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-49\n",
      "  done: false\n",
      "  episode_len_mean: 1346.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 66.37294805165301\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 436\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.177299737930298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01001816801726818\n",
      "          model: {}\n",
      "          policy_loss: -0.02881176397204399\n",
      "          total_loss: 2.413037061691284\n",
      "          vf_explained_var: 0.6113097667694092\n",
      "          vf_loss: 2.4317054748535156\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.43333333333333\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08957580559198954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31667754349457516\n",
      "    mean_inference_ms: 0.5211458338421836\n",
      "    mean_raw_obs_processing_ms: 0.0699359119041447\n",
      "  time_since_restore: 653.9037961959839\n",
      "  time_this_iter_s: 4.026112079620361\n",
      "  time_total_s: 653.9037961959839\n",
      "  timers:\n",
      "    learn_throughput: 1933.809\n",
      "    learn_time_ms: 2068.456\n",
      "    load_throughput: 29641724.382\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 931.427\n",
      "    sample_time_ms: 4294.484\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1671818209\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:51 (running for 00:11:04.62)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         653.904</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> 66.3729</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1346.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:56 (running for 00:11:09.72)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         657.987</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> 66.9769</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1346.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 1350.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 69.00297647506746\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 441\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2565758228302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008600729517638683\n",
      "          model: {}\n",
      "          policy_loss: -0.029129138216376305\n",
      "          total_loss: 3.206780195236206\n",
      "          vf_explained_var: 0.5763169527053833\n",
      "          vf_loss: 3.227201223373413\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.92\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0896641123457716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316880790383373\n",
      "    mean_inference_ms: 0.5217383248021558\n",
      "    mean_raw_obs_processing_ms: 0.07000630745383107\n",
      "  time_since_restore: 662.031911611557\n",
      "  time_this_iter_s: 4.044794082641602\n",
      "  time_total_s: 662.031911611557\n",
      "  timers:\n",
      "    learn_throughput: 1989.723\n",
      "    learn_time_ms: 2010.33\n",
      "    load_throughput: 29610335.334\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 933.359\n",
      "    sample_time_ms: 4285.598\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1671818217\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:01 (running for 00:11:14.82)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         666.046</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\"> 70.2796</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1350.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 1379.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 74.54114709397021\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 446\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.204482316970825\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008961320854723454\n",
      "          model: {}\n",
      "          policy_loss: -0.02460744045674801\n",
      "          total_loss: 2.5113046169281006\n",
      "          vf_explained_var: 0.5614314675331116\n",
      "          vf_loss: 2.5268383026123047\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.65\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08975002145732258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171253615104138\n",
      "    mean_inference_ms: 0.5221938162768166\n",
      "    mean_raw_obs_processing_ms: 0.07007074204522987\n",
      "  time_since_restore: 670.062967300415\n",
      "  time_this_iter_s: 4.016982078552246\n",
      "  time_total_s: 670.062967300415\n",
      "  timers:\n",
      "    learn_throughput: 2053.543\n",
      "    learn_time_ms: 1947.853\n",
      "    load_throughput: 29863325.027\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 978.468\n",
      "    sample_time_ms: 4088.024\n",
      "    update_time_ms: 1.437\n",
      "  timestamp: 1671818225\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:06 (running for 00:11:19.86)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         670.063</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> 74.5411</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1379.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:11 (running for 00:11:24.91)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         674.093</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  75.123</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1379.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 1394.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 78.109069994111\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 452\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2254371643066406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008994014002382755\n",
      "          model: {}\n",
      "          policy_loss: -0.031866997480392456\n",
      "          total_loss: 2.239166498184204\n",
      "          vf_explained_var: 0.5651485323905945\n",
      "          vf_loss: 2.2619271278381348\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.17999999999999\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08981458340251397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172652171623913\n",
      "    mean_inference_ms: 0.5226222166779463\n",
      "    mean_raw_obs_processing_ms: 0.07011941778558727\n",
      "  time_since_restore: 678.1003065109253\n",
      "  time_this_iter_s: 4.007109880447388\n",
      "  time_total_s: 678.1003065109253\n",
      "  timers:\n",
      "    learn_throughput: 2053.903\n",
      "    learn_time_ms: 1947.512\n",
      "    load_throughput: 30012908.766\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 987.277\n",
      "    sample_time_ms: 4051.547\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1671818233\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:16 (running for 00:11:29.94)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">           678.1</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> 78.1091</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1394.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:21 (running for 00:11:35.06)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         682.172</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> 79.0305</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1394.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 1393.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 79.99243735256908\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2322611808776855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0071028308011591434\n",
      "          model: {}\n",
      "          policy_loss: -0.030082108452916145\n",
      "          total_loss: 9.26272201538086\n",
      "          vf_explained_var: 0.5659865736961365\n",
      "          vf_loss: 9.285612106323242\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 36.35\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08987236609889492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31738774869238096\n",
      "    mean_inference_ms: 0.5229688329560273\n",
      "    mean_raw_obs_processing_ms: 0.07016066030780642\n",
      "  time_since_restore: 686.2089760303497\n",
      "  time_this_iter_s: 4.037159442901611\n",
      "  time_total_s: 686.2089760303497\n",
      "  timers:\n",
      "    learn_throughput: 2051.849\n",
      "    learn_time_ms: 1949.461\n",
      "    load_throughput: 32084941.671\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 986.064\n",
      "    sample_time_ms: 4056.533\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1671818241\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:26 (running for 00:11:40.19)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          690.29</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> 80.3943</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-30\n",
      "  done: false\n",
      "  episode_len_mean: 1393.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 81.18645039663168\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 463\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2718379497528076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009212736040353775\n",
      "          model: {}\n",
      "          policy_loss: -0.029092730954289436\n",
      "          total_loss: 2.3715531826019287\n",
      "          vf_explained_var: 0.6008093953132629\n",
      "          vf_loss: 2.3913180828094482\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 34.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08992700932394586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31752120785546484\n",
      "    mean_inference_ms: 0.5232621091556249\n",
      "    mean_raw_obs_processing_ms: 0.07019887886983084\n",
      "  time_since_restore: 694.6848835945129\n",
      "  time_this_iter_s: 4.395348072052002\n",
      "  time_total_s: 694.6848835945129\n",
      "  timers:\n",
      "    learn_throughput: 2030.454\n",
      "    learn_time_ms: 1970.003\n",
      "    load_throughput: 32710501.072\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 980.193\n",
      "    sample_time_ms: 4080.83\n",
      "    update_time_ms: 1.456\n",
      "  timestamp: 1671818250\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:32 (running for 00:11:45.61)<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         694.685</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> 81.1865</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:37 (running for 00:11:50.73)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         698.785</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\"> 81.7178</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 1380.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 80.53942816985963\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 470\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.139829158782959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00637059798464179\n",
      "          model: {}\n",
      "          policy_loss: -0.033145446330308914\n",
      "          total_loss: 117.55618286132812\n",
      "          vf_explained_var: 0.2730858325958252\n",
      "          vf_loss: 117.58287048339844\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.38333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09000760141093485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3177395588276255\n",
      "    mean_inference_ms: 0.5236669120017611\n",
      "    mean_raw_obs_processing_ms: 0.07025764058603018\n",
      "  time_since_restore: 702.8028135299683\n",
      "  time_this_iter_s: 4.017354726791382\n",
      "  time_total_s: 702.8028135299683\n",
      "  timers:\n",
      "    learn_throughput: 2025.692\n",
      "    learn_time_ms: 1974.634\n",
      "    load_throughput: 32678644.332\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 976.123\n",
      "    sample_time_ms: 4097.843\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1671818258\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:42 (running for 00:11:55.76)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         706.781</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\"> 81.1051</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1380.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 1366.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 79.3544606917004\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 476\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0782907009124756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011249763891100883\n",
      "          model: {}\n",
      "          policy_loss: -0.0308041013777256\n",
      "          total_loss: 12.833398818969727\n",
      "          vf_explained_var: 0.5108848214149475\n",
      "          vf_loss: 12.852811813354492\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.38333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09006364526431637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3178404503990074\n",
      "    mean_inference_ms: 0.5240359090767374\n",
      "    mean_raw_obs_processing_ms: 0.07029820909590243\n",
      "  time_since_restore: 710.8771681785583\n",
      "  time_this_iter_s: 4.095668315887451\n",
      "  time_total_s: 710.8771681785583\n",
      "  timers:\n",
      "    learn_throughput: 2024.169\n",
      "    learn_time_ms: 1976.12\n",
      "    load_throughput: 33900214.185\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 975.37\n",
      "    sample_time_ms: 4101.009\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1671818266\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:47 (running for 00:12:00.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         710.877</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 79.3545</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:52 (running for 00:12:05.92)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         714.879</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> 80.1382</td><td style=\"text-align: right;\">             147.251</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 1366.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.25126510115922\n",
      "  episode_reward_mean: 80.40847320797435\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 481\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.20212459564209\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009055829606950283\n",
      "          model: {}\n",
      "          policy_loss: -0.033792540431022644\n",
      "          total_loss: 4.765120029449463\n",
      "          vf_explained_var: 0.5354225039482117\n",
      "          vf_loss: 4.789742946624756\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09011979590673563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31799487457924824\n",
      "    mean_inference_ms: 0.524307434764039\n",
      "    mean_raw_obs_processing_ms: 0.07034018683605428\n",
      "  time_since_restore: 718.8134818077087\n",
      "  time_this_iter_s: 3.9347705841064453\n",
      "  time_total_s: 718.8134818077087\n",
      "  timers:\n",
      "    learn_throughput: 2025.842\n",
      "    learn_time_ms: 1974.487\n",
      "    load_throughput: 32768000.0\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 976.609\n",
      "    sample_time_ms: 4095.807\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671818274\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:58 (running for 00:12:11.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         718.813</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> 80.4085</td><td style=\"text-align: right;\">             147.251</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 1400.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 151.26318671870854\n",
      "  episode_reward_mean: 87.09442703855493\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 486\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8513638973236084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008961099199950695\n",
      "          model: {}\n",
      "          policy_loss: -0.028916602954268456\n",
      "          total_loss: 2.6553854942321777\n",
      "          vf_explained_var: 0.5985805988311768\n",
      "          vf_loss: 2.6752288341522217\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.2\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09017036937851768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31811351076414424\n",
      "    mean_inference_ms: 0.524575675215681\n",
      "    mean_raw_obs_processing_ms: 0.07037534665446524\n",
      "  time_since_restore: 726.7648782730103\n",
      "  time_this_iter_s: 3.935490131378174\n",
      "  time_total_s: 726.7648782730103\n",
      "  timers:\n",
      "    learn_throughput: 2031.658\n",
      "    learn_time_ms: 1968.835\n",
      "    load_throughput: 32736031.22\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 980.225\n",
      "    sample_time_ms: 4080.696\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1671818282\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:04 (running for 00:12:17.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         726.765</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> 87.0944</td><td style=\"text-align: right;\">             151.263</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1400.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:10 (running for 00:12:23.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         730.733</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> 90.4803</td><td style=\"text-align: right;\">             151.263</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1414.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 1429.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 151.26318671870854\n",
      "  episode_reward_mean: 93.5864754174947\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 492\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8561146259307861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00982006173580885\n",
      "          model: {}\n",
      "          policy_loss: -0.027513107284903526\n",
      "          total_loss: 2.826796770095825\n",
      "          vf_explained_var: 0.6242074370384216\n",
      "          vf_loss: 2.844367027282715\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.916666666666664\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09022913547509027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31824960634206767\n",
      "    mean_inference_ms: 0.5248798195830648\n",
      "    mean_raw_obs_processing_ms: 0.07041690575770904\n",
      "  time_since_restore: 734.8074824810028\n",
      "  time_this_iter_s: 4.074692487716675\n",
      "  time_total_s: 734.8074824810028\n",
      "  timers:\n",
      "    learn_throughput: 2055.712\n",
      "    learn_time_ms: 1945.798\n",
      "    load_throughput: 32793620.016\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 986.37\n",
      "    sample_time_ms: 4055.273\n",
      "    update_time_ms: 1.419\n",
      "  timestamp: 1671818290\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:15 (running for 00:12:28.99)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         738.827</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> 96.2941</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1443.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-18\n",
      "  done: false\n",
      "  episode_len_mean: 1443.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 96.92724446063967\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 496\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9297425746917725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009765749797224998\n",
      "          model: {}\n",
      "          policy_loss: -0.031220557168126106\n",
      "          total_loss: 2.3137872219085693\n",
      "          vf_explained_var: 0.6765412092208862\n",
      "          vf_loss: 2.3351197242736816\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09026447923978669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31831583233013794\n",
      "    mean_inference_ms: 0.5250834841790782\n",
      "    mean_raw_obs_processing_ms: 0.0704406825602085\n",
      "  time_since_restore: 742.8542339801788\n",
      "  time_this_iter_s: 4.027336597442627\n",
      "  time_total_s: 742.8542339801788\n",
      "  timers:\n",
      "    learn_throughput: 2065.469\n",
      "    learn_time_ms: 1936.606\n",
      "    load_throughput: 32844980.423\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 992.421\n",
      "    sample_time_ms: 4030.548\n",
      "    update_time_ms: 1.443\n",
      "  timestamp: 1671818298\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:20 (running for 00:12:34.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         742.854</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> 96.9272</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1443.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:25 (running for 00:12:39.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         746.891</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> 100.063</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1458.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-26\n",
      "  done: false\n",
      "  episode_len_mean: 1472.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 103.08487672342753\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 502\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9912160634994507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009914674796164036\n",
      "          model: {}\n",
      "          policy_loss: -0.027051905170083046\n",
      "          total_loss: 3.8043265342712402\n",
      "          vf_explained_var: 0.5716634392738342\n",
      "          vf_loss: 3.8213398456573486\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.583333333333336\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09031921564811883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184313046292195\n",
      "    mean_inference_ms: 0.5253832087791459\n",
      "    mean_raw_obs_processing_ms: 0.07047755122723778\n",
      "  time_since_restore: 750.8689427375793\n",
      "  time_this_iter_s: 3.9783523082733154\n",
      "  time_total_s: 750.8689427375793\n",
      "  timers:\n",
      "    learn_throughput: 2066.801\n",
      "    learn_time_ms: 1935.358\n",
      "    load_throughput: 33825032.258\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 994.464\n",
      "    sample_time_ms: 4022.267\n",
      "    update_time_ms: 1.427\n",
      "  timestamp: 1671818306\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:31 (running for 00:12:45.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         754.823</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\"> 101.703</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1467.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-34\n",
      "  done: false\n",
      "  episode_len_mean: 1443.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 98.00017965300813\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 508\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9284090995788574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009837925434112549\n",
      "          model: {}\n",
      "          policy_loss: -0.0339592844247818\n",
      "          total_loss: 117.75109100341797\n",
      "          vf_explained_var: -0.11257561296224594\n",
      "          vf_loss: 117.77509307861328\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.43333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09037100269325254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31853237660904404\n",
      "    mean_inference_ms: 0.5256610569854435\n",
      "    mean_raw_obs_processing_ms: 0.07051130708633016\n",
      "  time_since_restore: 758.7778739929199\n",
      "  time_this_iter_s: 3.954930067062378\n",
      "  time_total_s: 758.7778739929199\n",
      "  timers:\n",
      "    learn_throughput: 2069.653\n",
      "    learn_time_ms: 1932.691\n",
      "    load_throughput: 35128174.204\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 995.867\n",
      "    sample_time_ms: 4016.602\n",
      "    update_time_ms: 1.422\n",
      "  timestamp: 1671818314\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:37 (running for 00:12:51.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         758.778</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> 98.0002</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:42 (running for 00:12:56.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         762.758</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> 98.8024</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 1443.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 99.2611229595821\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 513\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8944987058639526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009089785628020763\n",
      "          model: {}\n",
      "          policy_loss: -0.03356286883354187\n",
      "          total_loss: 2.626680612564087\n",
      "          vf_explained_var: 0.5540494322776794\n",
      "          vf_loss: 2.6510400772094727\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.6\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09041398069266997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3186345120178261\n",
      "    mean_inference_ms: 0.5258697612000426\n",
      "    mean_raw_obs_processing_ms: 0.07054137089245695\n",
      "  time_since_restore: 766.795618057251\n",
      "  time_this_iter_s: 4.037226676940918\n",
      "  time_total_s: 766.795618057251\n",
      "  timers:\n",
      "    learn_throughput: 2066.691\n",
      "    learn_time_ms: 1935.461\n",
      "    load_throughput: 35142890.658\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 994.604\n",
      "    sample_time_ms: 4021.701\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1671818322\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:47 (running for 00:13:01.14)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         770.817</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">  100.24</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 1451.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 101.43433429837191\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 519\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9832863807678223\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004306313116103411\n",
      "          model: {}\n",
      "          policy_loss: -0.021216535940766335\n",
      "          total_loss: 49.465518951416016\n",
      "          vf_explained_var: 0.5097894072532654\n",
      "          vf_loss: 49.48237609863281\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.71666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904491522654834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187041977237091\n",
      "    mean_inference_ms: 0.5260234892885487\n",
      "    mean_raw_obs_processing_ms: 0.07056162717787956\n",
      "  time_since_restore: 774.8638536930084\n",
      "  time_this_iter_s: 4.046861171722412\n",
      "  time_total_s: 774.8638536930084\n",
      "  timers:\n",
      "    learn_throughput: 2062.418\n",
      "    learn_time_ms: 1939.471\n",
      "    load_throughput: 35231448.971\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 994.997\n",
      "    sample_time_ms: 4020.113\n",
      "    update_time_ms: 1.424\n",
      "  timestamp: 1671818330\n",
      "  timesteps_since_restore: 632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:52 (running for 00:13:06.25)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         774.864</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> 101.434</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1451.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-58\n",
      "  done: false\n",
      "  episode_len_mean: 1466.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 104.73193088521928\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 523\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.263923406600952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015270866453647614\n",
      "          model: {}\n",
      "          policy_loss: -0.036166224628686905\n",
      "          total_loss: 4.279318332672119\n",
      "          vf_explained_var: 0.573413074016571\n",
      "          vf_loss: 4.307753562927246\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.18333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09046305248059394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31870525125412635\n",
      "    mean_inference_ms: 0.5261070717626054\n",
      "    mean_raw_obs_processing_ms: 0.07056570025268895\n",
      "  time_since_restore: 782.8214087486267\n",
      "  time_this_iter_s: 3.960432529449463\n",
      "  time_total_s: 782.8214087486267\n",
      "  timers:\n",
      "    learn_throughput: 2062.162\n",
      "    learn_time_ms: 1939.712\n",
      "    load_throughput: 35069431.438\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 995.878\n",
      "    sample_time_ms: 4016.557\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1671818338\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:58 (running for 00:13:12.21)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         782.821</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> 104.732</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:03 (running for 00:13:17.26)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         786.817</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\"> 107.611</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-06\n",
      "  done: false\n",
      "  episode_len_mean: 1481.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.10053406310233\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 529\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0093767642974854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01552146952599287\n",
      "          model: {}\n",
      "          policy_loss: -0.02709922567009926\n",
      "          total_loss: 4.140784740447998\n",
      "          vf_explained_var: 0.49240657687187195\n",
      "          vf_loss: 4.1600260734558105\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.6\n",
      "    ram_util_percent: 29.116666666666664\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904913419695422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31876409963172736\n",
      "    mean_inference_ms: 0.5261917979684595\n",
      "    mean_raw_obs_processing_ms: 0.07057983689547226\n",
      "  time_since_restore: 790.7967224121094\n",
      "  time_this_iter_s: 3.979743719100952\n",
      "  time_total_s: 790.7967224121094\n",
      "  timers:\n",
      "    learn_throughput: 2064.864\n",
      "    learn_time_ms: 1937.173\n",
      "    load_throughput: 35062102.403\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 996.352\n",
      "    sample_time_ms: 4014.646\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1671818346\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:09 (running for 00:13:23.22)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         790.797</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\"> 108.101</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-14\n",
      "  done: false\n",
      "  episode_len_mean: 1481.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.69614895957615\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 533\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.972563624382019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01606319472193718\n",
      "          model: {}\n",
      "          policy_loss: -0.03846196085214615\n",
      "          total_loss: 2.9623382091522217\n",
      "          vf_explained_var: 0.6525730490684509\n",
      "          vf_loss: 2.992668390274048\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.8\n",
      "    ram_util_percent: 29.1\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09050595957707232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31877946676959834\n",
      "    mean_inference_ms: 0.5262478018596556\n",
      "    mean_raw_obs_processing_ms: 0.0705855764898176\n",
      "  time_since_restore: 798.6506869792938\n",
      "  time_this_iter_s: 3.896947145462036\n",
      "  time_total_s: 798.6506869792938\n",
      "  timers:\n",
      "    learn_throughput: 2063.967\n",
      "    learn_time_ms: 1938.016\n",
      "    load_throughput: 35164988.472\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 997.966\n",
      "    sample_time_ms: 4008.151\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1671818354\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:15 (running for 00:13:29.15)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         798.651</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> 108.696</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:21 (running for 00:13:35.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         802.644</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> 108.425</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 1480.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.48519569951101\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 539\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.140388250350952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017859743908047676\n",
      "          model: {}\n",
      "          policy_loss: -0.03953013941645622\n",
      "          total_loss: 3.592892646789551\n",
      "          vf_explained_var: 0.5088661313056946\n",
      "          vf_loss: 3.6233813762664795\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.083333333333336\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905256077739236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31879683787417584\n",
      "    mean_inference_ms: 0.5263186908934042\n",
      "    mean_raw_obs_processing_ms: 0.0705929866882034\n",
      "  time_since_restore: 806.6391081809998\n",
      "  time_this_iter_s: 3.9950547218322754\n",
      "  time_total_s: 806.6391081809998\n",
      "  timers:\n",
      "    learn_throughput: 2066.962\n",
      "    learn_time_ms: 1935.207\n",
      "    load_throughput: 35172360.587\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 998.606\n",
      "    sample_time_ms: 4005.586\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671818362\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:26 (running for 00:13:40.17)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         810.647</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> 108.626</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 1466.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 106.66917669276721\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 544\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7537689208984375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018452072516083717\n",
      "          model: {}\n",
      "          policy_loss: -0.024161245673894882\n",
      "          total_loss: 18.26736831665039\n",
      "          vf_explained_var: 0.5194090604782104\n",
      "          vf_loss: 18.28218650817871\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.550000000000004\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053735406430505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187798085477536\n",
      "    mean_inference_ms: 0.5263811552643191\n",
      "    mean_raw_obs_processing_ms: 0.07059400759641116\n",
      "  time_since_restore: 814.6282787322998\n",
      "  time_this_iter_s: 3.9817054271698\n",
      "  time_total_s: 814.6282787322998\n",
      "  timers:\n",
      "    learn_throughput: 2074.156\n",
      "    learn_time_ms: 1928.495\n",
      "    load_throughput: 35216658.27\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 999.4\n",
      "    sample_time_ms: 4002.4\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671818370\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:31 (running for 00:13:45.18)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         814.628</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> 106.669</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:36 (running for 00:13:50.22)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         818.605</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> 107.758</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 1480.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 110.35076007801894\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 550\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9181017875671387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014733814634382725\n",
      "          model: {}\n",
      "          policy_loss: -0.0371541827917099\n",
      "          total_loss: 3.881101369857788\n",
      "          vf_explained_var: 0.5383106470108032\n",
      "          vf_loss: 3.910796642303467\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09055357336088968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31878708061463173\n",
      "    mean_inference_ms: 0.5264362797737904\n",
      "    mean_raw_obs_processing_ms: 0.07059913173512816\n",
      "  time_since_restore: 822.5185160636902\n",
      "  time_this_iter_s: 3.9136862754821777\n",
      "  time_total_s: 822.5185160636902\n",
      "  timers:\n",
      "    learn_throughput: 2074.542\n",
      "    learn_time_ms: 1928.137\n",
      "    load_throughput: 35424864.865\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1002.563\n",
      "    sample_time_ms: 3989.773\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1671818378\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:42 (running for 00:13:56.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         826.508</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> 110.068</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 1465.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.30934537967141\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 555\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.776754379272461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012994810938835144\n",
      "          model: {}\n",
      "          policy_loss: -0.025578750297427177\n",
      "          total_loss: 33.37646484375\n",
      "          vf_explained_var: 0.6455994844436646\n",
      "          vf_loss: 33.39546203613281\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09056516119974745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187866348534209\n",
      "    mean_inference_ms: 0.5264742432615271\n",
      "    mean_raw_obs_processing_ms: 0.07060209768340403\n",
      "  time_since_restore: 830.4624907970428\n",
      "  time_this_iter_s: 3.954704999923706\n",
      "  time_total_s: 830.4624907970428\n",
      "  timers:\n",
      "    learn_throughput: 2074.11\n",
      "    learn_time_ms: 1928.538\n",
      "    load_throughput: 35552481.458\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1003.352\n",
      "    sample_time_ms: 3986.636\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671818386\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:47 (running for 00:14:01.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         830.462</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> 108.309</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1465.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:52 (running for 00:14:06.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         834.484</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\"> 111.254</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 1480.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 111.988369270871\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 561\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.834228277206421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01627211645245552\n",
      "          model: {}\n",
      "          policy_loss: -0.03025376982986927\n",
      "          total_loss: 2.6952555179595947\n",
      "          vf_explained_var: 0.7106389999389648\n",
      "          vf_loss: 2.717271566390991\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.0\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058059641339135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3188046131661466\n",
      "    mean_inference_ms: 0.5264979168704504\n",
      "    mean_raw_obs_processing_ms: 0.0706075590408524\n",
      "  time_since_restore: 838.4072070121765\n",
      "  time_this_iter_s: 3.9231433868408203\n",
      "  time_total_s: 838.4072070121765\n",
      "  timers:\n",
      "    learn_throughput: 2072.803\n",
      "    learn_time_ms: 1929.754\n",
      "    load_throughput: 31512426.747\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1001.751\n",
      "    sample_time_ms: 3993.009\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671818394\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:58 (running for 00:14:12.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         842.335</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\"> 112.288</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 1480.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 112.5882183347492\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 565\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8180304765701294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015471341088414192\n",
      "          model: {}\n",
      "          policy_loss: -0.03155018761754036\n",
      "          total_loss: 1.9245480298995972\n",
      "          vf_explained_var: 0.7282019257545471\n",
      "          vf_loss: 1.9482659101486206\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.949999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058446192012916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187810892418695\n",
      "    mean_inference_ms: 0.5264982096348566\n",
      "    mean_raw_obs_processing_ms: 0.07060448835664604\n",
      "  time_since_restore: 846.2946693897247\n",
      "  time_this_iter_s: 3.9592814445495605\n",
      "  time_total_s: 846.2946693897247\n",
      "  timers:\n",
      "    learn_throughput: 2072.149\n",
      "    learn_time_ms: 1930.363\n",
      "    load_throughput: 29449211.866\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1003.616\n",
      "    sample_time_ms: 3985.588\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818402\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:04 (running for 00:14:18.00)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         846.295</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\"> 112.588</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:09 (running for 00:14:23.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         850.267</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> 116.241</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1503.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 1503.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 116.55379401048896\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 571\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9036041498184204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016182834282517433\n",
      "          model: {}\n",
      "          policy_loss: -0.03180156648159027\n",
      "          total_loss: 2.4185869693756104\n",
      "          vf_explained_var: 0.6380742788314819\n",
      "          vf_loss: 2.4421958923339844\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.15\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058909008943206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31874202263951706\n",
      "    mean_inference_ms: 0.5264926271200592\n",
      "    mean_raw_obs_processing_ms: 0.07059898169109878\n",
      "  time_since_restore: 854.2399389743805\n",
      "  time_this_iter_s: 3.972935199737549\n",
      "  time_total_s: 854.2399389743805\n",
      "  timers:\n",
      "    learn_throughput: 2068.367\n",
      "    learn_time_ms: 1933.893\n",
      "    load_throughput: 27786048.36\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 1005.257\n",
      "    sample_time_ms: 3979.084\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1671818410\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:15 (running for 00:14:28.96)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         858.191</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\"> 117.298</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1503.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 1504.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 117.94291416281503\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 576\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8034014701843262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012455887161195278\n",
      "          model: {}\n",
      "          policy_loss: -0.030662240460515022\n",
      "          total_loss: 94.1190414428711\n",
      "          vf_explained_var: 0.2867136597633362\n",
      "          vf_loss: 94.14339447021484\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.86\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059190324643213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187038555524555\n",
      "    mean_inference_ms: 0.526480747905874\n",
      "    mean_raw_obs_processing_ms: 0.07059180751620692\n",
      "  time_since_restore: 862.1637780666351\n",
      "  time_this_iter_s: 3.9727847576141357\n",
      "  time_total_s: 862.1637780666351\n",
      "  timers:\n",
      "    learn_throughput: 2069.059\n",
      "    learn_time_ms: 1933.246\n",
      "    load_throughput: 26677080.617\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 1003.909\n",
      "    sample_time_ms: 3984.426\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818418\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:20 (running for 00:14:33.98)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         862.164</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\"> 117.943</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1504.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 1504.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 119.3531006627639\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 582\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8471916913986206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016527485102415085\n",
      "          model: {}\n",
      "          policy_loss: -0.031330883502960205\n",
      "          total_loss: 4.180943489074707\n",
      "          vf_explained_var: 0.5633950233459473\n",
      "          vf_loss: 4.203907489776611\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.550000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059446984571924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31865831650570337\n",
      "    mean_inference_ms: 0.5264641335649449\n",
      "    mean_raw_obs_processing_ms: 0.07058383853585332\n",
      "  time_since_restore: 870.082897901535\n",
      "  time_this_iter_s: 3.9546597003936768\n",
      "  time_total_s: 870.082897901535\n",
      "  timers:\n",
      "    learn_throughput: 2069.023\n",
      "    learn_time_ms: 1933.28\n",
      "    load_throughput: 26622050.143\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 1004.564\n",
      "    sample_time_ms: 3981.829\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671818426\n",
      "  timesteps_since_restore: 728000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:26 (running for 00:14:39.91)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         870.083</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\"> 119.353</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1504.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:31 (running for 00:14:44.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         874.051</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">  121.98</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">            1516.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-34\n",
      "  done: false\n",
      "  episode_len_mean: 1516.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 122.18547219602313\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 586\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.926087737083435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012157624587416649\n",
      "          model: {}\n",
      "          policy_loss: -0.03196243941783905\n",
      "          total_loss: 3.8844289779663086\n",
      "          vf_explained_var: 0.6053617000579834\n",
      "          vf_loss: 3.9071593284606934\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.21666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059593612165955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31862850855476105\n",
      "    mean_inference_ms: 0.5264518862128275\n",
      "    mean_raw_obs_processing_ms: 0.07057819592741867\n",
      "  time_since_restore: 878.0074260234833\n",
      "  time_this_iter_s: 3.9566478729248047\n",
      "  time_total_s: 878.0074260234833\n",
      "  timers:\n",
      "    learn_throughput: 2072.032\n",
      "    learn_time_ms: 1930.473\n",
      "    load_throughput: 29475080.815\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1004.301\n",
      "    sample_time_ms: 3982.87\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1671818434\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:37 (running for 00:14:50.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         878.007</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\"> 122.185</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">            1516.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 1486.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 118.3570604541922\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 594\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.752101182937622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013806791976094246\n",
      "          model: {}\n",
      "          policy_loss: -0.03381708264350891\n",
      "          total_loss: 43.44443130493164\n",
      "          vf_explained_var: 0.027147194370627403\n",
      "          vf_loss: 43.46776580810547\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.519999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059776396599885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.318569735323229\n",
      "    mean_inference_ms: 0.5264228995532331\n",
      "    mean_raw_obs_processing_ms: 0.07056633370940191\n",
      "  time_since_restore: 885.9561908245087\n",
      "  time_this_iter_s: 3.961832284927368\n",
      "  time_total_s: 885.9561908245087\n",
      "  timers:\n",
      "    learn_throughput: 2071.082\n",
      "    learn_time_ms: 1931.358\n",
      "    load_throughput: 31553913.861\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1003.637\n",
      "    sample_time_ms: 3985.503\n",
      "    update_time_ms: 1.408\n",
      "  timestamp: 1671818442\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:42 (running for 00:14:55.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         885.956</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\"> 118.357</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1486.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:48 (running for 00:15:01.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          889.96</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\"> 116.262</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 1471.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 116.57366809175647\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 599\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9909077882766724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010573149658739567\n",
      "          model: {}\n",
      "          policy_loss: -0.03185923770070076\n",
      "          total_loss: 2.889963150024414\n",
      "          vf_explained_var: 0.5293506979942322\n",
      "          vf_loss: 2.9137935638427734\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.25000000000001\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09060049494254377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3185562663642656\n",
      "    mean_inference_ms: 0.5263837595468669\n",
      "    mean_raw_obs_processing_ms: 0.07056200636428528\n",
      "  time_since_restore: 893.9568929672241\n",
      "  time_this_iter_s: 3.996941566467285\n",
      "  time_total_s: 893.9568929672241\n",
      "  timers:\n",
      "    learn_throughput: 2071.904\n",
      "    learn_time_ms: 1930.591\n",
      "    load_throughput: 33743395.012\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 1002.248\n",
      "    sample_time_ms: 3991.027\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1671818450\n",
      "  timesteps_since_restore: 752000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:53 (running for 00:15:06.93)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         893.957</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\"> 116.574</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:58 (running for 00:15:11.95)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          897.98</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\"> 116.692</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 1475.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 119.03418156623087\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 606\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.803322672843933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016566909849643707\n",
      "          model: {}\n",
      "          policy_loss: -0.02549314685165882\n",
      "          total_loss: 10.045744895935059\n",
      "          vf_explained_var: 0.5733997225761414\n",
      "          vf_loss: 10.058658599853516\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.22\n",
      "    ram_util_percent: 29.24\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059969853185117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184989378979168\n",
      "    mean_inference_ms: 0.5263445664821316\n",
      "    mean_raw_obs_processing_ms: 0.07055080584504098\n",
      "  time_since_restore: 902.0137250423431\n",
      "  time_this_iter_s: 4.033287525177002\n",
      "  time_total_s: 902.0137250423431\n",
      "  timers:\n",
      "    learn_throughput: 2058.743\n",
      "    learn_time_ms: 1942.933\n",
      "    load_throughput: 35507335.45\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1000.209\n",
      "    sample_time_ms: 3999.163\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1671818458\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:03 (running for 00:15:17.06)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         906.054</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\"> 121.525</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1485.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 1485.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 121.82626207060939\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 610\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.841288685798645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010425618849694729\n",
      "          model: {}\n",
      "          policy_loss: -0.0314665213227272\n",
      "          total_loss: 2.797149181365967\n",
      "          vf_explained_var: 0.5720115900039673\n",
      "          vf_loss: 2.8206984996795654\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.233333333333334\n",
      "    ram_util_percent: 29.25\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09060004873950744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31846940712546506\n",
      "    mean_inference_ms: 0.5263259442314299\n",
      "    mean_raw_obs_processing_ms: 0.07054488348841313\n",
      "  time_since_restore: 910.0619647502899\n",
      "  time_this_iter_s: 4.007719039916992\n",
      "  time_total_s: 910.0619647502899\n",
      "  timers:\n",
      "    learn_throughput: 2056.266\n",
      "    learn_time_ms: 1945.274\n",
      "    load_throughput: 34879866.944\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 995.412\n",
      "    sample_time_ms: 4018.435\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1671818466\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:08 (running for 00:15:22.09)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         910.062</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\"> 121.826</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1485.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:13 (running for 00:15:27.14)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         914.063</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\"> 114.578</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1448.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 1443.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 114.35580726549148\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 618\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8702466487884521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00830814614892006\n",
      "          model: {}\n",
      "          policy_loss: -0.032873667776584625\n",
      "          total_loss: 62.43569564819336\n",
      "          vf_explained_var: -0.05174282565712929\n",
      "          vf_loss: 62.46225357055664\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059933871446325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184087246800651\n",
      "    mean_inference_ms: 0.5262827400018135\n",
      "    mean_raw_obs_processing_ms: 0.07053257983933549\n",
      "  time_since_restore: 918.0408627986908\n",
      "  time_this_iter_s: 3.9775795936584473\n",
      "  time_total_s: 918.0408627986908\n",
      "  timers:\n",
      "    learn_throughput: 2050.138\n",
      "    learn_time_ms: 1951.088\n",
      "    load_throughput: 33777362.593\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 995.368\n",
      "    sample_time_ms: 4018.613\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1671818474\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:19 (running for 00:15:33.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">          922.02</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\"> 111.768</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 1435.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 112.40180741662654\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 624\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7632206678390503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012623617425560951\n",
      "          model: {}\n",
      "          policy_loss: -0.0342831090092659\n",
      "          total_loss: 10.354460716247559\n",
      "          vf_explained_var: 0.32655295729637146\n",
      "          vf_loss: 10.379156112670898\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059784362501871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31836265956490506\n",
      "    mean_inference_ms: 0.5262454871493252\n",
      "    mean_raw_obs_processing_ms: 0.07052388873282874\n",
      "  time_since_restore: 925.9963090419769\n",
      "  time_this_iter_s: 3.9758732318878174\n",
      "  time_total_s: 925.9963090419769\n",
      "  timers:\n",
      "    learn_throughput: 2052.089\n",
      "    learn_time_ms: 1949.233\n",
      "    load_throughput: 33804585.936\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 993.742\n",
      "    sample_time_ms: 4025.189\n",
      "    update_time_ms: 1.307\n",
      "  timestamp: 1671818482\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:24 (running for 00:15:38.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         925.996</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\"> 112.402</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 1435.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 113.71228094785818\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 629\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6843246221542358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012338604778051376\n",
      "          model: {}\n",
      "          policy_loss: -0.03559298440814018\n",
      "          total_loss: 3.518728494644165\n",
      "          vf_explained_var: 0.5752816796302795\n",
      "          vf_loss: 3.544951915740967\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.5\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059297990454476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31829483332164527\n",
      "    mean_inference_ms: 0.52622218188453\n",
      "    mean_raw_obs_processing_ms: 0.07051190506946271\n",
      "  time_since_restore: 933.9019014835358\n",
      "  time_this_iter_s: 3.95416522026062\n",
      "  time_total_s: 933.9019014835358\n",
      "  timers:\n",
      "    learn_throughput: 2054.719\n",
      "    learn_time_ms: 1946.738\n",
      "    load_throughput: 29428549.377\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 995.88\n",
      "    sample_time_ms: 4016.549\n",
      "    update_time_ms: 1.301\n",
      "  timestamp: 1671818490\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:30 (running for 00:15:44.04)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         933.902</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\"> 113.712</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:36 (running for 00:15:50.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:41 (running for 00:15:55.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:46 (running for 00:16:00.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:51 (running for 00:16:05.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:56 (running for 00:16:10.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:01 (running for 00:16:15.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:06 (running for 00:16:20.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:11 (running for 00:16:25.06)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:16 (running for 00:16:30.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:21 (running for 00:16:35.07)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:26 (running for 00:16:40.07)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:31 (running for 00:16:45.08)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:36 (running for 00:16:50.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:41 (running for 00:16:55.08)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:46 (running for 00:17:00.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:51 (running for 00:17:05.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:56 (running for 00:17:10.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:01 (running for 00:17:15.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:06 (running for 00:17:20.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:11 (running for 00:17:25.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:16 (running for 00:17:30.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:21 (running for 00:17:35.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:26 (running for 00:17:40.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:31 (running for 00:17:45.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:36 (running for 00:17:50.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:41 (running for 00:17:55.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:46 (running for 00:18:00.14)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 1399.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 108.49618049238978\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 636\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1401.37\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 185.7117156425905\n",
      "    episode_reward_mean: 122.69003269304832\n",
      "    episode_reward_min: -125.47815031323209\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 172\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 66\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 53\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 316\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 125\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 114\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 197\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 941\n",
      "      - 326\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1314\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 469\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 884\n",
      "      - 49\n",
      "      - 1600\n",
      "      - 116\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 552\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 43\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 149.06631515961195\n",
      "      - 165.36630016202412\n",
      "      - 150.16312024965637\n",
      "      - 167.3331413615125\n",
      "      - 146.19492161818397\n",
      "      - 164.819637178761\n",
      "      - 173.25949694204462\n",
      "      - 154.66637734712705\n",
      "      - 152.3743595816807\n",
      "      - -109.67007631247925\n",
      "      - 158.44678973361337\n",
      "      - 163.29132458625\n",
      "      - 166.67461397800656\n",
      "      - 173.78336109787526\n",
      "      - -109.9757485422374\n",
      "      - 180.1468038256037\n",
      "      - 163.94747005522157\n",
      "      - 177.48298191890302\n",
      "      - 158.91257978269255\n",
      "      - 137.8840230017593\n",
      "      - -112.74218594359296\n",
      "      - 163.09915326468268\n",
      "      - 153.28971587954652\n",
      "      - 150.27729846926596\n",
      "      - 159.95573874871513\n",
      "      - 164.8039544843766\n",
      "      - -95.18881831746921\n",
      "      - 173.49529847003092\n",
      "      - 183.88083337077884\n",
      "      - 148.34020111928388\n",
      "      - 166.17278561846942\n",
      "      - 173.33757019175835\n",
      "      - 169.90713591592558\n",
      "      - -125.47815031323209\n",
      "      - 169.8569929471858\n",
      "      - 176.5879855119103\n",
      "      - 165.49049298103725\n",
      "      - 158.73943595305113\n",
      "      - 177.9899867382664\n",
      "      - 165.2061003097217\n",
      "      - 143.93231822310818\n",
      "      - -122.06450227405566\n",
      "      - 147.46065465523552\n",
      "      - 172.97328595418261\n",
      "      - -97.61214504390087\n",
      "      - 171.6531890222104\n",
      "      - 174.20239245675552\n",
      "      - 164.7151735041841\n",
      "      - 155.32967759781673\n",
      "      - -5.698755377249128\n",
      "      - -81.20216972100175\n",
      "      - 173.2634460884432\n",
      "      - 175.6528004911764\n",
      "      - 157.77365415387706\n",
      "      - 142.24665636856534\n",
      "      - 170.70217431535244\n",
      "      - 169.14053756063615\n",
      "      - 153.36254129771947\n",
      "      - 164.7672906334058\n",
      "      - 168.67513054751296\n",
      "      - 185.7117156425905\n",
      "      - 161.22175307947995\n",
      "      - 165.71426000720479\n",
      "      - 163.38755497342655\n",
      "      - 158.35018650532925\n",
      "      - 156.58624747373918\n",
      "      - 162.05757006329534\n",
      "      - 163.20626224210739\n",
      "      - 152.0132946290787\n",
      "      - 28.540362135918258\n",
      "      - 149.1325222966744\n",
      "      - 161.99260957729118\n",
      "      - 167.5039522758472\n",
      "      - 178.3918964123676\n",
      "      - 169.66908968050018\n",
      "      - -98.2572885084109\n",
      "      - 158.9066725337994\n",
      "      - 137.104935453105\n",
      "      - 154.65999270550267\n",
      "      - 152.83880013596007\n",
      "      - -13.295182696641078\n",
      "      - -109.15990223238245\n",
      "      - 162.47585706223936\n",
      "      - -120.45624262170742\n",
      "      - 166.02330797171376\n",
      "      - 155.90546083689716\n",
      "      - 159.4734666096223\n",
      "      - 157.61934289224695\n",
      "      - -66.5250102512706\n",
      "      - 156.677045571681\n",
      "      - 151.68979673795351\n",
      "      - 162.7835029901353\n",
      "      - 152.1290673296426\n",
      "      - 169.63393734791634\n",
      "      - 137.70206295110657\n",
      "      - 167.49980648779163\n",
      "      - 149.08656887335636\n",
      "      - -110.28800771718348\n",
      "      - 169.8426518430852\n",
      "      - 170.99068142432458\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08663718695203562\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.30769731701085673\n",
      "      mean_inference_ms: 0.4996862664130859\n",
      "      mean_raw_obs_processing_ms: 0.05997848157144513\n",
      "    timesteps_this_iter: 140137\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4528028964996338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01978171244263649\n",
      "          model: {}\n",
      "          policy_loss: -0.03475615754723549\n",
      "          total_loss: 66.0904541015625\n",
      "          vf_explained_var: 0.4531557261943817\n",
      "          vf_loss: 66.11019134521484\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.52061855670103\n",
      "    ram_util_percent: 29.191237113402053\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059690969283768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31829943801769645\n",
      "    mean_inference_ms: 0.5261531093170988\n",
      "    mean_raw_obs_processing_ms: 0.07051084035234906\n",
      "  time_since_restore: 1074.2548213005066\n",
      "  time_this_iter_s: 136.41759324073792\n",
      "  time_total_s: 1074.2548213005066\n",
      "  timers:\n",
      "    learn_throughput: 2066.922\n",
      "    learn_time_ms: 1935.245\n",
      "    load_throughput: 29495808.72\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 998.478\n",
      "    sample_time_ms: 4006.096\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671818631\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:52 (running for 00:18:05.46)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         1074.25</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\"> 108.496</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1399.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:57 (running for 00:18:10.58)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         1078.36</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\"> 110.617</td><td style=\"text-align: right;\">              174.37</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1400.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-03-59\n",
      "  done: false\n",
      "  episode_len_mean: 1399.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 174.3699093596896\n",
      "  episode_reward_mean: 109.84156250635111\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 641\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.627944827079773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013459350913763046\n",
      "          model: {}\n",
      "          policy_loss: -0.030898146331310272\n",
      "          total_loss: 81.63282012939453\n",
      "          vf_explained_var: 0.34929969906806946\n",
      "          vf_loss: 81.65350341796875\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.7\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059270609189732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31823588612001713\n",
      "    mean_inference_ms: 0.5261377788089274\n",
      "    mean_raw_obs_processing_ms: 0.07050053528069006\n",
      "  time_since_restore: 1082.3728258609772\n",
      "  time_this_iter_s: 4.0164995193481445\n",
      "  time_total_s: 1082.3728258609772\n",
      "  timers:\n",
      "    learn_throughput: 2063.365\n",
      "    learn_time_ms: 1938.581\n",
      "    load_throughput: 29274500.087\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 231.866\n",
      "    sample_time_ms: 17251.378\n",
      "    update_time_ms: 1.321\n",
      "  timestamp: 1671818639\n",
      "  timesteps_since_restore: 808000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:02 (running for 00:18:15.62)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         1082.37</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\"> 109.842</td><td style=\"text-align: right;\">              174.37</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1399.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-07\n",
      "  done: false\n",
      "  episode_len_mean: 1413.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 174.3699093596896\n",
      "  episode_reward_mean: 113.62615159373904\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 646\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6323111057281494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012912311591207981\n",
      "          model: {}\n",
      "          policy_loss: -0.034840963780879974\n",
      "          total_loss: 4.1058502197265625\n",
      "          vf_explained_var: 0.5010325312614441\n",
      "          vf_loss: 4.130885601043701\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.86\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905975485597752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3182562622018304\n",
      "    mean_inference_ms: 0.5260859795193377\n",
      "    mean_raw_obs_processing_ms: 0.07050308576116841\n",
      "  time_since_restore: 1090.3315000534058\n",
      "  time_this_iter_s: 3.9613938331604004\n",
      "  time_total_s: 1090.3315000534058\n",
      "  timers:\n",
      "    learn_throughput: 2066.836\n",
      "    learn_time_ms: 1935.326\n",
      "    load_throughput: 29172693.445\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 231.821\n",
      "    sample_time_ms: 17254.705\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1671818647\n",
      "  timesteps_since_restore: 816000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:08 (running for 00:18:21.61)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         1090.33</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\"> 113.626</td><td style=\"text-align: right;\">              174.37</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1413.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:13 (running for 00:18:26.61)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         1094.31</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\"> 114.134</td><td style=\"text-align: right;\">              174.37</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1413.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 1413.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 174.77476333284736\n",
      "  episode_reward_mean: 114.92838934800933\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 651\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7500083446502686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013021019287407398\n",
      "          model: {}\n",
      "          policy_loss: -0.03527991846203804\n",
      "          total_loss: 5.4613165855407715\n",
      "          vf_explained_var: 0.4719953238964081\n",
      "          vf_loss: 5.486708164215088\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.18333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059336875184537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31819250772348434\n",
      "    mean_inference_ms: 0.526071505268029\n",
      "    mean_raw_obs_processing_ms: 0.07049238073023763\n",
      "  time_since_restore: 1098.259015083313\n",
      "  time_this_iter_s: 3.945863723754883\n",
      "  time_total_s: 1098.259015083313\n",
      "  timers:\n",
      "    learn_throughput: 2065.739\n",
      "    learn_time_ms: 1936.353\n",
      "    load_throughput: 29056487.703\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 231.927\n",
      "    sample_time_ms: 17246.773\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1671818655\n",
      "  timesteps_since_restore: 824000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:19 (running for 00:18:32.56)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         1102.22</td><td style=\"text-align: right;\">828000</td><td style=\"text-align: right;\"> 114.099</td><td style=\"text-align: right;\">             174.775</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1413.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 1429.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 175.23345990369415\n",
      "  episode_reward_mean: 116.98339783762712\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 656\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5802336931228638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012906422838568687\n",
      "          model: {}\n",
      "          policy_loss: -0.035336531698703766\n",
      "          total_loss: 6.766406059265137\n",
      "          vf_explained_var: 0.4604024887084961\n",
      "          vf_loss: 6.7919416427612305\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059206174254691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3181559081846771\n",
      "    mean_inference_ms: 0.5260439469451608\n",
      "    mean_raw_obs_processing_ms: 0.07048618876297014\n",
      "  time_since_restore: 1106.1729898452759\n",
      "  time_this_iter_s: 3.9481799602508545\n",
      "  time_total_s: 1106.1729898452759\n",
      "  timers:\n",
      "    learn_throughput: 2065.891\n",
      "    learn_time_ms: 1936.21\n",
      "    load_throughput: 33241957.599\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 231.895\n",
      "    sample_time_ms: 17249.172\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671818663\n",
      "  timesteps_since_restore: 832000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:25 (running for 00:18:38.53)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         1106.17</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\"> 116.983</td><td style=\"text-align: right;\">             175.233</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1429.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:30 (running for 00:18:43.54)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         1110.13</td><td style=\"text-align: right;\">836000</td><td style=\"text-align: right;\"> 117.717</td><td style=\"text-align: right;\">              175.38</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1429.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 1429.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 177.05414952313467\n",
      "  episode_reward_mean: 118.3094136356717\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 661\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5045819282531738\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016324857249855995\n",
      "          model: {}\n",
      "          policy_loss: -0.03339763730764389\n",
      "          total_loss: 3.167414665222168\n",
      "          vf_explained_var: 0.6334409713745117\n",
      "          vf_loss: 3.188415765762329\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.56\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058702108281963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.318089272257226\n",
      "    mean_inference_ms: 0.5260251897950968\n",
      "    mean_raw_obs_processing_ms: 0.070474819053564\n",
      "  time_since_restore: 1114.0554463863373\n",
      "  time_this_iter_s: 3.9220073223114014\n",
      "  time_total_s: 1114.0554463863373\n",
      "  timers:\n",
      "    learn_throughput: 2064.412\n",
      "    learn_time_ms: 1937.598\n",
      "    load_throughput: 33182784.81\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 231.958\n",
      "    sample_time_ms: 17244.484\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671818671\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:36 (running for 00:18:49.45)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         1118.03</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\"> 119.063</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1429.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 1429.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 185.52681446943214\n",
      "  episode_reward_mean: 119.3915789722955\n",
      "  episode_reward_min: -123.9389057140009\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 666\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4887526035308838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012916810810565948\n",
      "          model: {}\n",
      "          policy_loss: -0.04163338616490364\n",
      "          total_loss: 3.9306278228759766\n",
      "          vf_explained_var: 0.6246837973594666\n",
      "          vf_loss: 3.9624526500701904\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.13333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058892059383382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3180830163804259\n",
      "    mean_inference_ms: 0.5259875314921945\n",
      "    mean_raw_obs_processing_ms: 0.07047335589730915\n",
      "  time_since_restore: 1122.0072910785675\n",
      "  time_this_iter_s: 3.974412679672241\n",
      "  time_total_s: 1122.0072910785675\n",
      "  timers:\n",
      "    learn_throughput: 2068.307\n",
      "    learn_time_ms: 1933.949\n",
      "    load_throughput: 33380851.572\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1004.49\n",
      "    sample_time_ms: 3982.119\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1671818679\n",
      "  timesteps_since_restore: 848000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:42 (running for 00:18:55.45)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         1122.01</td><td style=\"text-align: right;\">848000</td><td style=\"text-align: right;\"> 119.392</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -123.939</td><td style=\"text-align: right;\">           1429.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-47\n",
      "  done: false\n",
      "  episode_len_mean: 1393.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 185.52681446943214\n",
      "  episode_reward_mean: 114.68480211288892\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 674\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8137831687927246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012608184479176998\n",
      "          model: {}\n",
      "          policy_loss: -0.03517071157693863\n",
      "          total_loss: 74.56105041503906\n",
      "          vf_explained_var: 0.12978100776672363\n",
      "          vf_loss: 74.58663940429688\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.949999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058701796687876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3180301229304454\n",
      "    mean_inference_ms: 0.5259470594818814\n",
      "    mean_raw_obs_processing_ms: 0.07046533898193293\n",
      "  time_since_restore: 1129.9615564346313\n",
      "  time_this_iter_s: 3.943645477294922\n",
      "  time_total_s: 1129.9615564346313\n",
      "  timers:\n",
      "    learn_throughput: 2066.283\n",
      "    learn_time_ms: 1935.843\n",
      "    load_throughput: 32232883.766\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1004.273\n",
      "    sample_time_ms: 3982.981\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1671818687\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:48 (running for 00:19:01.44)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         1129.96</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\"> 114.685</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1393.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:53 (running for 00:19:06.44)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         1133.94</td><td style=\"text-align: right;\">860000</td><td style=\"text-align: right;\"> 114.867</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1391.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-04-55\n",
      "  done: false\n",
      "  episode_len_mean: 1361.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 185.52681446943214\n",
      "  episode_reward_mean: 110.12013726276862\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 682\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4566895961761475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013560023158788681\n",
      "          model: {}\n",
      "          policy_loss: -0.027117980644106865\n",
      "          total_loss: 18.54791259765625\n",
      "          vf_explained_var: 0.33979010581970215\n",
      "          vf_loss: 18.56473731994629\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.56666666666666\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058538422569501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31798029147363943\n",
      "    mean_inference_ms: 0.525909104550432\n",
      "    mean_raw_obs_processing_ms: 0.07045912899126737\n",
      "  time_since_restore: 1137.9627730846405\n",
      "  time_this_iter_s: 4.018096685409546\n",
      "  time_total_s: 1137.9627730846405\n",
      "  timers:\n",
      "    learn_throughput: 2063.113\n",
      "    learn_time_ms: 1938.817\n",
      "    load_throughput: 30492940.749\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1003.161\n",
      "    sample_time_ms: 3987.397\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1671818695\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:04:58 (running for 00:19:11.48)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         1137.96</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\">  110.12</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1361.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 1355.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 185.52681446943214\n",
      "  episode_reward_mean: 108.83938896013557\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 687\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5169562101364136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014285476878285408\n",
      "          model: {}\n",
      "          policy_loss: -0.04049955680966377\n",
      "          total_loss: 4.214131832122803\n",
      "          vf_explained_var: 0.5339968204498291\n",
      "          vf_loss: 4.243783473968506\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.68333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058446595341495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3179519896833749\n",
      "    mean_inference_ms: 0.5258892899995903\n",
      "    mean_raw_obs_processing_ms: 0.07045651143316635\n",
      "  time_since_restore: 1145.9086437225342\n",
      "  time_this_iter_s: 3.915259838104248\n",
      "  time_total_s: 1145.9086437225342\n",
      "  timers:\n",
      "    learn_throughput: 2060.99\n",
      "    learn_time_ms: 1940.815\n",
      "    load_throughput: 28846657.497\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 1001.707\n",
      "    sample_time_ms: 3993.184\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671818703\n",
      "  timesteps_since_restore: 872000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:04 (running for 00:19:17.47)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         1145.91</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\"> 108.839</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1355.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:09 (running for 00:19:22.59)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         1149.97</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\"> 111.587</td><td style=\"text-align: right;\">             185.527</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">            1370.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 1385.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 186.06446534564137\n",
      "  episode_reward_mean: 114.99480003127114\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 692\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4446711540222168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01056183222681284\n",
      "          model: {}\n",
      "          policy_loss: -0.06637992709875107\n",
      "          total_loss: 6.97886323928833\n",
      "          vf_explained_var: 0.4930166006088257\n",
      "          vf_loss: 7.037222385406494\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.660000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058407291092813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3179241509101346\n",
      "    mean_inference_ms: 0.52586944851401\n",
      "    mean_raw_obs_processing_ms: 0.07045354519586616\n",
      "  time_since_restore: 1153.9116904735565\n",
      "  time_this_iter_s: 3.942897081375122\n",
      "  time_total_s: 1153.9116904735565\n",
      "  timers:\n",
      "    learn_throughput: 2061.715\n",
      "    learn_time_ms: 1940.133\n",
      "    load_throughput: 28777385.935\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 998.554\n",
      "    sample_time_ms: 4005.791\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671818711\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:15 (running for 00:19:28.52)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         1153.91</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\"> 114.995</td><td style=\"text-align: right;\">             186.064</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1385.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-19\n",
      "  done: false\n",
      "  episode_len_mean: 1381.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 189.2861129534452\n",
      "  episode_reward_mean: 114.44016446424685\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 698\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4912382364273071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007872228510677814\n",
      "          model: {}\n",
      "          policy_loss: -0.03317747637629509\n",
      "          total_loss: 72.15650177001953\n",
      "          vf_explained_var: 0.2801720201969147\n",
      "          vf_loss: 72.1836929321289\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.833333333333336\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058412332826184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31789265732288013\n",
      "    mean_inference_ms: 0.5258504111796029\n",
      "    mean_raw_obs_processing_ms: 0.07045106933622547\n",
      "  time_since_restore: 1161.9172277450562\n",
      "  time_this_iter_s: 3.943268060684204\n",
      "  time_total_s: 1161.9172277450562\n",
      "  timers:\n",
      "    learn_throughput: 2063.354\n",
      "    learn_time_ms: 1938.591\n",
      "    load_throughput: 27813687.003\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 996.659\n",
      "    sample_time_ms: 4013.408\n",
      "    update_time_ms: 1.344\n",
      "  timestamp: 1671818719\n",
      "  timesteps_since_restore: 888000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:20 (running for 00:19:33.59)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         1161.92</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\">  114.44</td><td style=\"text-align: right;\">             189.286</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1381.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:26 (running for 00:19:39.59)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         1165.92</td><td style=\"text-align: right;\">892000</td><td style=\"text-align: right;\">  113.71</td><td style=\"text-align: right;\">             191.843</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1376.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 1391.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 191.84259757799975\n",
      "  episode_reward_mean: 117.69382347615498\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 704\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.497290849685669\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015142383985221386\n",
      "          model: {}\n",
      "          policy_loss: -0.041029490530490875\n",
      "          total_loss: 7.258159160614014\n",
      "          vf_explained_var: 0.40256959199905396\n",
      "          vf_loss: 7.287690162658691\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.260000000000005\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058414283426709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31786000609252774\n",
      "    mean_inference_ms: 0.5258318298550981\n",
      "    mean_raw_obs_processing_ms: 0.07044825892883663\n",
      "  time_since_restore: 1169.8791580200195\n",
      "  time_this_iter_s: 3.956796646118164\n",
      "  time_total_s: 1169.8791580200195\n",
      "  timers:\n",
      "    learn_throughput: 2064.863\n",
      "    learn_time_ms: 1937.175\n",
      "    load_throughput: 28946197.378\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 997.285\n",
      "    sample_time_ms: 4010.89\n",
      "    update_time_ms: 1.344\n",
      "  timestamp: 1671818727\n",
      "  timesteps_since_restore: 896000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:32 (running for 00:19:45.57)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         1173.83</td><td style=\"text-align: right;\">900000</td><td style=\"text-align: right;\"> 115.665</td><td style=\"text-align: right;\">             191.843</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1376.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-35\n",
      "  done: false\n",
      "  episode_len_mean: 1371.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 191.84259757799975\n",
      "  episode_reward_mean: 114.52744790106672\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 709\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4530141353607178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009823355823755264\n",
      "          model: {}\n",
      "          policy_loss: -0.030132494866847992\n",
      "          total_loss: 76.54645538330078\n",
      "          vf_explained_var: 0.24514512717723846\n",
      "          vf_loss: 76.56912231445312\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71666666666666\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058315868236808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31782958870670286\n",
      "    mean_inference_ms: 0.525813100265548\n",
      "    mean_raw_obs_processing_ms: 0.07044511611094938\n",
      "  time_since_restore: 1177.7508313655853\n",
      "  time_this_iter_s: 3.925750494003296\n",
      "  time_total_s: 1177.7508313655853\n",
      "  timers:\n",
      "    learn_throughput: 2069.333\n",
      "    learn_time_ms: 1932.99\n",
      "    load_throughput: 30604188.252\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 998.983\n",
      "    sample_time_ms: 4004.073\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818735\n",
      "  timesteps_since_restore: 904000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:38 (running for 00:19:51.48)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         1177.75</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\"> 114.527</td><td style=\"text-align: right;\">             191.843</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1371.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 1404.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 191.84259757799975\n",
      "  episode_reward_mean: 122.68885670809122\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 715\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3323469161987305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014989539049565792\n",
      "          model: {}\n",
      "          policy_loss: -0.043060578405857086\n",
      "          total_loss: 8.598257064819336\n",
      "          vf_explained_var: 0.22086280584335327\n",
      "          vf_loss: 8.629935264587402\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058044698612804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3177854460463868\n",
      "    mean_inference_ms: 0.5257820482096666\n",
      "    mean_raw_obs_processing_ms: 0.07044027412321274\n",
      "  time_since_restore: 1185.633826494217\n",
      "  time_this_iter_s: 3.9088284969329834\n",
      "  time_total_s: 1185.633826494217\n",
      "  timers:\n",
      "    learn_throughput: 2067.71\n",
      "    learn_time_ms: 1934.507\n",
      "    load_throughput: 29542553.266\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 1001.761\n",
      "    sample_time_ms: 3992.968\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671818742\n",
      "  timesteps_since_restore: 912000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:43 (running for 00:19:57.40)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         1185.63</td><td style=\"text-align: right;\">912000</td><td style=\"text-align: right;\"> 122.689</td><td style=\"text-align: right;\">             191.843</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1404.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:49 (running for 00:20:02.53)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         1189.74</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\"> 123.181</td><td style=\"text-align: right;\">             196.737</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1404.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 1423.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 196.73737492763766\n",
      "  episode_reward_mean: 127.68785093103271\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 720\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4446159601211548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014568410813808441\n",
      "          model: {}\n",
      "          policy_loss: -0.03774072974920273\n",
      "          total_loss: 6.058254241943359\n",
      "          vf_explained_var: 0.49291637539863586\n",
      "          vf_loss: 6.08493185043335\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.0\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09057816452079058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31774777508904384\n",
      "    mean_inference_ms: 0.5257691332331951\n",
      "    mean_raw_obs_processing_ms: 0.07043549397378453\n",
      "  time_since_restore: 1193.7068252563477\n",
      "  time_this_iter_s: 3.9645214080810547\n",
      "  time_total_s: 1193.7068252563477\n",
      "  timers:\n",
      "    learn_throughput: 2064.915\n",
      "    learn_time_ms: 1937.126\n",
      "    load_throughput: 29626021.543\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 1000.393\n",
      "    sample_time_ms: 3998.429\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1671818751\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:05:54 (running for 00:20:07.57)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         1193.71</td><td style=\"text-align: right;\">920000</td><td style=\"text-align: right;\"> 127.688</td><td style=\"text-align: right;\">             196.737</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1423.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 1415.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 198.33584745967124\n",
      "  episode_reward_mean: 127.72709679002153\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 726\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1588369607925415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017440589144825935\n",
      "          model: {}\n",
      "          policy_loss: -0.025379382073879242\n",
      "          total_loss: 48.23670959472656\n",
      "          vf_explained_var: 0.47876203060150146\n",
      "          vf_loss: 48.24884033203125\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.449999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09057552056250277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3177030753251182\n",
      "    mean_inference_ms: 0.5257538233546017\n",
      "    mean_raw_obs_processing_ms: 0.0704295191361473\n",
      "  time_since_restore: 1201.6494386196136\n",
      "  time_this_iter_s: 3.920945644378662\n",
      "  time_total_s: 1201.6494386196136\n",
      "  timers:\n",
      "    learn_throughput: 2065.136\n",
      "    learn_time_ms: 1936.918\n",
      "    load_throughput: 29815560.69\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 1001.333\n",
      "    sample_time_ms: 3994.676\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671818759\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:00 (running for 00:20:13.50)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         1201.65</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\"> 127.727</td><td style=\"text-align: right;\">             198.336</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1415.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:05 (running for 00:20:18.52)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         1205.61</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\"> 128.329</td><td style=\"text-align: right;\">             199.921</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1415.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 1415.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.08919329290754\n",
      "  episode_reward_mean: 129.0372253876405\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 730\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0532292127609253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011515344493091106\n",
      "          model: {}\n",
      "          policy_loss: -0.03736228495836258\n",
      "          total_loss: 5.6366400718688965\n",
      "          vf_explained_var: 0.5529510974884033\n",
      "          vf_loss: 5.665257453918457\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.42\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09057360443428951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31767247978971386\n",
      "    mean_inference_ms: 0.5257431728257917\n",
      "    mean_raw_obs_processing_ms: 0.07042551134832734\n",
      "  time_since_restore: 1209.5438883304596\n",
      "  time_this_iter_s: 3.9300644397735596\n",
      "  time_total_s: 1209.5438883304596\n",
      "  timers:\n",
      "    learn_throughput: 2064.808\n",
      "    learn_time_ms: 1937.226\n",
      "    load_throughput: 30476323.342\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1003.544\n",
      "    sample_time_ms: 3985.874\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1671818767\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:11 (running for 00:20:24.42)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         1213.51</td><td style=\"text-align: right;\">940000</td><td style=\"text-align: right;\"> 129.953</td><td style=\"text-align: right;\">             205.468</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1415.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 1450.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 205.4677736120279\n",
      "  episode_reward_mean: 137.89627228737635\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 736\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.926683783531189\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013598710298538208\n",
      "          model: {}\n",
      "          policy_loss: -0.033213261514902115\n",
      "          total_loss: 6.113685607910156\n",
      "          vf_explained_var: 0.6993993520736694\n",
      "          vf_loss: 6.136571884155273\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.23333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09056458696825902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3175709714386621\n",
      "    mean_inference_ms: 0.5257517349474877\n",
      "    mean_raw_obs_processing_ms: 0.07041026016002534\n",
      "  time_since_restore: 1217.4623141288757\n",
      "  time_this_iter_s: 3.950068950653076\n",
      "  time_total_s: 1217.4623141288757\n",
      "  timers:\n",
      "    learn_throughput: 2064.853\n",
      "    learn_time_ms: 1937.184\n",
      "    load_throughput: 30548463.219\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1002.188\n",
      "    sample_time_ms: 3991.267\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671818774\n",
      "  timesteps_since_restore: 944000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:16 (running for 00:20:30.39)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         1217.46</td><td style=\"text-align: right;\">944000</td><td style=\"text-align: right;\"> 137.896</td><td style=\"text-align: right;\">             205.468</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1450.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 1450.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.6851682835261\n",
      "  episode_reward_mean: 138.80180866715685\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 740\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9868481755256653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013225910253822803\n",
      "          model: {}\n",
      "          policy_loss: -0.04076690226793289\n",
      "          total_loss: 4.082947254180908\n",
      "          vf_explained_var: 0.5022882223129272\n",
      "          vf_loss: 4.113670825958252\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.6\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09056491204275974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31756442068442803\n",
      "    mean_inference_ms: 0.5257218347451438\n",
      "    mean_raw_obs_processing_ms: 0.07040917693152961\n",
      "  time_since_restore: 1225.3523638248444\n",
      "  time_this_iter_s: 3.917705535888672\n",
      "  time_total_s: 1225.3523638248444\n",
      "  timers:\n",
      "    learn_throughput: 2066.69\n",
      "    learn_time_ms: 1935.462\n",
      "    load_throughput: 30738761.451\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1002.022\n",
      "    sample_time_ms: 3991.927\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671818782\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:22 (running for 00:20:36.32)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         1225.35</td><td style=\"text-align: right;\">952000</td><td style=\"text-align: right;\"> 138.802</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1450.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:28 (running for 00:20:42.29)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">          1229.3</td><td style=\"text-align: right;\">956000</td><td style=\"text-align: right;\"> 140.938</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1451.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-30\n",
      "  done: false\n",
      "  episode_len_mean: 1451.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.6851682835261\n",
      "  episode_reward_mean: 141.4178516108922\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 746\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1014089584350586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017989084124565125\n",
      "          model: {}\n",
      "          policy_loss: -0.03554762899875641\n",
      "          total_loss: 7.592430114746094\n",
      "          vf_explained_var: 0.5837175250053406\n",
      "          vf_loss: 7.614316463470459\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.42\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905569383065277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31748156001979133\n",
      "    mean_inference_ms: 0.5257053092648564\n",
      "    mean_raw_obs_processing_ms: 0.07039589794113416\n",
      "  time_since_restore: 1233.2408080101013\n",
      "  time_this_iter_s: 3.9389922618865967\n",
      "  time_total_s: 1233.2408080101013\n",
      "  timers:\n",
      "    learn_throughput: 2071.32\n",
      "    learn_time_ms: 1931.135\n",
      "    load_throughput: 28821879.402\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 1005.841\n",
      "    sample_time_ms: 3976.772\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1671818790\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:34 (running for 00:20:48.24)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         1237.21</td><td style=\"text-align: right;\">964000</td><td style=\"text-align: right;\"> 142.451</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1451.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 1451.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.6851682835261\n",
      "  episode_reward_mean: 142.8743712930969\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 750\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.04603910446167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015308528207242489\n",
      "          model: {}\n",
      "          policy_loss: -0.0287382360547781\n",
      "          total_loss: 5.410750389099121\n",
      "          vf_explained_var: 0.6450656056404114\n",
      "          vf_loss: 5.427863597869873\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.03333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09055677457805375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31747187461075477\n",
      "    mean_inference_ms: 0.5256731426150816\n",
      "    mean_raw_obs_processing_ms: 0.07039447213626064\n",
      "  time_since_restore: 1241.1811649799347\n",
      "  time_this_iter_s: 3.971419334411621\n",
      "  time_total_s: 1241.1811649799347\n",
      "  timers:\n",
      "    learn_throughput: 2071.082\n",
      "    learn_time_ms: 1931.358\n",
      "    load_throughput: 27850624.17\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 1006.988\n",
      "    sample_time_ms: 3972.242\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1671818798\n",
      "  timesteps_since_restore: 968000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:40 (running for 00:20:54.23)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         1241.18</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\"> 142.874</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1451.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-46\n",
      "  done: false\n",
      "  episode_len_mean: 1451.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.6851682835261\n",
      "  episode_reward_mean: 146.1763524335125\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 756\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0568052530288696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012863273732364178\n",
      "          model: {}\n",
      "          policy_loss: -0.032112471759319305\n",
      "          total_loss: 5.388760089874268\n",
      "          vf_explained_var: 0.5356580018997192\n",
      "          vf_loss: 5.411104679107666\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.11666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09054898018975832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.317388776600598\n",
      "    mean_inference_ms: 0.5256577354814937\n",
      "    mean_raw_obs_processing_ms: 0.07038151856211833\n",
      "  time_since_restore: 1249.1178939342499\n",
      "  time_this_iter_s: 3.958653450012207\n",
      "  time_total_s: 1249.1178939342499\n",
      "  timers:\n",
      "    learn_throughput: 2068.992\n",
      "    learn_time_ms: 1933.308\n",
      "    load_throughput: 26383418.777\n",
      "    load_time_ms: 0.152\n",
      "    sample_throughput: 1005.626\n",
      "    sample_time_ms: 3977.622\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671818806\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:46 (running for 00:21:00.24)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         1249.12</td><td style=\"text-align: right;\">976000</td><td style=\"text-align: right;\"> 146.176</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1451.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:52 (running for 00:21:06.21)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         1253.09</td><td style=\"text-align: right;\">980000</td><td style=\"text-align: right;\"> 146.535</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">           1451.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 1443.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.6851682835261\n",
      "  episode_reward_mean: 145.06921991494693\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 761\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0470184087753296\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007360894698649645\n",
      "          model: {}\n",
      "          policy_loss: -0.03212977945804596\n",
      "          total_loss: 51.86956024169922\n",
      "          vf_explained_var: 0.5544787049293518\n",
      "          vf_loss: 51.89610290527344\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.339999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09054548174276839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173452697153259\n",
      "    mean_inference_ms: 0.525636428883064\n",
      "    mean_raw_obs_processing_ms: 0.07037487304914158\n",
      "  time_since_restore: 1257.0666904449463\n",
      "  time_this_iter_s: 3.972991466522217\n",
      "  time_total_s: 1257.0666904449463\n",
      "  timers:\n",
      "    learn_throughput: 2063.276\n",
      "    learn_time_ms: 1938.664\n",
      "    load_throughput: 25130641.102\n",
      "    load_time_ms: 0.159\n",
      "    sample_throughput: 1005.273\n",
      "    sample_time_ms: 3979.02\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1671818814\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:06:57 (running for 00:21:11.25)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         1257.07</td><td style=\"text-align: right;\">984000</td><td style=\"text-align: right;\"> 145.069</td><td style=\"text-align: right;\">             206.685</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">            1443.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 1443.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.99750403259836\n",
      "  episode_reward_mean: 146.22890721982688\n",
      "  episode_reward_min: -127.60949705404106\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 766\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6418552994728088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01546668354421854\n",
      "          model: {}\n",
      "          policy_loss: -0.0377785786986351\n",
      "          total_loss: 3.631972551345825\n",
      "          vf_explained_var: 0.6529402732849121\n",
      "          vf_loss: 3.658006191253662\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.5\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09054228322772084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31730416361404074\n",
      "    mean_inference_ms: 0.5256174215797818\n",
      "    mean_raw_obs_processing_ms: 0.07036874618354065\n",
      "  time_since_restore: 1264.987965106964\n",
      "  time_this_iter_s: 3.893846273422241\n",
      "  time_total_s: 1264.987965106964\n",
      "  timers:\n",
      "    learn_throughput: 2064.115\n",
      "    learn_time_ms: 1937.876\n",
      "    load_throughput: 27051299.581\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 1003.749\n",
      "    sample_time_ms: 3985.06\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1671818822\n",
      "  timesteps_since_restore: 992000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:03 (running for 00:21:17.16)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         1264.99</td><td style=\"text-align: right;\">992000</td><td style=\"text-align: right;\"> 146.229</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">            1443.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:08 (running for 00:21:22.23)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         1268.99</td><td style=\"text-align: right;\">996000</td><td style=\"text-align: right;\"> 146.711</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -127.609</td><td style=\"text-align: right;\">            1443.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-10\n",
      "  done: false\n",
      "  episode_len_mean: 1470.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.99750403259836\n",
      "  episode_reward_mean: 152.44546916844408\n",
      "  episode_reward_min: -123.64114992791849\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 771\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8155806064605713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01565035618841648\n",
      "          model: {}\n",
      "          policy_loss: -0.0369875468313694\n",
      "          total_loss: 6.2596659660339355\n",
      "          vf_explained_var: 0.5815273523330688\n",
      "          vf_loss: 6.284769535064697\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053582953592805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31723362207409483\n",
      "    mean_inference_ms: 0.5256125264118768\n",
      "    mean_raw_obs_processing_ms: 0.07035733092718378\n",
      "  time_since_restore: 1272.9319279193878\n",
      "  time_this_iter_s: 3.9402689933776855\n",
      "  time_total_s: 1272.9319279193878\n",
      "  timers:\n",
      "    learn_throughput: 2062.093\n",
      "    learn_time_ms: 1939.776\n",
      "    load_throughput: 28811980.079\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 1003.043\n",
      "    sample_time_ms: 3987.864\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1671818830\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:14 (running for 00:21:28.15)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         1276.92</td><td style=\"text-align: right;\">1004000</td><td style=\"text-align: right;\"> 155.808</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -123.641</td><td style=\"text-align: right;\">           1483.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 1483.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.99750403259836\n",
      "  episode_reward_mean: 156.23458625212646\n",
      "  episode_reward_min: -123.64114992791849\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 776\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.697386622428894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01502902340143919\n",
      "          model: {}\n",
      "          policy_loss: -0.038421060889959335\n",
      "          total_loss: 6.764355659484863\n",
      "          vf_explained_var: 0.5834468007087708\n",
      "          vf_loss: 6.7913641929626465\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.733333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905359630013809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31722067913758845\n",
      "    mean_inference_ms: 0.5255827991764139\n",
      "    mean_raw_obs_processing_ms: 0.07035566539214234\n",
      "  time_since_restore: 1280.9107813835144\n",
      "  time_this_iter_s: 3.988520383834839\n",
      "  time_total_s: 1280.9107813835144\n",
      "  timers:\n",
      "    learn_throughput: 2062.157\n",
      "    learn_time_ms: 1939.717\n",
      "    load_throughput: 30920044.231\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1001.725\n",
      "    sample_time_ms: 3993.112\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671818838\n",
      "  timesteps_since_restore: 1008000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:19 (running for 00:21:33.20)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         1280.91</td><td style=\"text-align: right;\">1008000</td><td style=\"text-align: right;\"> 156.235</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -123.641</td><td style=\"text-align: right;\">           1483.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:25 (running for 00:21:39.15)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         1284.88</td><td style=\"text-align: right;\">1012000</td><td style=\"text-align: right;\"> 161.669</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1511.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-26\n",
      "  done: false\n",
      "  episode_len_mean: 1511.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.99750403259836\n",
      "  episode_reward_mean: 161.9577473161668\n",
      "  episode_reward_min: -120.35819745947421\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 782\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0348082780838013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014273136854171753\n",
      "          model: {}\n",
      "          policy_loss: -0.038975249975919724\n",
      "          total_loss: 5.366977691650391\n",
      "          vf_explained_var: 0.5390746593475342\n",
      "          vf_loss: 5.395113945007324\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.099999999999994\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053233762104186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171699347285693\n",
      "    mean_inference_ms: 0.5255636196890078\n",
      "    mean_raw_obs_processing_ms: 0.07034721774552606\n",
      "  time_since_restore: 1288.8491139411926\n",
      "  time_this_iter_s: 3.969209909439087\n",
      "  time_total_s: 1288.8491139411926\n",
      "  timers:\n",
      "    learn_throughput: 2064.811\n",
      "    learn_time_ms: 1937.223\n",
      "    load_throughput: 32915864.234\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 1001.5\n",
      "    sample_time_ms: 3994.008\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1671818846\n",
      "  timesteps_since_restore: 1016000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:30 (running for 00:21:44.16)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         1292.82</td><td style=\"text-align: right;\">1020000</td><td style=\"text-align: right;\"> 164.677</td><td style=\"text-align: right;\">             208.998</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1517.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-34\n",
      "  done: false\n",
      "  episode_len_mean: 1517.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 210.9513228339365\n",
      "  episode_reward_mean: 165.45210644861436\n",
      "  episode_reward_min: -120.35819745947421\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 787\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9347715377807617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015986815094947815\n",
      "          model: {}\n",
      "          policy_loss: -0.03739297762513161\n",
      "          total_loss: 5.974961280822754\n",
      "          vf_explained_var: 0.6027705073356628\n",
      "          vf_loss: 6.000214099884033\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.4\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053219726892432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171532931750467\n",
      "    mean_inference_ms: 0.5255330708305319\n",
      "    mean_raw_obs_processing_ms: 0.07034406362367873\n",
      "  time_since_restore: 1296.7761743068695\n",
      "  time_this_iter_s: 3.957496404647827\n",
      "  time_total_s: 1296.7761743068695\n",
      "  timers:\n",
      "    learn_throughput: 2070.452\n",
      "    learn_time_ms: 1931.945\n",
      "    load_throughput: 34218266.368\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1002.038\n",
      "    sample_time_ms: 3991.866\n",
      "    update_time_ms: 1.318\n",
      "  timestamp: 1671818854\n",
      "  timesteps_since_restore: 1024000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:36 (running for 00:21:50.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         1296.78</td><td style=\"text-align: right;\">1024000</td><td style=\"text-align: right;\"> 165.452</td><td style=\"text-align: right;\">             210.951</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1517.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:41 (running for 00:21:55.17)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         1300.77</td><td style=\"text-align: right;\">1028000</td><td style=\"text-align: right;\"> 163.565</td><td style=\"text-align: right;\">             210.951</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1507.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 1507.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 210.9513228339365\n",
      "  episode_reward_mean: 163.8647701288271\n",
      "  episode_reward_min: -120.35819745947421\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 793\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7264443635940552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020876344293355942\n",
      "          model: {}\n",
      "          policy_loss: -0.03971205651760101\n",
      "          total_loss: 9.09487533569336\n",
      "          vf_explained_var: 0.32208025455474854\n",
      "          vf_loss: 9.118734359741211\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.05\n",
      "    ram_util_percent: 29.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09052469763106022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3170701646263183\n",
      "    mean_inference_ms: 0.525525092996315\n",
      "    mean_raw_obs_processing_ms: 0.07033021338323933\n",
      "  time_since_restore: 1304.7382016181946\n",
      "  time_this_iter_s: 3.963620662689209\n",
      "  time_total_s: 1304.7382016181946\n",
      "  timers:\n",
      "    learn_throughput: 2065.916\n",
      "    learn_time_ms: 1936.187\n",
      "    load_throughput: 32072674.441\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1001.87\n",
      "    sample_time_ms: 3992.534\n",
      "    update_time_ms: 1.329\n",
      "  timestamp: 1671818862\n",
      "  timesteps_since_restore: 1032000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:47 (running for 00:22:01.12)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         1308.72</td><td style=\"text-align: right;\">1036000</td><td style=\"text-align: right;\"> 164.968</td><td style=\"text-align: right;\">             211.032</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1508.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 1512.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 211.0319402346462\n",
      "  episode_reward_mean: 166.4462456861048\n",
      "  episode_reward_min: -120.35819745947421\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 799\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8261255025863647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011889290995895863\n",
      "          model: {}\n",
      "          policy_loss: -0.03714711591601372\n",
      "          total_loss: 7.86063814163208\n",
      "          vf_explained_var: 0.4152494966983795\n",
      "          vf_loss: 7.884242057800293\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.71666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09051968096862945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31701220344645525\n",
      "    mean_inference_ms: 0.5254994571548984\n",
      "    mean_raw_obs_processing_ms: 0.0703205365986322\n",
      "  time_since_restore: 1312.685767173767\n",
      "  time_this_iter_s: 3.963972806930542\n",
      "  time_total_s: 1312.685767173767\n",
      "  timers:\n",
      "    learn_throughput: 2065.056\n",
      "    learn_time_ms: 1936.993\n",
      "    load_throughput: 31920121.766\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1001.154\n",
      "    sample_time_ms: 3995.389\n",
      "    update_time_ms: 1.328\n",
      "  timestamp: 1671818870\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:52 (running for 00:22:06.13)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         1312.69</td><td style=\"text-align: right;\">1040000</td><td style=\"text-align: right;\"> 166.446</td><td style=\"text-align: right;\">             211.032</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1512.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:07:57 (running for 00:22:11.20)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         1316.77</td><td style=\"text-align: right;\">1044000</td><td style=\"text-align: right;\"> 166.416</td><td style=\"text-align: right;\">             211.032</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">           1508.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-07-58\n",
      "  done: false\n",
      "  episode_len_mean: 1504.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 211.0319402346462\n",
      "  episode_reward_mean: 165.2957536493601\n",
      "  episode_reward_min: -120.35819745947421\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 804\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8332561254501343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010321004316210747\n",
      "          model: {}\n",
      "          policy_loss: -0.03065507858991623\n",
      "          total_loss: 70.48257446289062\n",
      "          vf_explained_var: 0.294505774974823\n",
      "          vf_loss: 70.50147247314453\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.15\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905127607986545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31693636803328445\n",
      "    mean_inference_ms: 0.5254939998087107\n",
      "    mean_raw_obs_processing_ms: 0.07030806999528305\n",
      "  time_since_restore: 1320.7585060596466\n",
      "  time_this_iter_s: 3.991276741027832\n",
      "  time_total_s: 1320.7585060596466\n",
      "  timers:\n",
      "    learn_throughput: 2054.859\n",
      "    learn_time_ms: 1946.605\n",
      "    load_throughput: 30632127.077\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 999.499\n",
      "    sample_time_ms: 4002.004\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671818878\n",
      "  timesteps_since_restore: 1048000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:02 (running for 00:22:16.25)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         1320.76</td><td style=\"text-align: right;\">1048000</td><td style=\"text-align: right;\"> 165.296</td><td style=\"text-align: right;\">             211.032</td><td style=\"text-align: right;\">            -120.358</td><td style=\"text-align: right;\">            1504.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-06\n",
      "  done: false\n",
      "  episode_len_mean: 1524.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 221.34494626971056\n",
      "  episode_reward_mean: 171.41201144769616\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 810\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6487250924110413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012388425879180431\n",
      "          model: {}\n",
      "          policy_loss: -0.05137466639280319\n",
      "          total_loss: 5.3003435134887695\n",
      "          vf_explained_var: 0.5380586385726929\n",
      "          vf_loss: 5.337606906890869\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.050000000000004\n",
      "    ram_util_percent: 29.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09051246059407163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169124080904838\n",
      "    mean_inference_ms: 0.525462830912316\n",
      "    mean_raw_obs_processing_ms: 0.07030412045756268\n",
      "  time_since_restore: 1328.7388586997986\n",
      "  time_this_iter_s: 3.948500871658325\n",
      "  time_total_s: 1328.7388586997986\n",
      "  timers:\n",
      "    learn_throughput: 2053.484\n",
      "    learn_time_ms: 1947.909\n",
      "    load_throughput: 30570728.863\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 997.7\n",
      "    sample_time_ms: 4009.221\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671818886\n",
      "  timesteps_since_restore: 1056000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:08 (running for 00:22:22.24)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         1328.74</td><td style=\"text-align: right;\">1056000</td><td style=\"text-align: right;\"> 171.412</td><td style=\"text-align: right;\">             221.345</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:13 (running for 00:22:27.29)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         1332.73</td><td style=\"text-align: right;\">1060000</td><td style=\"text-align: right;\"> 173.314</td><td style=\"text-align: right;\">             221.345</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1528.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-14\n",
      "  done: false\n",
      "  episode_len_mean: 1515.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 222.45318018701727\n",
      "  episode_reward_mean: 171.6154640257642\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 815\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7290449738502502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00749625451862812\n",
      "          model: {}\n",
      "          policy_loss: -0.03543867543339729\n",
      "          total_loss: 41.00996398925781\n",
      "          vf_explained_var: 0.6081036329269409\n",
      "          vf_loss: 41.036861419677734\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.51666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905065847943553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3168429548649572\n",
      "    mean_inference_ms: 0.5254628305949341\n",
      "    mean_raw_obs_processing_ms: 0.07029259627325353\n",
      "  time_since_restore: 1336.756781578064\n",
      "  time_this_iter_s: 4.0304694175720215\n",
      "  time_total_s: 1336.756781578064\n",
      "  timers:\n",
      "    learn_throughput: 2043.993\n",
      "    learn_time_ms: 1956.953\n",
      "    load_throughput: 30283783.394\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 996.837\n",
      "    sample_time_ms: 4012.693\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1671818894\n",
      "  timesteps_since_restore: 1064000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:18 (running for 00:22:32.30)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         1336.76</td><td style=\"text-align: right;\">1064000</td><td style=\"text-align: right;\"> 171.615</td><td style=\"text-align: right;\">             222.453</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1515.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1068000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-20\n",
      "  done: false\n",
      "  episode_len_mean: 1515.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 222.45318018701727\n",
      "  episode_reward_mean: 171.86371814486316\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 817\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6706108450889587\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011875615455210209\n",
      "          model: {}\n",
      "          policy_loss: -0.041799843311309814\n",
      "          total_loss: 4.118782997131348\n",
      "          vf_explained_var: 0.6159564256668091\n",
      "          vf_loss: 4.147055625915527\n",
      "    num_agent_steps_sampled: 1068000\n",
      "    num_agent_steps_trained: 1068000\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.14285714285715\n",
      "    ram_util_percent: 29.87142857142857\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09050835319937703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31683503060839535\n",
      "    mean_inference_ms: 0.5254715318008885\n",
      "    mean_raw_obs_processing_ms: 0.07029248198467754\n",
      "  time_since_restore: 1341.8990745544434\n",
      "  time_this_iter_s: 5.1422929763793945\n",
      "  time_total_s: 1341.8990745544434\n",
      "  timers:\n",
      "    learn_throughput: 2022.507\n",
      "    learn_time_ms: 1977.743\n",
      "    load_throughput: 28378240.866\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 972.65\n",
      "    sample_time_ms: 4112.475\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671818900\n",
      "  timesteps_since_restore: 1068000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:24 (running for 00:22:37.48)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         1345.87</td><td style=\"text-align: right;\">1072000</td><td style=\"text-align: right;\"> 172.758</td><td style=\"text-align: right;\">             222.453</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1515.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1076000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 1515.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 222.45318018701727\n",
      "  episode_reward_mean: 173.35277833408406\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 823\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5379042625427246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012977455742657185\n",
      "          model: {}\n",
      "          policy_loss: -0.04169512167572975\n",
      "          total_loss: 3.998227596282959\n",
      "          vf_explained_var: 0.6461814045906067\n",
      "          vf_loss: 4.025140285491943\n",
      "    num_agent_steps_sampled: 1076000\n",
      "    num_agent_steps_trained: 1076000\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.8\n",
      "    ram_util_percent: 29.6\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09051393234612959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3168121107340329\n",
      "    mean_inference_ms: 0.5254996499149465\n",
      "    mean_raw_obs_processing_ms: 0.07029260537088783\n",
      "  time_since_restore: 1349.8856511116028\n",
      "  time_this_iter_s: 4.017136335372925\n",
      "  time_total_s: 1349.8856511116028\n",
      "  timers:\n",
      "    learn_throughput: 2023.647\n",
      "    learn_time_ms: 1976.629\n",
      "    load_throughput: 30034400.286\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 966.708\n",
      "    sample_time_ms: 4137.755\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671818908\n",
      "  timesteps_since_restore: 1076000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:29 (running for 00:22:42.49)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         1349.89</td><td style=\"text-align: right;\">1076000</td><td style=\"text-align: right;\"> 173.353</td><td style=\"text-align: right;\">             222.453</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1515.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:34 (running for 00:22:47.54)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         1353.89</td><td style=\"text-align: right;\">1080000</td><td style=\"text-align: right;\">  176.31</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1084000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 224.29640249289147\n",
      "  episode_reward_mean: 176.74389091572294\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 827\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.527676522731781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011360645294189453\n",
      "          model: {}\n",
      "          policy_loss: -0.04182352125644684\n",
      "          total_loss: 4.134506702423096\n",
      "          vf_explained_var: 0.6125631928443909\n",
      "          vf_loss: 4.163389682769775\n",
      "    num_agent_steps_sampled: 1084000\n",
      "    num_agent_steps_trained: 1084000\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.333333333333336\n",
      "    ram_util_percent: 29.599999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905215994573589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3168280779388101\n",
      "    mean_inference_ms: 0.5255087781317859\n",
      "    mean_raw_obs_processing_ms: 0.070297635295204\n",
      "  time_since_restore: 1357.916320323944\n",
      "  time_this_iter_s: 4.028685808181763\n",
      "  time_total_s: 1357.916320323944\n",
      "  timers:\n",
      "    learn_throughput: 2031.293\n",
      "    learn_time_ms: 1969.189\n",
      "    load_throughput: 31877666.73\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 965.699\n",
      "    sample_time_ms: 4142.078\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1671818916\n",
      "  timesteps_since_restore: 1084000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:39 (running for 00:22:52.56)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         1357.92</td><td style=\"text-align: right;\">1084000</td><td style=\"text-align: right;\"> 176.744</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:44 (running for 00:22:57.63)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         1361.93</td><td style=\"text-align: right;\">1088000</td><td style=\"text-align: right;\"> 176.477</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-44\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 224.29640249289147\n",
      "  episode_reward_mean: 176.85374947036314\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 833\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6396161317825317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011884063482284546\n",
      "          model: {}\n",
      "          policy_loss: -0.04149371758103371\n",
      "          total_loss: 4.7814531326293945\n",
      "          vf_explained_var: 0.5920572876930237\n",
      "          vf_loss: 4.809410095214844\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65\n",
      "    ram_util_percent: 29.516666666666666\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09052547046476045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31678526563595605\n",
      "    mean_inference_ms: 0.5255599923703828\n",
      "    mean_raw_obs_processing_ms: 0.07029506734601948\n",
      "  time_since_restore: 1366.1649732589722\n",
      "  time_this_iter_s: 4.237122535705566\n",
      "  time_total_s: 1366.1649732589722\n",
      "  timers:\n",
      "    learn_throughput: 2004.379\n",
      "    learn_time_ms: 1995.631\n",
      "    load_throughput: 31920121.766\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 967.726\n",
      "    sample_time_ms: 4133.403\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1671818924\n",
      "  timesteps_since_restore: 1092000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:49 (running for 00:23:02.95)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         1370.25</td><td style=\"text-align: right;\">1096000</td><td style=\"text-align: right;\"> 177.009</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 224.29640249289147\n",
      "  episode_reward_mean: 177.24027152938012\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 837\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6096396446228027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010691864416003227\n",
      "          model: {}\n",
      "          policy_loss: -0.03852628543972969\n",
      "          total_loss: 5.136207580566406\n",
      "          vf_explained_var: 0.6694434285163879\n",
      "          vf_loss: 5.162554740905762\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_agent_steps_trained: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.62\n",
      "    ram_util_percent: 29.7\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053439601738639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31680764018233687\n",
      "    mean_inference_ms: 0.5255781220599389\n",
      "    mean_raw_obs_processing_ms: 0.07030163471095839\n",
      "  time_since_restore: 1374.2403357028961\n",
      "  time_this_iter_s: 3.9875497817993164\n",
      "  time_total_s: 1374.2403357028961\n",
      "  timers:\n",
      "    learn_throughput: 2008.314\n",
      "    learn_time_ms: 1991.72\n",
      "    load_throughput: 33000031.471\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 958.366\n",
      "    sample_time_ms: 4173.773\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1671818932\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:08:54 (running for 00:23:07.98)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         1374.24</td><td style=\"text-align: right;\">1100000</td><td style=\"text-align: right;\">  177.24</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:00 (running for 00:23:13.44)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         1378.71</td><td style=\"text-align: right;\">1104000</td><td style=\"text-align: right;\"> 178.098</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 224.29640249289147\n",
      "  episode_reward_mean: 178.19366483337387\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 843\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5654357075691223\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011256265453994274\n",
      "          model: {}\n",
      "          policy_loss: -0.03548154607415199\n",
      "          total_loss: 4.7212138175964355\n",
      "          vf_explained_var: 0.6374779343605042\n",
      "          vf_loss: 4.7438740730285645\n",
      "    num_agent_steps_sampled: 1108000\n",
      "    num_agent_steps_trained: 1108000\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.050000000000004\n",
      "    ram_util_percent: 29.916666666666668\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09054473521378333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3167872236288267\n",
      "    mean_inference_ms: 0.52566878222905\n",
      "    mean_raw_obs_processing_ms: 0.07030534864183041\n",
      "  time_since_restore: 1382.847987651825\n",
      "  time_this_iter_s: 4.142078638076782\n",
      "  time_total_s: 1382.847987651825\n",
      "  timers:\n",
      "    learn_throughput: 2013.274\n",
      "    learn_time_ms: 1986.813\n",
      "    load_throughput: 33222209.901\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 971.888\n",
      "    sample_time_ms: 4115.699\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1671818941\n",
      "  timesteps_since_restore: 1108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:05 (running for 00:23:18.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         1382.85</td><td style=\"text-align: right;\">1108000</td><td style=\"text-align: right;\"> 178.194</td><td style=\"text-align: right;\">             224.296</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 179.15171503777918\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 847\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46499544382095337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01176782138645649\n",
      "          model: {}\n",
      "          policy_loss: -0.0350702628493309\n",
      "          total_loss: 4.401148319244385\n",
      "          vf_explained_var: 0.6428684592247009\n",
      "          vf_loss: 4.42281436920166\n",
      "    num_agent_steps_sampled: 1116000\n",
      "    num_agent_steps_trained: 1116000\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.98333333333333\n",
      "    ram_util_percent: 29.950000000000003\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09055837590853763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31682525850221155\n",
      "    mean_inference_ms: 0.525717092156251\n",
      "    mean_raw_obs_processing_ms: 0.07031628745363122\n",
      "  time_since_restore: 1391.171787261963\n",
      "  time_this_iter_s: 4.017972230911255\n",
      "  time_total_s: 1391.171787261963\n",
      "  timers:\n",
      "    learn_throughput: 2001.646\n",
      "    learn_time_ms: 1998.355\n",
      "    load_throughput: 31412125.07\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 964.818\n",
      "    sample_time_ms: 4145.86\n",
      "    update_time_ms: 1.363\n",
      "  timestamp: 1671818949\n",
      "  timesteps_since_restore: 1116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:10 (running for 00:23:23.97)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         1391.17</td><td style=\"text-align: right;\">1116000</td><td style=\"text-align: right;\"> 179.152</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:15 (running for 00:23:29.02)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         1395.16</td><td style=\"text-align: right;\">1120000</td><td style=\"text-align: right;\"> 179.899</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-17\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 180.0542785318916\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 853\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.49448832869529724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008047284558415413\n",
      "          model: {}\n",
      "          policy_loss: -0.035851042717695236\n",
      "          total_loss: 7.065448760986328\n",
      "          vf_explained_var: 0.5592160820960999\n",
      "          vf_loss: 7.087549686431885\n",
      "    num_agent_steps_sampled: 1124000\n",
      "    num_agent_steps_trained: 1124000\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.13333333333333\n",
      "    ram_util_percent: 29.883333333333336\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.090571187241136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31681397373441667\n",
      "    mean_inference_ms: 0.5258226096214305\n",
      "    mean_raw_obs_processing_ms: 0.07032243664301129\n",
      "  time_since_restore: 1399.1251692771912\n",
      "  time_this_iter_s: 3.967487096786499\n",
      "  time_total_s: 1399.1251692771912\n",
      "  timers:\n",
      "    learn_throughput: 2001.377\n",
      "    learn_time_ms: 1998.624\n",
      "    load_throughput: 31353421.79\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 966.168\n",
      "    sample_time_ms: 4140.067\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818957\n",
      "  timesteps_since_restore: 1124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:21 (running for 00:23:34.93)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         1403.08</td><td style=\"text-align: right;\">1128000</td><td style=\"text-align: right;\"> 179.931</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 1524.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 180.30428165990963\n",
      "  episode_reward_min: -98.04082014436958\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 857\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5318430066108704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018695438280701637\n",
      "          model: {}\n",
      "          policy_loss: -0.05155877396464348\n",
      "          total_loss: 32.977813720703125\n",
      "          vf_explained_var: 0.23262342810630798\n",
      "          vf_loss: 32.997432708740234\n",
      "    num_agent_steps_sampled: 1132000\n",
      "    num_agent_steps_trained: 1132000\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.16\n",
      "    ram_util_percent: 29.9\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058496917296774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3168532131208161\n",
      "    mean_inference_ms: 0.5258712107419311\n",
      "    mean_raw_obs_processing_ms: 0.07033380522000765\n",
      "  time_since_restore: 1407.0216200351715\n",
      "  time_this_iter_s: 3.943145990371704\n",
      "  time_total_s: 1407.0216200351715\n",
      "  timers:\n",
      "    learn_throughput: 2032.298\n",
      "    learn_time_ms: 1968.215\n",
      "    load_throughput: 29526955.297\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 968.524\n",
      "    sample_time_ms: 4129.994\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1671818965\n",
      "  timesteps_since_restore: 1132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:26 (running for 00:23:39.94)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         1407.02</td><td style=\"text-align: right;\">1132000</td><td style=\"text-align: right;\"> 180.304</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -98.0408</td><td style=\"text-align: right;\">           1524.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:32 (running for 00:23:45.91)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         1411.02</td><td style=\"text-align: right;\">1136000</td><td style=\"text-align: right;\"> 177.833</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -115.739</td><td style=\"text-align: right;\">           1515.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-33\n",
      "  done: false\n",
      "  episode_len_mean: 1515.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 178.24752568223775\n",
      "  episode_reward_min: -115.73855615203578\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 864\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5721243619918823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009371885098516941\n",
      "          model: {}\n",
      "          policy_loss: -0.033633019775152206\n",
      "          total_loss: 6.08428430557251\n",
      "          vf_explained_var: 0.5635334253311157\n",
      "          vf_loss: 6.10190486907959\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.449999999999996\n",
      "    ram_util_percent: 29.78333333333333\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09060337062948413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3168733017382289\n",
      "    mean_inference_ms: 0.5259782389544138\n",
      "    mean_raw_obs_processing_ms: 0.07034614173613866\n",
      "  time_since_restore: 1415.045352935791\n",
      "  time_this_iter_s: 4.026736259460449\n",
      "  time_total_s: 1415.045352935791\n",
      "  timers:\n",
      "    learn_throughput: 2027.544\n",
      "    learn_time_ms: 1972.83\n",
      "    load_throughput: 29547756.252\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 976.627\n",
      "    sample_time_ms: 4095.728\n",
      "    update_time_ms: 1.369\n",
      "  timestamp: 1671818973\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:37 (running for 00:23:50.95)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         1419.03</td><td style=\"text-align: right;\">1144000</td><td style=\"text-align: right;\"> 169.507</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">           1472.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 1472.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 169.78894126418902\n",
      "  episode_reward_min: -123.17123156908464\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 871\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3230062425136566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012523733079433441\n",
      "          model: {}\n",
      "          policy_loss: -0.028261125087738037\n",
      "          total_loss: 5.201045989990234\n",
      "          vf_explained_var: 0.5725093483924866\n",
      "          vf_loss: 5.207909107208252\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.24\n",
      "    ram_util_percent: 29.7\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09062544912040135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31692757200603805\n",
      "    mean_inference_ms: 0.5260736831839233\n",
      "    mean_raw_obs_processing_ms: 0.07036418890006205\n",
      "  time_since_restore: 1423.0459427833557\n",
      "  time_this_iter_s: 4.0190980434417725\n",
      "  time_total_s: 1423.0459427833557\n",
      "  timers:\n",
      "    learn_throughput: 2054.055\n",
      "    learn_time_ms: 1947.367\n",
      "    load_throughput: 31459246.203\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 984.619\n",
      "    sample_time_ms: 4062.486\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671818981\n",
      "  timesteps_since_restore: 1148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:42 (running for 00:23:56.00)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         1423.05</td><td style=\"text-align: right;\">1148000</td><td style=\"text-align: right;\"> 169.789</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">           1472.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:47 (running for 00:24:01.08)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         1427.08</td><td style=\"text-align: right;\">1152000</td><td style=\"text-align: right;\"> 170.206</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">           1472.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 1476.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 172.2529546497488\n",
      "  episode_reward_min: -123.17123156908464\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 877\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4008089601993561\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008675486780703068\n",
      "          model: {}\n",
      "          policy_loss: -0.030519600957632065\n",
      "          total_loss: 5.621848106384277\n",
      "          vf_explained_var: 0.6048008799552917\n",
      "          vf_loss: 5.637545108795166\n",
      "    num_agent_steps_sampled: 1156000\n",
      "    num_agent_steps_trained: 1156000\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.66666666666667\n",
      "    ram_util_percent: 29.7\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09064227174581264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169530412788936\n",
      "    mean_inference_ms: 0.526171048443791\n",
      "    mean_raw_obs_processing_ms: 0.07037586193761455\n",
      "  time_since_restore: 1431.0316162109375\n",
      "  time_this_iter_s: 3.9554197788238525\n",
      "  time_total_s: 1431.0316162109375\n",
      "  timers:\n",
      "    learn_throughput: 2064.692\n",
      "    learn_time_ms: 1937.335\n",
      "    load_throughput: 31400366.835\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 998.406\n",
      "    sample_time_ms: 4006.387\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818989\n",
      "  timesteps_since_restore: 1156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:53 (running for 00:24:07.00)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         1434.99</td><td style=\"text-align: right;\">1160000</td><td style=\"text-align: right;\">  172.39</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">           1476.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 1490.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.44607506550398\n",
      "  episode_reward_mean: 175.6842821848398\n",
      "  episode_reward_min: -123.17123156908464\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 881\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3815174996852875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00824158638715744\n",
      "          model: {}\n",
      "          policy_loss: -0.024448463693261147\n",
      "          total_loss: 3.9978291988372803\n",
      "          vf_explained_var: 0.7055416107177734\n",
      "          vf_loss: 4.0081963539123535\n",
      "    num_agent_steps_sampled: 1164000\n",
      "    num_agent_steps_trained: 1164000\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.81666666666667\n",
      "    ram_util_percent: 29.7\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09065326843143603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31696990451102697\n",
      "    mean_inference_ms: 0.52623448202845\n",
      "    mean_raw_obs_processing_ms: 0.07038334580958286\n",
      "  time_since_restore: 1438.9628190994263\n",
      "  time_this_iter_s: 3.973215341567993\n",
      "  time_total_s: 1438.9628190994263\n",
      "  timers:\n",
      "    learn_throughput: 2066.418\n",
      "    learn_time_ms: 1935.717\n",
      "    load_throughput: 31465146.287\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 998.777\n",
      "    sample_time_ms: 4004.897\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671818997\n",
      "  timesteps_since_restore: 1164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 291\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:09:58 (running for 00:24:12.04)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         1438.96</td><td style=\"text-align: right;\">1164000</td><td style=\"text-align: right;\"> 175.684</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">            1490.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:04 (running for 00:24:17.98)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         1442.93</td><td style=\"text-align: right;\">1168000</td><td style=\"text-align: right;\"> 176.056</td><td style=\"text-align: right;\">             225.446</td><td style=\"text-align: right;\">            -123.171</td><td style=\"text-align: right;\">            1490.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 1475.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 173.83052697522305\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 888\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4281536340713501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01975950039923191\n",
      "          model: {}\n",
      "          policy_loss: -0.009904660284519196\n",
      "          total_loss: 52.691017150878906\n",
      "          vf_explained_var: 0.6336494088172913\n",
      "          vf_loss: 52.66716384887695\n",
      "    num_agent_steps_sampled: 1172000\n",
      "    num_agent_steps_trained: 1172000\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.4\n",
      "    ram_util_percent: 29.7\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09067220740248295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31699974293437905\n",
      "    mean_inference_ms: 0.5263426942618993\n",
      "    mean_raw_obs_processing_ms: 0.07039699994991985\n",
      "  time_since_restore: 1446.9374158382416\n",
      "  time_this_iter_s: 4.01104474067688\n",
      "  time_total_s: 1446.9374158382416\n",
      "  timers:\n",
      "    learn_throughput: 2062.512\n",
      "    learn_time_ms: 1939.382\n",
      "    load_throughput: 30515125.5\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 997.43\n",
      "    sample_time_ms: 4010.306\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1671819005\n",
      "  timesteps_since_restore: 1172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 293\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:09 (running for 00:24:23.07)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         1446.94</td><td style=\"text-align: right;\">1172000</td><td style=\"text-align: right;\"> 173.831</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1475.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-10-13\n",
      "  done: false\n",
      "  episode_len_mean: 1485.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 176.86615285474127\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 892\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.562843918800354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006236076354980469\n",
      "          model: {}\n",
      "          policy_loss: -0.029382582753896713\n",
      "          total_loss: 5.364028453826904\n",
      "          vf_explained_var: 0.6441875100135803\n",
      "          vf_loss: 5.382756233215332\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_agent_steps_trained: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.019999999999996\n",
      "    ram_util_percent: 29.96\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068816703228602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3170513613439274\n",
      "    mean_inference_ms: 0.5264027684963355\n",
      "    mean_raw_obs_processing_ms: 0.07041135952033409\n",
      "  time_since_restore: 1455.2752220630646\n",
      "  time_this_iter_s: 4.00589394569397\n",
      "  time_total_s: 1455.2752220630646\n",
      "  timers:\n",
      "    learn_throughput: 2062.137\n",
      "    learn_time_ms: 1939.735\n",
      "    load_throughput: 30399014.314\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 988.532\n",
      "    sample_time_ms: 4046.405\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1671819013\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:14 (running for 00:24:28.39)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         1455.28</td><td style=\"text-align: right;\">1180000</td><td style=\"text-align: right;\"> 176.866</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1485.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 1499.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 180.36259208403834\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 895\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5564520955085754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007873700000345707\n",
      "          model: {}\n",
      "          policy_loss: -0.047693297266960144\n",
      "          total_loss: 6.27182674407959\n",
      "          vf_explained_var: 0.6197390556335449\n",
      "          vf_loss: 6.30606746673584\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.96249999999999\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070151470678545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3170989749982034\n",
      "    mean_inference_ms: 0.5264479479501152\n",
      "    mean_raw_obs_processing_ms: 0.07042391434623282\n",
      "  time_since_restore: 1460.3126063346863\n",
      "  time_this_iter_s: 5.037384271621704\n",
      "  time_total_s: 1460.3126063346863\n",
      "  timers:\n",
      "    learn_throughput: 1974.462\n",
      "    learn_time_ms: 2025.869\n",
      "    load_throughput: 30115268.354\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 984.545\n",
      "    sample_time_ms: 4062.791\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1671819019\n",
      "  timesteps_since_restore: 1184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 296\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:20 (running for 00:24:33.46)<br>Memory usage on this node: 9.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         1460.31</td><td style=\"text-align: right;\">1184000</td><td style=\"text-align: right;\"> 180.363</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1499.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:25 (running for 00:24:38.94)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         1464.77</td><td style=\"text-align: right;\">1188000</td><td style=\"text-align: right;\"> 181.016</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1499.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 1499.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 181.24585912024176\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 900\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4453258216381073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008000057190656662\n",
      "          model: {}\n",
      "          policy_loss: -0.030343055725097656\n",
      "          total_loss: 4.807168960571289\n",
      "          vf_explained_var: 0.6264505982398987\n",
      "          vf_loss: 4.823843002319336\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.61666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09071991598899463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31711896270177414\n",
      "    mean_inference_ms: 0.5265889418575531\n",
      "    mean_raw_obs_processing_ms: 0.07043673635447294\n",
      "  time_since_restore: 1468.8862347602844\n",
      "  time_this_iter_s: 4.1114661693573\n",
      "  time_total_s: 1468.8862347602844\n",
      "  timers:\n",
      "    learn_throughput: 1973.663\n",
      "    learn_time_ms: 2026.688\n",
      "    load_throughput: 30959985.237\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 951.983\n",
      "    sample_time_ms: 4201.756\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1671819027\n",
      "  timesteps_since_restore: 1192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:30 (running for 00:24:44.11)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         1468.89</td><td style=\"text-align: right;\">1192000</td><td style=\"text-align: right;\"> 181.246</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1499.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:35 (running for 00:24:49.12)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:40 (running for 00:24:54.16)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:45 (running for 00:24:59.16)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:50 (running for 00:25:04.16)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:10:55 (running for 00:25:09.17)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:00 (running for 00:25:14.17)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:05 (running for 00:25:19.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:10 (running for 00:25:24.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:15 (running for 00:25:29.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:20 (running for 00:25:34.19)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:25 (running for 00:25:39.19)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:30 (running for 00:25:44.20)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:35 (running for 00:25:49.20)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:40 (running for 00:25:54.21)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:45 (running for 00:25:59.21)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:50 (running for 00:26:04.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:11:55 (running for 00:26:09.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:00 (running for 00:26:14.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:05 (running for 00:26:19.23)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:10 (running for 00:26:24.23)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:15 (running for 00:26:29.24)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:20 (running for 00:26:34.24)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:25 (running for 00:26:39.24)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:30 (running for 00:26:44.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:35 (running for 00:26:49.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:40 (running for 00:26:54.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:45 (running for 00:26:59.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:50 (running for 00:27:04.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:12:55 (running for 00:27:09.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1472.92</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\"> 182.102</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1498.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 1498.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 182.13592001675792\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 906\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1507.28\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 236.70483349953426\n",
      "    episode_reward_mean: 185.95677122478457\n",
      "    episode_reward_min: -125.91359300048525\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 84\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 991\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1471\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 508\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 160\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 97\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 131\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 86\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 214.5345853699981\n",
      "      - 210.0077637453449\n",
      "      - 209.71140147534504\n",
      "      - 193.00736315859444\n",
      "      - 217.8695141157916\n",
      "      - 209.3898284059131\n",
      "      - 209.75809813164742\n",
      "      - 222.30000321577708\n",
      "      - 195.3519460639899\n",
      "      - -117.15930449984906\n",
      "      - 194.63031574248765\n",
      "      - 217.28697189126063\n",
      "      - 192.42291445702995\n",
      "      - 236.70483349953426\n",
      "      - 215.91620648306298\n",
      "      - 220.82978483304984\n",
      "      - 31.12091818564201\n",
      "      - 215.94737136896183\n",
      "      - 200.5223800149617\n",
      "      - 73.39844172131296\n",
      "      - 219.89779215140769\n",
      "      - 202.23234083142532\n",
      "      - 194.65878007522073\n",
      "      - 207.91922028505377\n",
      "      - 212.60765766188806\n",
      "      - 205.86369822993936\n",
      "      - 208.2138937369381\n",
      "      - 219.83831748030227\n",
      "      - -39.749644500400315\n",
      "      - 206.8694316705184\n",
      "      - 217.18255567328194\n",
      "      - 215.1758372727176\n",
      "      - 215.69959063671382\n",
      "      - 184.9681311035839\n",
      "      - 218.83510114964443\n",
      "      - 210.34778546944622\n",
      "      - 202.12271489321824\n",
      "      - 212.58335817196362\n",
      "      - 211.7028030428713\n",
      "      - 204.2508880389331\n",
      "      - 199.63040281430676\n",
      "      - 213.27031918059703\n",
      "      - 208.92168475579408\n",
      "      - 201.0323190873462\n",
      "      - 199.08072476520573\n",
      "      - 202.4384777208327\n",
      "      - 214.97365733483133\n",
      "      - 199.51593754439747\n",
      "      - 220.0081203464373\n",
      "      - 216.38915810683002\n",
      "      - 196.43909884137688\n",
      "      - 197.20517332533672\n",
      "      - 193.73374490024426\n",
      "      - 216.82843444543815\n",
      "      - -125.91359300048525\n",
      "      - 219.62612255368296\n",
      "      - 198.23147011977616\n",
      "      - 198.76044724383377\n",
      "      - 226.73302327000826\n",
      "      - 207.14696429625667\n",
      "      - -121.72435779114379\n",
      "      - 218.7299481669\n",
      "      - 215.10681934725923\n",
      "      - -122.4623593103004\n",
      "      - 180.65159734436332\n",
      "      - 195.01075453259702\n",
      "      - 181.14992556341926\n",
      "      - 217.41810965055407\n",
      "      - 212.36735246620725\n",
      "      - 208.88328224109566\n",
      "      - 215.7373502591171\n",
      "      - 212.74640838308733\n",
      "      - 213.96502706763724\n",
      "      - 208.62803568745917\n",
      "      - 197.11537328440014\n",
      "      - 201.65926421523218\n",
      "      - 205.52319494533222\n",
      "      - 207.0845650761047\n",
      "      - -120.36637646190884\n",
      "      - 194.01157066491677\n",
      "      - 210.90513386795416\n",
      "      - 219.29547986960503\n",
      "      - 213.44003693213824\n",
      "      - 212.80187434277485\n",
      "      - 223.18980730820633\n",
      "      - 218.26880777084946\n",
      "      - 201.19924376382724\n",
      "      - 202.48418343543315\n",
      "      - 196.99242735440367\n",
      "      - 198.0101067146631\n",
      "      - 208.9785760026776\n",
      "      - 210.28068312540898\n",
      "      - 199.30148538785943\n",
      "      - 210.05124440402\n",
      "      - 221.72708680956828\n",
      "      - 205.94895637158427\n",
      "      - 198.4572791785412\n",
      "      - 202.92164526593172\n",
      "      - 207.51554690445377\n",
      "      - 213.8487542296533\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08625003535110087\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.30587818433981917\n",
      "      mean_inference_ms: 0.4972206566949652\n",
      "      mean_raw_obs_processing_ms: 0.05958895074586578\n",
      "    timesteps_this_iter: 150728\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4854457378387451\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007545504719018936\n",
      "          model: {}\n",
      "          policy_loss: -0.04976982995867729\n",
      "          total_loss: 6.7435808181762695\n",
      "          vf_explained_var: 0.6331602931022644\n",
      "          vf_loss: 6.780458450317383\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.128846153846155\n",
      "    ram_util_percent: 29.95336538461538\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074806706553204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.317202633667389\n",
      "    mean_inference_ms: 0.5267248617544549\n",
      "    mean_raw_obs_processing_ms: 0.07046224240658927\n",
      "  time_since_restore: 1618.8869264125824\n",
      "  time_this_iter_s: 145.96749019622803\n",
      "  time_total_s: 1618.8869264125824\n",
      "  timers:\n",
      "    learn_throughput: 1971.173\n",
      "    learn_time_ms: 2029.249\n",
      "    load_throughput: 29096801.942\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 951.087\n",
      "    sample_time_ms: 4205.716\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671819177\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 300\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:01 (running for 00:27:15.13)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         1622.88</td><td style=\"text-align: right;\">1204000</td><td style=\"text-align: right;\"> 180.864</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1495.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 1480.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 177.6558871607521\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 912\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3229266107082367\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00794850941747427\n",
      "          model: {}\n",
      "          policy_loss: -0.031983643770217896\n",
      "          total_loss: 22.615230560302734\n",
      "          vf_explained_var: 0.5459794402122498\n",
      "          vf_loss: 22.633634567260742\n",
      "    num_agent_steps_sampled: 1208000\n",
      "    num_agent_steps_trained: 1208000\n",
      "    num_steps_sampled: 1208000\n",
      "    num_steps_trained: 1208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.8\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09076720488824234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172038303099856\n",
      "    mean_inference_ms: 0.5269059547227947\n",
      "    mean_raw_obs_processing_ms: 0.07047395612027256\n",
      "  time_since_restore: 1626.896385192871\n",
      "  time_this_iter_s: 4.0201990604400635\n",
      "  time_total_s: 1626.896385192871\n",
      "  timers:\n",
      "    learn_throughput: 1964.668\n",
      "    learn_time_ms: 2035.967\n",
      "    load_throughput: 31691001.133\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 217.246\n",
      "    sample_time_ms: 18412.322\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671819185\n",
      "  timesteps_since_restore: 1208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1208000\n",
      "  training_iteration: 302\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:06 (running for 00:27:20.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">          1626.9</td><td style=\"text-align: right;\">1208000</td><td style=\"text-align: right;\"> 177.656</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1480.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:11 (running for 00:27:25.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         1630.92</td><td style=\"text-align: right;\">1212000</td><td style=\"text-align: right;\"> 180.382</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1493.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 1493.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 180.2257344460634\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 917\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5728305578231812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00712328078225255\n",
      "          model: {}\n",
      "          policy_loss: -0.05053296312689781\n",
      "          total_loss: 6.139254570007324\n",
      "          vf_explained_var: 0.5368897318840027\n",
      "          vf_loss: 6.177617073059082\n",
      "    num_agent_steps_sampled: 1216000\n",
      "    num_agent_steps_trained: 1216000\n",
      "    num_steps_sampled: 1216000\n",
      "    num_steps_trained: 1216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.833333333333336\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078796078911511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172688468062639\n",
      "    mean_inference_ms: 0.5269956892254158\n",
      "    mean_raw_obs_processing_ms: 0.07049349444577323\n",
      "  time_since_restore: 1634.8903250694275\n",
      "  time_this_iter_s: 3.965635299682617\n",
      "  time_total_s: 1634.8903250694275\n",
      "  timers:\n",
      "    learn_throughput: 1967.294\n",
      "    learn_time_ms: 2033.25\n",
      "    load_throughput: 29937930.05\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 217.572\n",
      "    sample_time_ms: 18384.752\n",
      "    update_time_ms: 1.421\n",
      "  timestamp: 1671819193\n",
      "  timesteps_since_restore: 1216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1216000\n",
      "  training_iteration: 304\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:17 (running for 00:27:31.21)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         1634.89</td><td style=\"text-align: right;\">1216000</td><td style=\"text-align: right;\"> 180.226</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1493.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 175.12976924117976\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 923\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5034015774726868\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00984145887196064\n",
      "          model: {}\n",
      "          policy_loss: -0.041276928037405014\n",
      "          total_loss: 6.500810146331787\n",
      "          vf_explained_var: 0.504819929599762\n",
      "          vf_loss: 6.525271892547607\n",
      "    num_agent_steps_sampled: 1224000\n",
      "    num_agent_steps_trained: 1224000\n",
      "    num_steps_sampled: 1224000\n",
      "    num_steps_trained: 1224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.220000000000006\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080450856266044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173015629558292\n",
      "    mean_inference_ms: 0.5270889395815757\n",
      "    mean_raw_obs_processing_ms: 0.07050708897520556\n",
      "  time_since_restore: 1642.9294674396515\n",
      "  time_this_iter_s: 4.008806228637695\n",
      "  time_total_s: 1642.9294674396515\n",
      "  timers:\n",
      "    learn_throughput: 2054.127\n",
      "    learn_time_ms: 1947.299\n",
      "    load_throughput: 29325670.337\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 217.748\n",
      "    sample_time_ms: 18369.858\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671819201\n",
      "  timesteps_since_restore: 1224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1224000\n",
      "  training_iteration: 306\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:22 (running for 00:27:36.29)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         1642.93</td><td style=\"text-align: right;\">1224000</td><td style=\"text-align: right;\">  175.13</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:27 (running for 00:27:41.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         1646.93</td><td style=\"text-align: right;\">1228000</td><td style=\"text-align: right;\">  174.96</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 174.96813392150943\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 928\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5159955024719238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008643397130072117\n",
      "          model: {}\n",
      "          policy_loss: -0.031081801280379295\n",
      "          total_loss: 4.072921276092529\n",
      "          vf_explained_var: 0.6928542256355286\n",
      "          vf_loss: 4.089235305786133\n",
      "    num_agent_steps_sampled: 1232000\n",
      "    num_agent_steps_trained: 1232000\n",
      "    num_steps_sampled: 1232000\n",
      "    num_steps_trained: 1232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.35\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082071098129164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31735339953358155\n",
      "    mean_inference_ms: 0.5271481529465085\n",
      "    mean_raw_obs_processing_ms: 0.07052302657417915\n",
      "  time_since_restore: 1650.8813586235046\n",
      "  time_this_iter_s: 3.949065685272217\n",
      "  time_total_s: 1650.8813586235046\n",
      "  timers:\n",
      "    learn_throughput: 2052.946\n",
      "    learn_time_ms: 1948.419\n",
      "    load_throughput: 29943273.246\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 219.552\n",
      "    sample_time_ms: 18218.918\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671819209\n",
      "  timesteps_since_restore: 1232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1232000\n",
      "  training_iteration: 308\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:32 (running for 00:27:46.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         1650.88</td><td style=\"text-align: right;\">1232000</td><td style=\"text-align: right;\"> 174.968</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 175.41125526799706\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 933\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5421395301818848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008133459836244583\n",
      "          model: {}\n",
      "          policy_loss: -0.040386732667684555\n",
      "          total_loss: 4.5908026695251465\n",
      "          vf_explained_var: 0.6942516565322876\n",
      "          vf_loss: 4.617292404174805\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_agent_steps_trained: 1240000\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.0\n",
      "    ram_util_percent: 29.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09083101941281317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173509432285945\n",
      "    mean_inference_ms: 0.5272375666350356\n",
      "    mean_raw_obs_processing_ms: 0.0705292400551186\n",
      "  time_since_restore: 1658.7893912792206\n",
      "  time_this_iter_s: 3.9184272289276123\n",
      "  time_total_s: 1658.7893912792206\n",
      "  timers:\n",
      "    learn_throughput: 2055.199\n",
      "    learn_time_ms: 1946.283\n",
      "    load_throughput: 31883724.819\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 219.598\n",
      "    sample_time_ms: 18215.095\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671819217\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 310\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:38 (running for 00:27:52.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         1658.79</td><td style=\"text-align: right;\">1240000</td><td style=\"text-align: right;\"> 175.411</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:43 (running for 00:27:57.34)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         1662.85</td><td style=\"text-align: right;\">1244000</td><td style=\"text-align: right;\"> 176.056</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 176.3731921717914\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 938\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.40908077359199524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007859100587666035\n",
      "          model: {}\n",
      "          policy_loss: -0.03712195158004761\n",
      "          total_loss: 5.6353983879089355\n",
      "          vf_explained_var: 0.632342517375946\n",
      "          vf_loss: 5.659092426300049\n",
      "    num_agent_steps_sampled: 1248000\n",
      "    num_agent_steps_trained: 1248000\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.18333333333334\n",
      "    ram_util_percent: 29.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0908449985365328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31739563260018655\n",
      "    mean_inference_ms: 0.527283139610552\n",
      "    mean_raw_obs_processing_ms: 0.07054258642711132\n",
      "  time_since_restore: 1666.8031446933746\n",
      "  time_this_iter_s: 3.9575531482696533\n",
      "  time_total_s: 1666.8031446933746\n",
      "  timers:\n",
      "    learn_throughput: 2059.507\n",
      "    learn_time_ms: 1942.213\n",
      "    load_throughput: 31962690.036\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 996.248\n",
      "    sample_time_ms: 4015.064\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1671819225\n",
      "  timesteps_since_restore: 1248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 312\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:49 (running for 00:28:03.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         1670.76</td><td style=\"text-align: right;\">1252000</td><td style=\"text-align: right;\"> 176.152</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 176.22927244218548\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 943\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5860411524772644\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00979949813336134\n",
      "          model: {}\n",
      "          policy_loss: -0.03308046981692314\n",
      "          total_loss: 4.942696571350098\n",
      "          vf_explained_var: 0.6372955441474915\n",
      "          vf_loss: 4.959034442901611\n",
      "    num_agent_steps_sampled: 1256000\n",
      "    num_agent_steps_trained: 1256000\n",
      "    num_steps_sampled: 1256000\n",
      "    num_steps_trained: 1256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.1\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085051065858187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31737899132574315\n",
      "    mean_inference_ms: 0.5273426717652981\n",
      "    mean_raw_obs_processing_ms: 0.07054407845534758\n",
      "  time_since_restore: 1674.7308490276337\n",
      "  time_this_iter_s: 3.9722511768341064\n",
      "  time_total_s: 1674.7308490276337\n",
      "  timers:\n",
      "    learn_throughput: 2063.054\n",
      "    learn_time_ms: 1938.873\n",
      "    load_throughput: 32023699.179\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 998.686\n",
      "    sample_time_ms: 4005.261\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1671819233\n",
      "  timesteps_since_restore: 1256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1256000\n",
      "  training_iteration: 314\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:54 (running for 00:28:08.28)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         1674.73</td><td style=\"text-align: right;\">1256000</td><td style=\"text-align: right;\"> 176.229</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:13:59 (running for 00:28:13.29)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         1678.76</td><td style=\"text-align: right;\">1260000</td><td style=\"text-align: right;\"> 175.976</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 1470.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 175.96559820335528\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 948\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5491624474525452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007406504359096289\n",
      "          model: {}\n",
      "          policy_loss: -0.03430861979722977\n",
      "          total_loss: 4.0344157218933105\n",
      "          vf_explained_var: 0.713049590587616\n",
      "          vf_loss: 4.056069374084473\n",
      "    num_agent_steps_sampled: 1264000\n",
      "    num_agent_steps_trained: 1264000\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 29.98\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085972109736046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31741112108170816\n",
      "    mean_inference_ms: 0.5273589964149419\n",
      "    mean_raw_obs_processing_ms: 0.0705533552361938\n",
      "  time_since_restore: 1682.7557985782623\n",
      "  time_this_iter_s: 3.9930598735809326\n",
      "  time_total_s: 1682.7557985782623\n",
      "  timers:\n",
      "    learn_throughput: 2061.232\n",
      "    learn_time_ms: 1940.587\n",
      "    load_throughput: 32375947.511\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1000.257\n",
      "    sample_time_ms: 3998.974\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1671819241\n",
      "  timesteps_since_restore: 1264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 316\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:04 (running for 00:28:18.34)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         1682.76</td><td style=\"text-align: right;\">1264000</td><td style=\"text-align: right;\"> 175.966</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1470.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 1440.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 169.45870449592022\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 955\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6512280106544495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007900015451014042\n",
      "          model: {}\n",
      "          policy_loss: -0.03696248307824135\n",
      "          total_loss: 4.607110023498535\n",
      "          vf_explained_var: 0.6609721779823303\n",
      "          vf_loss: 4.6305742263793945\n",
      "    num_agent_steps_sampled: 1272000\n",
      "    num_agent_steps_trained: 1272000\n",
      "    num_steps_sampled: 1272000\n",
      "    num_steps_trained: 1272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.88333333333333\n",
      "    ram_util_percent: 29.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0908639058776375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31736991695933026\n",
      "    mean_inference_ms: 0.5274418296357117\n",
      "    mean_raw_obs_processing_ms: 0.07055173497958658\n",
      "  time_since_restore: 1690.7251870632172\n",
      "  time_this_iter_s: 3.9673259258270264\n",
      "  time_total_s: 1690.7251870632172\n",
      "  timers:\n",
      "    learn_throughput: 2061.541\n",
      "    learn_time_ms: 1940.296\n",
      "    load_throughput: 32704124.756\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 998.611\n",
      "    sample_time_ms: 4005.565\n",
      "    update_time_ms: 1.331\n",
      "  timestamp: 1671819249\n",
      "  timesteps_since_restore: 1272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1272000\n",
      "  training_iteration: 318\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:10 (running for 00:28:24.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         1690.73</td><td style=\"text-align: right;\">1272000</td><td style=\"text-align: right;\"> 169.459</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1440.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:15 (running for 00:28:29.37)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         1694.72</td><td style=\"text-align: right;\">1276000</td><td style=\"text-align: right;\"> 169.327</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1440.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 1442.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 171.25005458525985\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 960\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5024704933166504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008370338939130306\n",
      "          model: {}\n",
      "          policy_loss: -0.035832859575748444\n",
      "          total_loss: 4.913761138916016\n",
      "          vf_explained_var: 0.601920485496521\n",
      "          vf_loss: 4.935292720794678\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_agent_steps_trained: 1280000\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.16\n",
      "    ram_util_percent: 29.9\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09087306510272752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31740390194205736\n",
      "    mean_inference_ms: 0.5274592525907049\n",
      "    mean_raw_obs_processing_ms: 0.07056146569136393\n",
      "  time_since_restore: 1698.6818470954895\n",
      "  time_this_iter_s: 3.9595882892608643\n",
      "  time_total_s: 1698.6818470954895\n",
      "  timers:\n",
      "    learn_throughput: 2059.096\n",
      "    learn_time_ms: 1942.6\n",
      "    load_throughput: 31714964.083\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 998.751\n",
      "    sample_time_ms: 4005.004\n",
      "    update_time_ms: 1.333\n",
      "  timestamp: 1671819257\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 320\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:21 (running for 00:28:35.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         1702.66</td><td style=\"text-align: right;\">1284000</td><td style=\"text-align: right;\"> 174.727</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1457.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 1457.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 174.67773581126949\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 965\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4868353605270386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011540856212377548\n",
      "          model: {}\n",
      "          policy_loss: -0.03567415103316307\n",
      "          total_loss: 4.700705528259277\n",
      "          vf_explained_var: 0.6239129900932312\n",
      "          vf_loss: 4.71666145324707\n",
      "    num_agent_steps_sampled: 1288000\n",
      "    num_agent_steps_trained: 1288000\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.083333333333336\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09087670520131247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31738348531838534\n",
      "    mean_inference_ms: 0.5275101551296238\n",
      "    mean_raw_obs_processing_ms: 0.07056184834585713\n",
      "  time_since_restore: 1706.6685855388641\n",
      "  time_this_iter_s: 4.004721641540527\n",
      "  time_total_s: 1706.6685855388641\n",
      "  timers:\n",
      "    learn_throughput: 2054.634\n",
      "    learn_time_ms: 1946.819\n",
      "    load_throughput: 31661098.32\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 999.14\n",
      "    sample_time_ms: 4003.442\n",
      "    update_time_ms: 1.347\n",
      "  timestamp: 1671819265\n",
      "  timesteps_since_restore: 1288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 322\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:26 (running for 00:28:40.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         1706.67</td><td style=\"text-align: right;\">1288000</td><td style=\"text-align: right;\"> 174.678</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1457.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:32 (running for 00:28:45.43)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         1710.72</td><td style=\"text-align: right;\">1292000</td><td style=\"text-align: right;\"> 178.604</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1478.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 1493.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 182.0557621692321\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 971\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.41844621300697327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008024717681109905\n",
      "          model: {}\n",
      "          policy_loss: -0.03834105283021927\n",
      "          total_loss: 8.656558990478516\n",
      "          vf_explained_var: 0.5033231973648071\n",
      "          vf_loss: 8.68118953704834\n",
      "    num_agent_steps_sampled: 1296000\n",
      "    num_agent_steps_trained: 1296000\n",
      "    num_steps_sampled: 1296000\n",
      "    num_steps_trained: 1296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.779999999999994\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09088166496605089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173626135947564\n",
      "    mean_inference_ms: 0.5275678433452243\n",
      "    mean_raw_obs_processing_ms: 0.07056302052569037\n",
      "  time_since_restore: 1714.6392600536346\n",
      "  time_this_iter_s: 3.918887138366699\n",
      "  time_total_s: 1714.6392600536346\n",
      "  timers:\n",
      "    learn_throughput: 2054.503\n",
      "    learn_time_ms: 1946.943\n",
      "    load_throughput: 33400788.373\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 996.87\n",
      "    sample_time_ms: 4012.561\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1671819273\n",
      "  timesteps_since_restore: 1296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1296000\n",
      "  training_iteration: 324\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:37 (running for 00:28:51.38)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         1718.63</td><td style=\"text-align: right;\">1300000</td><td style=\"text-align: right;\"> 182.323</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1493.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 1493.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 182.40530237495256\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 975\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3765949308872223\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009035891853272915\n",
      "          model: {}\n",
      "          policy_loss: -0.03428514301776886\n",
      "          total_loss: 9.774237632751465\n",
      "          vf_explained_var: 0.5980774760246277\n",
      "          vf_loss: 9.793084144592285\n",
      "    num_agent_steps_sampled: 1304000\n",
      "    num_agent_steps_trained: 1304000\n",
      "    num_steps_sampled: 1304000\n",
      "    num_steps_trained: 1304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.13333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09088626523550573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173642087233867\n",
      "    mean_inference_ms: 0.5275914617647721\n",
      "    mean_raw_obs_processing_ms: 0.07056612169786422\n",
      "  time_since_restore: 1722.5920476913452\n",
      "  time_this_iter_s: 3.9608609676361084\n",
      "  time_total_s: 1722.5920476913452\n",
      "  timers:\n",
      "    learn_throughput: 2056.957\n",
      "    learn_time_ms: 1944.62\n",
      "    load_throughput: 34106964.83\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 997.726\n",
      "    sample_time_ms: 4009.115\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1671819281\n",
      "  timesteps_since_restore: 1304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1304000\n",
      "  training_iteration: 326\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:43 (running for 00:28:57.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         1722.59</td><td style=\"text-align: right;\">1304000</td><td style=\"text-align: right;\"> 182.405</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1493.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:49 (running for 00:29:02.43)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         1726.61</td><td style=\"text-align: right;\">1308000</td><td style=\"text-align: right;\"> 178.388</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1473.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-49\n",
      "  done: false\n",
      "  episode_len_mean: 1473.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 237.7994186833128\n",
      "  episode_reward_mean: 178.518792694164\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 982\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3391016721725464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00939147174358368\n",
      "          model: {}\n",
      "          policy_loss: -0.03287900611758232\n",
      "          total_loss: 7.664388179779053\n",
      "          vf_explained_var: 0.5104994773864746\n",
      "          vf_loss: 7.681220531463623\n",
      "    num_agent_steps_sampled: 1312000\n",
      "    num_agent_steps_trained: 1312000\n",
      "    num_steps_sampled: 1312000\n",
      "    num_steps_trained: 1312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.416666666666664\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09089491781946918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31736993940244906\n",
      "    mean_inference_ms: 0.5276369206916195\n",
      "    mean_raw_obs_processing_ms: 0.07057250277421817\n",
      "  time_since_restore: 1730.5940294265747\n",
      "  time_this_iter_s: 3.9850881099700928\n",
      "  time_total_s: 1730.5940294265747\n",
      "  timers:\n",
      "    learn_throughput: 2053.909\n",
      "    learn_time_ms: 1947.506\n",
      "    load_throughput: 33777362.593\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 998.42\n",
      "    sample_time_ms: 4006.33\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1671819289\n",
      "  timesteps_since_restore: 1312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1312000\n",
      "  training_iteration: 328\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:14:54 (running for 00:29:08.39)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         1734.56</td><td style=\"text-align: right;\">1316000</td><td style=\"text-align: right;\"> 178.519</td><td style=\"text-align: right;\">             237.799</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1473.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 1473.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 233.3591299053538\n",
      "  episode_reward_mean: 178.4344813551048\n",
      "  episode_reward_min: -124.46880284822795\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 986\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36397531628608704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008718147873878479\n",
      "          model: {}\n",
      "          policy_loss: -0.032213278114795685\n",
      "          total_loss: 4.85113525390625\n",
      "          vf_explained_var: 0.6625667810440063\n",
      "          vf_loss: 4.868453025817871\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.48\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09089982769973895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31737293672741257\n",
      "    mean_inference_ms: 0.5276626276643421\n",
      "    mean_raw_obs_processing_ms: 0.07057578745205441\n",
      "  time_since_restore: 1738.529235124588\n",
      "  time_this_iter_s: 3.9651265144348145\n",
      "  time_total_s: 1738.529235124588\n",
      "  timers:\n",
      "    learn_throughput: 2058.451\n",
      "    learn_time_ms: 1943.209\n",
      "    load_throughput: 34843646.937\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 997.314\n",
      "    sample_time_ms: 4010.773\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1671819297\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 330\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:00 (running for 00:29:13.42)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         1738.53</td><td style=\"text-align: right;\">1320000</td><td style=\"text-align: right;\"> 178.434</td><td style=\"text-align: right;\">             233.359</td><td style=\"text-align: right;\">            -124.469</td><td style=\"text-align: right;\">           1473.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 1487.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 233.3591299053538\n",
      "  episode_reward_mean: 182.06317958047154\n",
      "  episode_reward_min: -123.84886225136803\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 992\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4558686912059784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007437818683683872\n",
      "          model: {}\n",
      "          policy_loss: -0.03304281830787659\n",
      "          total_loss: 4.452449321746826\n",
      "          vf_explained_var: 0.6407299041748047\n",
      "          vf_loss: 4.472784519195557\n",
      "    num_agent_steps_sampled: 1328000\n",
      "    num_agent_steps_trained: 1328000\n",
      "    num_steps_sampled: 1328000\n",
      "    num_steps_trained: 1328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.449999999999996\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090227480113161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173446517244722\n",
      "    mean_inference_ms: 0.5277051086812901\n",
      "    mean_raw_obs_processing_ms: 0.07057444466562603\n",
      "  time_since_restore: 1746.4458467960358\n",
      "  time_this_iter_s: 3.9208626747131348\n",
      "  time_total_s: 1746.4458467960358\n",
      "  timers:\n",
      "    learn_throughput: 2063.976\n",
      "    learn_time_ms: 1938.007\n",
      "    load_throughput: 34757025.067\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 998.566\n",
      "    sample_time_ms: 4005.743\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671819305\n",
      "  timesteps_since_restore: 1328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1328000\n",
      "  training_iteration: 332\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:05 (running for 00:29:19.33)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         1746.45</td><td style=\"text-align: right;\">1328000</td><td style=\"text-align: right;\"> 182.063</td><td style=\"text-align: right;\">             233.359</td><td style=\"text-align: right;\">            -123.849</td><td style=\"text-align: right;\">           1487.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:10 (running for 00:29:24.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         1750.46</td><td style=\"text-align: right;\">1332000</td><td style=\"text-align: right;\"> 182.536</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -123.849</td><td style=\"text-align: right;\">           1487.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-13\n",
      "  done: false\n",
      "  episode_len_mean: 1471.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 178.0929471068019\n",
      "  episode_reward_min: -124.76896839992578\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 998\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4540668725967407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008279788307845592\n",
      "          model: {}\n",
      "          policy_loss: -0.07282809168100357\n",
      "          total_loss: 84.2465591430664\n",
      "          vf_explained_var: 0.10994373261928558\n",
      "          vf_loss: 84.30523681640625\n",
      "    num_agent_steps_sampled: 1336000\n",
      "    num_agent_steps_trained: 1336000\n",
      "    num_steps_sampled: 1336000\n",
      "    num_steps_trained: 1336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.61666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090238391199257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173276232751532\n",
      "    mean_inference_ms: 0.5276991858034533\n",
      "    mean_raw_obs_processing_ms: 0.0705725980274587\n",
      "  time_since_restore: 1754.3943717479706\n",
      "  time_this_iter_s: 3.93669056892395\n",
      "  time_total_s: 1754.3943717479706\n",
      "  timers:\n",
      "    learn_throughput: 2061.969\n",
      "    learn_time_ms: 1939.893\n",
      "    load_throughput: 32251472.511\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1001.341\n",
      "    sample_time_ms: 3994.644\n",
      "    update_time_ms: 1.335\n",
      "  timestamp: 1671819313\n",
      "  timesteps_since_restore: 1336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1336000\n",
      "  training_iteration: 334\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:16 (running for 00:29:30.32)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         1754.39</td><td style=\"text-align: right;\">1336000</td><td style=\"text-align: right;\"> 178.093</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1471.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 1444.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 172.7004927744117\n",
      "  episode_reward_min: -124.76896839992578\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1006\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4438202977180481\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009242678992450237\n",
      "          model: {}\n",
      "          policy_loss: -0.03794991597533226\n",
      "          total_loss: 21.420665740966797\n",
      "          vf_explained_var: 0.26015400886535645\n",
      "          vf_loss: 21.442821502685547\n",
      "    num_agent_steps_sampled: 1344000\n",
      "    num_agent_steps_trained: 1344000\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.583333333333336\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090117026934834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173177516287392\n",
      "    mean_inference_ms: 0.5276458506535174\n",
      "    mean_raw_obs_processing_ms: 0.07057115812034492\n",
      "  time_since_restore: 1762.3637924194336\n",
      "  time_this_iter_s: 3.953172206878662\n",
      "  time_total_s: 1762.3637924194336\n",
      "  timers:\n",
      "    learn_throughput: 2059.653\n",
      "    learn_time_ms: 1942.075\n",
      "    load_throughput: 30344033.279\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1000.883\n",
      "    sample_time_ms: 3996.473\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1671819321\n",
      "  timesteps_since_restore: 1344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 336\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:21 (running for 00:29:35.33)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         1762.36</td><td style=\"text-align: right;\">1344000</td><td style=\"text-align: right;\">   172.7</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1444.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:27 (running for 00:29:41.32)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         1766.34</td><td style=\"text-align: right;\">1348000</td><td style=\"text-align: right;\"> 174.179</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1447.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-29\n",
      "  done: false\n",
      "  episode_len_mean: 1447.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 174.0709173452089\n",
      "  episode_reward_min: -124.76896839992578\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6051448583602905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008567959070205688\n",
      "          model: {}\n",
      "          policy_loss: -0.036606207489967346\n",
      "          total_loss: 4.3061957359313965\n",
      "          vf_explained_var: 0.5897098183631897\n",
      "          vf_loss: 4.328162670135498\n",
      "    num_agent_steps_sampled: 1352000\n",
      "    num_agent_steps_trained: 1352000\n",
      "    num_steps_sampled: 1352000\n",
      "    num_steps_trained: 1352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090210465848526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31732817064755897\n",
      "    mean_inference_ms: 0.5276125261789231\n",
      "    mean_raw_obs_processing_ms: 0.07057280240960478\n",
      "  time_since_restore: 1770.3163385391235\n",
      "  time_this_iter_s: 3.975830078125\n",
      "  time_total_s: 1770.3163385391235\n",
      "  timers:\n",
      "    learn_throughput: 2065.59\n",
      "    learn_time_ms: 1936.493\n",
      "    load_throughput: 26681323.155\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 1001.229\n",
      "    sample_time_ms: 3995.09\n",
      "    update_time_ms: 1.35\n",
      "  timestamp: 1671819329\n",
      "  timesteps_since_restore: 1352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1352000\n",
      "  training_iteration: 338\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:32 (running for 00:29:46.37)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         1770.32</td><td style=\"text-align: right;\">1352000</td><td style=\"text-align: right;\"> 174.071</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1447.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-37\n",
      "  done: false\n",
      "  episode_len_mean: 1462.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 177.41351987320567\n",
      "  episode_reward_min: -124.76896839992578\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1016\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.49100327491760254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00887451134622097\n",
      "          model: {}\n",
      "          policy_loss: -0.03768984600901604\n",
      "          total_loss: 5.158135414123535\n",
      "          vf_explained_var: 0.6736761331558228\n",
      "          vf_loss: 5.180662155151367\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_agent_steps_trained: 1360000\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.6\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09089946675153084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3173036732466813\n",
      "    mean_inference_ms: 0.5275870595596889\n",
      "    mean_raw_obs_processing_ms: 0.07056831947729741\n",
      "  time_since_restore: 1778.2700788974762\n",
      "  time_this_iter_s: 3.965592384338379\n",
      "  time_total_s: 1778.2700788974762\n",
      "  timers:\n",
      "    learn_throughput: 2060.06\n",
      "    learn_time_ms: 1941.691\n",
      "    load_throughput: 26437466.12\n",
      "    load_time_ms: 0.151\n",
      "    sample_throughput: 1002.022\n",
      "    sample_time_ms: 3991.929\n",
      "    update_time_ms: 1.344\n",
      "  timestamp: 1671819337\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 340\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:38 (running for 00:29:52.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         1778.27</td><td style=\"text-align: right;\">1360000</td><td style=\"text-align: right;\"> 177.414</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1462.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:43 (running for 00:29:57.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         1782.26</td><td style=\"text-align: right;\">1364000</td><td style=\"text-align: right;\"> 177.853</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1462.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 1469.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 179.82641136686664\n",
      "  episode_reward_min: -124.76896839992578\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4214198887348175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008346213959157467\n",
      "          model: {}\n",
      "          policy_loss: -0.031151114031672478\n",
      "          total_loss: 4.68673849105835\n",
      "          vf_explained_var: 0.6858780384063721\n",
      "          vf_loss: 4.703629016876221\n",
      "    num_agent_steps_sampled: 1368000\n",
      "    num_agent_steps_trained: 1368000\n",
      "    num_steps_sampled: 1368000\n",
      "    num_steps_trained: 1368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.98333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0908976832490703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172867466099519\n",
      "    mean_inference_ms: 0.5275736341620196\n",
      "    mean_raw_obs_processing_ms: 0.07056516398670155\n",
      "  time_since_restore: 1786.3472406864166\n",
      "  time_this_iter_s: 4.089438438415527\n",
      "  time_total_s: 1786.3472406864166\n",
      "  timers:\n",
      "    learn_throughput: 2062.894\n",
      "    learn_time_ms: 1939.023\n",
      "    load_throughput: 26504290.679\n",
      "    load_time_ms: 0.151\n",
      "    sample_throughput: 998.208\n",
      "    sample_time_ms: 4007.183\n",
      "    update_time_ms: 1.338\n",
      "  timestamp: 1671819345\n",
      "  timesteps_since_restore: 1368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1368000\n",
      "  training_iteration: 342\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:49 (running for 00:30:02.43)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         1786.35</td><td style=\"text-align: right;\">1368000</td><td style=\"text-align: right;\"> 179.826</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.769</td><td style=\"text-align: right;\">           1469.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 1469.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 180.62032659457017\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1027\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2094411849975586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006740041077136993\n",
      "          model: {}\n",
      "          policy_loss: -0.03027893230319023\n",
      "          total_loss: 22.304990768432617\n",
      "          vf_explained_var: 0.6252467036247253\n",
      "          vf_loss: 22.323753356933594\n",
      "    num_agent_steps_sampled: 1376000\n",
      "    num_agent_steps_trained: 1376000\n",
      "    num_steps_sampled: 1376000\n",
      "    num_steps_trained: 1376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.6\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09089350418641615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172528396222547\n",
      "    mean_inference_ms: 0.5275505714386656\n",
      "    mean_raw_obs_processing_ms: 0.07055860361546235\n",
      "  time_since_restore: 1794.2428584098816\n",
      "  time_this_iter_s: 3.905517339706421\n",
      "  time_total_s: 1794.2428584098816\n",
      "  timers:\n",
      "    learn_throughput: 2064.446\n",
      "    learn_time_ms: 1937.566\n",
      "    load_throughput: 28315976.371\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 998.575\n",
      "    sample_time_ms: 4005.709\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1671819353\n",
      "  timesteps_since_restore: 1376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1376000\n",
      "  training_iteration: 344\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:15:54 (running for 00:30:08.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         1794.24</td><td style=\"text-align: right;\">1376000</td><td style=\"text-align: right;\">  180.62</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1469.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:00 (running for 00:30:14.32)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         1798.19</td><td style=\"text-align: right;\">1380000</td><td style=\"text-align: right;\"> 180.915</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1469.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-01\n",
      "  done: false\n",
      "  episode_len_mean: 1460.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 178.1321944242264\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1032\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3903031647205353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008711587637662888\n",
      "          model: {}\n",
      "          policy_loss: -0.033949971199035645\n",
      "          total_loss: 15.910520553588867\n",
      "          vf_explained_var: 0.4995027482509613\n",
      "          vf_loss: 15.929585456848145\n",
      "    num_agent_steps_sampled: 1384000\n",
      "    num_agent_steps_trained: 1384000\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.71666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09089280224644586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172528479870461\n",
      "    mean_inference_ms: 0.5275152784097142\n",
      "    mean_raw_obs_processing_ms: 0.07055830558135431\n",
      "  time_since_restore: 1802.1471037864685\n",
      "  time_this_iter_s: 3.960905075073242\n",
      "  time_total_s: 1802.1471037864685\n",
      "  timers:\n",
      "    learn_throughput: 2063.705\n",
      "    learn_time_ms: 1938.262\n",
      "    load_throughput: 29986087.578\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 1001.665\n",
      "    sample_time_ms: 3993.353\n",
      "    update_time_ms: 1.338\n",
      "  timestamp: 1671819361\n",
      "  timesteps_since_restore: 1384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 346\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:06 (running for 00:30:20.31)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         1806.13</td><td style=\"text-align: right;\">1388000</td><td style=\"text-align: right;\"> 178.229</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1460.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 1460.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 178.33038715488289\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1037\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47751542925834656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077273170463740826\n",
      "          model: {}\n",
      "          policy_loss: -0.03783181309700012\n",
      "          total_loss: 4.8437910079956055\n",
      "          vf_explained_var: 0.5798079967498779\n",
      "          vf_loss: 4.868420124053955\n",
      "    num_agent_steps_sampled: 1392000\n",
      "    num_agent_steps_trained: 1392000\n",
      "    num_steps_sampled: 1392000\n",
      "    num_steps_trained: 1392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0908871499830374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172018443566069\n",
      "    mean_inference_ms: 0.5275161983036104\n",
      "    mean_raw_obs_processing_ms: 0.07054914830046047\n",
      "  time_since_restore: 1810.1070630550385\n",
      "  time_this_iter_s: 3.974260091781616\n",
      "  time_total_s: 1810.1070630550385\n",
      "  timers:\n",
      "    learn_throughput: 2062.108\n",
      "    learn_time_ms: 1939.763\n",
      "    load_throughput: 34923430.475\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1000.487\n",
      "    sample_time_ms: 3998.053\n",
      "    update_time_ms: 1.319\n",
      "  timestamp: 1671819369\n",
      "  timesteps_since_restore: 1392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1392000\n",
      "  training_iteration: 348\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:12 (running for 00:30:26.30)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         1810.11</td><td style=\"text-align: right;\">1392000</td><td style=\"text-align: right;\">  178.33</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1460.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 1460.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 179.24446679237658\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1042\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.35898175835609436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009196624159812927\n",
      "          model: {}\n",
      "          policy_loss: -0.03256221488118172\n",
      "          total_loss: 7.9817376136779785\n",
      "          vf_explained_var: 0.5836070775985718\n",
      "          vf_loss: 7.998586654663086\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_agent_steps_trained: 1400000\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.44\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0908864282359247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.317201826707311\n",
      "    mean_inference_ms: 0.5274800715743914\n",
      "    mean_raw_obs_processing_ms: 0.07054916940114546\n",
      "  time_since_restore: 1818.0206167697906\n",
      "  time_this_iter_s: 3.9246416091918945\n",
      "  time_total_s: 1818.0206167697906\n",
      "  timers:\n",
      "    learn_throughput: 2062.975\n",
      "    learn_time_ms: 1938.947\n",
      "    load_throughput: 35372582.754\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1001.02\n",
      "    sample_time_ms: 3995.922\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671819377\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 350\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:18 (running for 00:30:32.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         1818.02</td><td style=\"text-align: right;\">1400000</td><td style=\"text-align: right;\"> 179.244</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1460.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:23 (running for 00:30:37.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         1822.01</td><td style=\"text-align: right;\">1404000</td><td style=\"text-align: right;\"> 179.415</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1460.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 1460.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 179.54579730052396\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1047\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3279193639755249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008671561256051064\n",
      "          model: {}\n",
      "          policy_loss: -0.03422034904360771\n",
      "          total_loss: 6.009403228759766\n",
      "          vf_explained_var: 0.5591782927513123\n",
      "          vf_loss: 6.028807640075684\n",
      "    num_agent_steps_sampled: 1408000\n",
      "    num_agent_steps_trained: 1408000\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09088038236092613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171489191960608\n",
      "    mean_inference_ms: 0.5274778433809174\n",
      "    mean_raw_obs_processing_ms: 0.07053967622216217\n",
      "  time_since_restore: 1825.9282207489014\n",
      "  time_this_iter_s: 3.9223763942718506\n",
      "  time_total_s: 1825.9282207489014\n",
      "  timers:\n",
      "    learn_throughput: 2061.044\n",
      "    learn_time_ms: 1940.764\n",
      "    load_throughput: 35357673.34\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1005.804\n",
      "    sample_time_ms: 3976.917\n",
      "    update_time_ms: 1.338\n",
      "  timestamp: 1671819385\n",
      "  timesteps_since_restore: 1408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 352\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:29 (running for 00:30:43.21)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         1829.92</td><td style=\"text-align: right;\">1412000</td><td style=\"text-align: right;\"> 181.243</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1466.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-33\n",
      "  done: false\n",
      "  episode_len_mean: 1481.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 185.05579534922393\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1052\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3367745876312256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009882028214633465\n",
      "          model: {}\n",
      "          policy_loss: -0.039536163210868835\n",
      "          total_loss: 6.469747543334961\n",
      "          vf_explained_var: 0.42724162340164185\n",
      "          vf_loss: 6.4923996925354\n",
      "    num_agent_steps_sampled: 1416000\n",
      "    num_agent_steps_trained: 1416000\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.26666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09088152960838704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171717358594071\n",
      "    mean_inference_ms: 0.5274203837187079\n",
      "    mean_raw_obs_processing_ms: 0.07054336847839672\n",
      "  time_since_restore: 1833.9267473220825\n",
      "  time_this_iter_s: 4.008532762527466\n",
      "  time_total_s: 1833.9267473220825\n",
      "  timers:\n",
      "    learn_throughput: 2061.558\n",
      "    learn_time_ms: 1940.28\n",
      "    load_throughput: 35469801.268\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1003.094\n",
      "    sample_time_ms: 3987.664\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671819393\n",
      "  timesteps_since_restore: 1416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 354\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:34 (running for 00:30:48.24)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         1833.93</td><td style=\"text-align: right;\">1416000</td><td style=\"text-align: right;\"> 185.056</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1481.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:39 (running for 00:30:53.26)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         1837.93</td><td style=\"text-align: right;\">1420000</td><td style=\"text-align: right;\"> 183.498</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1475.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-41\n",
      "  done: false\n",
      "  episode_len_mean: 1475.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 183.99142784073965\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1058\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3583618104457855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009602149948477745\n",
      "          model: {}\n",
      "          policy_loss: -0.039937812834978104\n",
      "          total_loss: 5.78403377532959\n",
      "          vf_explained_var: 0.5150570869445801\n",
      "          vf_loss: 5.807565212249756\n",
      "    num_agent_steps_sampled: 1424000\n",
      "    num_agent_steps_trained: 1424000\n",
      "    num_steps_sampled: 1424000\n",
      "    num_steps_trained: 1424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.516666666666666\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09087512607723275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31711512144593434\n",
      "    mean_inference_ms: 0.5274135367232035\n",
      "    mean_raw_obs_processing_ms: 0.07053289086917804\n",
      "  time_since_restore: 1841.9263668060303\n",
      "  time_this_iter_s: 3.9924185276031494\n",
      "  time_total_s: 1841.9263668060303\n",
      "  timers:\n",
      "    learn_throughput: 2060.529\n",
      "    learn_time_ms: 1941.249\n",
      "    load_throughput: 34421862.946\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1000.676\n",
      "    sample_time_ms: 3997.299\n",
      "    update_time_ms: 1.383\n",
      "  timestamp: 1671819401\n",
      "  timesteps_since_restore: 1424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1424000\n",
      "  training_iteration: 356\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:44 (running for 00:30:58.28)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         1841.93</td><td style=\"text-align: right;\">1424000</td><td style=\"text-align: right;\"> 183.991</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1475.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 1474.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 182.9480733233851\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1063\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4150065779685974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009604109451174736\n",
      "          model: {}\n",
      "          policy_loss: -0.03648470714688301\n",
      "          total_loss: 5.212862491607666\n",
      "          vf_explained_var: 0.5799188017845154\n",
      "          vf_loss: 5.232937335968018\n",
      "    num_agent_steps_sampled: 1432000\n",
      "    num_agent_steps_trained: 1432000\n",
      "    num_steps_sampled: 1432000\n",
      "    num_steps_trained: 1432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.66666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09087191824556724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31708983002915986\n",
      "    mean_inference_ms: 0.5273936818650672\n",
      "    mean_raw_obs_processing_ms: 0.07052829160232099\n",
      "  time_since_restore: 1849.910133600235\n",
      "  time_this_iter_s: 3.9839859008789062\n",
      "  time_total_s: 1849.910133600235\n",
      "  timers:\n",
      "    learn_throughput: 2052.451\n",
      "    learn_time_ms: 1948.89\n",
      "    load_throughput: 34492631.579\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1000.975\n",
      "    sample_time_ms: 3996.102\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671819409\n",
      "  timesteps_since_restore: 1432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1432000\n",
      "  training_iteration: 358\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:49 (running for 00:31:03.30)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         1849.91</td><td style=\"text-align: right;\">1432000</td><td style=\"text-align: right;\"> 182.948</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1474.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:16:54 (running for 00:31:08.32)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         1853.91</td><td style=\"text-align: right;\">1436000</td><td style=\"text-align: right;\"> 183.413</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1474.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 1472.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.17490499273958\n",
      "  episode_reward_mean: 183.4914170317542\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1068\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3737556040287018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007067975588142872\n",
      "          model: {}\n",
      "          policy_loss: -0.039630547165870667\n",
      "          total_loss: 26.421735763549805\n",
      "          vf_explained_var: 0.6514424681663513\n",
      "          vf_loss: 26.449295043945312\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.9\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09086852256601904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3170642496461976\n",
      "    mean_inference_ms: 0.5273719679253132\n",
      "    mean_raw_obs_processing_ms: 0.07052345277253935\n",
      "  time_since_restore: 1857.856971502304\n",
      "  time_this_iter_s: 3.9454941749572754\n",
      "  time_total_s: 1857.856971502304\n",
      "  timers:\n",
      "    learn_throughput: 2056.656\n",
      "    learn_time_ms: 1944.905\n",
      "    load_throughput: 34337322.964\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 999.312\n",
      "    sample_time_ms: 4002.755\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671819417\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 360\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:00 (running for 00:31:14.28)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         1857.86</td><td style=\"text-align: right;\">1440000</td><td style=\"text-align: right;\"> 183.491</td><td style=\"text-align: right;\">             236.175</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1472.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 1447.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 178.16443261363375\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28293758630752563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008654574863612652\n",
      "          model: {}\n",
      "          policy_loss: -0.038096461445093155\n",
      "          total_loss: 92.83086395263672\n",
      "          vf_explained_var: 0.3694133460521698\n",
      "          vf_loss: 92.85417175292969\n",
      "    num_agent_steps_sampled: 1448000\n",
      "    num_agent_steps_trained: 1448000\n",
      "    num_steps_sampled: 1448000\n",
      "    num_steps_trained: 1448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.28333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09086363621395818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31702868274316937\n",
      "    mean_inference_ms: 0.5273419851455906\n",
      "    mean_raw_obs_processing_ms: 0.07051713661018778\n",
      "  time_since_restore: 1865.8195011615753\n",
      "  time_this_iter_s: 3.946939468383789\n",
      "  time_total_s: 1865.8195011615753\n",
      "  timers:\n",
      "    learn_throughput: 2055.2\n",
      "    learn_time_ms: 1946.283\n",
      "    load_throughput: 34393636.736\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 997.601\n",
      "    sample_time_ms: 4009.619\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1671819425\n",
      "  timesteps_since_restore: 1448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1448000\n",
      "  training_iteration: 362\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:05 (running for 00:31:19.29)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         1865.82</td><td style=\"text-align: right;\">1448000</td><td style=\"text-align: right;\"> 178.164</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1447.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:11 (running for 00:31:25.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         1869.79</td><td style=\"text-align: right;\">1452000</td><td style=\"text-align: right;\"> 182.827</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1467.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 1467.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 182.81503860126196\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.24229955673217773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008849875070154667\n",
      "          model: {}\n",
      "          policy_loss: -0.03909898176789284\n",
      "          total_loss: 4.675510406494141\n",
      "          vf_explained_var: 0.48812755942344666\n",
      "          vf_loss: 4.699488639831543\n",
      "    num_agent_steps_sampled: 1456000\n",
      "    num_agent_steps_trained: 1456000\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.480000000000004\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085981895130801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31700160887851064\n",
      "    mean_inference_ms: 0.5273188611245424\n",
      "    mean_raw_obs_processing_ms: 0.07051224568284613\n",
      "  time_since_restore: 1873.7503802776337\n",
      "  time_this_iter_s: 3.9587857723236084\n",
      "  time_total_s: 1873.7503802776337\n",
      "  timers:\n",
      "    learn_throughput: 2054.328\n",
      "    learn_time_ms: 1947.108\n",
      "    load_throughput: 33989497.569\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 999.898\n",
      "    sample_time_ms: 4000.408\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1671819433\n",
      "  timesteps_since_restore: 1456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 364\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:17 (running for 00:31:31.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         1877.75</td><td style=\"text-align: right;\">1460000</td><td style=\"text-align: right;\"> 182.503</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1467.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-21\n",
      "  done: false\n",
      "  episode_len_mean: 1461.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 180.6120221654741\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1086\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33040058612823486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004854461643844843\n",
      "          model: {}\n",
      "          policy_loss: -0.032762665301561356\n",
      "          total_loss: 27.73038101196289\n",
      "          vf_explained_var: 0.42970240116119385\n",
      "          vf_loss: 27.754846572875977\n",
      "    num_agent_steps_sampled: 1464000\n",
      "    num_agent_steps_trained: 1464000\n",
      "    num_steps_sampled: 1464000\n",
      "    num_steps_trained: 1464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.666666666666664\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085541767600869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169695942183264\n",
      "    mean_inference_ms: 0.5272913141456367\n",
      "    mean_raw_obs_processing_ms: 0.07050692906000613\n",
      "  time_since_restore: 1881.6996858119965\n",
      "  time_this_iter_s: 3.953639268875122\n",
      "  time_total_s: 1881.6996858119965\n",
      "  timers:\n",
      "    learn_throughput: 2059.04\n",
      "    learn_time_ms: 1942.653\n",
      "    load_throughput: 35010884.808\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 999.871\n",
      "    sample_time_ms: 4000.515\n",
      "    update_time_ms: 1.316\n",
      "  timestamp: 1671819441\n",
      "  timesteps_since_restore: 1464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1464000\n",
      "  training_iteration: 366\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:23 (running for 00:31:37.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">          1881.7</td><td style=\"text-align: right;\">1464000</td><td style=\"text-align: right;\"> 180.612</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">            1461.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:28 (running for 00:31:42.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">          1885.7</td><td style=\"text-align: right;\">1468000</td><td style=\"text-align: right;\"> 180.621</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">            1461.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 1461.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 180.44317881927986\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1090\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.21757957339286804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014361850917339325\n",
      "          model: {}\n",
      "          policy_loss: -0.03524860739707947\n",
      "          total_loss: 5.515250205993652\n",
      "          vf_explained_var: 0.5808554291725159\n",
      "          vf_loss: 5.538229465484619\n",
      "    num_agent_steps_sampled: 1472000\n",
      "    num_agent_steps_trained: 1472000\n",
      "    num_steps_sampled: 1472000\n",
      "    num_steps_trained: 1472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.18333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085257293103403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169485369720132\n",
      "    mean_inference_ms: 0.5272736973921309\n",
      "    mean_raw_obs_processing_ms: 0.07050333494765097\n",
      "  time_since_restore: 1889.6603786945343\n",
      "  time_this_iter_s: 3.956174850463867\n",
      "  time_total_s: 1889.6603786945343\n",
      "  timers:\n",
      "    learn_throughput: 2063.304\n",
      "    learn_time_ms: 1938.639\n",
      "    load_throughput: 32394701.68\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 1001.125\n",
      "    sample_time_ms: 3995.505\n",
      "    update_time_ms: 1.305\n",
      "  timestamp: 1671819449\n",
      "  timesteps_since_restore: 1472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1472000\n",
      "  training_iteration: 368\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:34 (running for 00:31:48.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         1893.61</td><td style=\"text-align: right;\">1476000</td><td style=\"text-align: right;\"> 180.192</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">            1461.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 1461.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 179.5228674913542\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1097\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.49173372983932495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011964238248765469\n",
      "          model: {}\n",
      "          policy_loss: -0.034567005932331085\n",
      "          total_loss: 71.44500732421875\n",
      "          vf_explained_var: 0.5367920398712158\n",
      "          vf_loss: 71.46935272216797\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_agent_steps_trained: 1480000\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.72\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09085006334399534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31693679204018055\n",
      "    mean_inference_ms: 0.5272242216953223\n",
      "    mean_raw_obs_processing_ms: 0.07050172954177363\n",
      "  time_since_restore: 1897.5897710323334\n",
      "  time_this_iter_s: 3.976475715637207\n",
      "  time_total_s: 1897.5897710323334\n",
      "  timers:\n",
      "    learn_throughput: 2062.515\n",
      "    learn_time_ms: 1939.38\n",
      "    load_throughput: 32570794.021\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 1001.646\n",
      "    sample_time_ms: 3993.428\n",
      "    update_time_ms: 1.32\n",
      "  timestamp: 1671819457\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 370\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:40 (running for 00:31:54.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         1897.59</td><td style=\"text-align: right;\">1480000</td><td style=\"text-align: right;\"> 179.523</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1461.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-45\n",
      "  done: false\n",
      "  episode_len_mean: 1477.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 183.0035956685785\n",
      "  episode_reward_min: -124.81640224261147\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1102\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3707936406135559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011369138956069946\n",
      "          model: {}\n",
      "          policy_loss: -0.04428251087665558\n",
      "          total_loss: 23.30701446533203\n",
      "          vf_explained_var: 0.3019463121891022\n",
      "          vf_loss: 23.341583251953125\n",
      "    num_agent_steps_sampled: 1488000\n",
      "    num_agent_steps_trained: 1488000\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09084431583892062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31688550294409934\n",
      "    mean_inference_ms: 0.5272227595595291\n",
      "    mean_raw_obs_processing_ms: 0.07049289744598751\n",
      "  time_since_restore: 1905.5799701213837\n",
      "  time_this_iter_s: 3.9459149837493896\n",
      "  time_total_s: 1905.5799701213837\n",
      "  timers:\n",
      "    learn_throughput: 2061.328\n",
      "    learn_time_ms: 1940.496\n",
      "    load_throughput: 32507684.557\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 1000.764\n",
      "    sample_time_ms: 3996.948\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1671819465\n",
      "  timesteps_since_restore: 1488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 372\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:46 (running for 00:32:00.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         1905.58</td><td style=\"text-align: right;\">1488000</td><td style=\"text-align: right;\"> 183.004</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -124.816</td><td style=\"text-align: right;\">           1477.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:51 (running for 00:32:05.29)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         1909.57</td><td style=\"text-align: right;\">1492000</td><td style=\"text-align: right;\"> 183.386</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -125.464</td><td style=\"text-align: right;\">           1477.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 1463.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 180.16479982127788\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1109\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29298245906829834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013322848826646805\n",
      "          model: {}\n",
      "          policy_loss: -0.041356030851602554\n",
      "          total_loss: 24.43768882751465\n",
      "          vf_explained_var: 0.4528323709964752\n",
      "          vf_loss: 24.467662811279297\n",
      "    num_agent_steps_sampled: 1496000\n",
      "    num_agent_steps_trained: 1496000\n",
      "    num_steps_sampled: 1496000\n",
      "    num_steps_trained: 1496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.559999999999995\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09083776931475869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31682591065868165\n",
      "    mean_inference_ms: 0.5272154169576233\n",
      "    mean_raw_obs_processing_ms: 0.0704827019831554\n",
      "  time_since_restore: 1913.675877571106\n",
      "  time_this_iter_s: 4.100974798202515\n",
      "  time_total_s: 1913.675877571106\n",
      "  timers:\n",
      "    learn_throughput: 2058.014\n",
      "    learn_time_ms: 1943.621\n",
      "    load_throughput: 32870721.003\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 997.476\n",
      "    sample_time_ms: 4010.123\n",
      "    update_time_ms: 1.307\n",
      "  timestamp: 1671819473\n",
      "  timesteps_since_restore: 1496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1496000\n",
      "  training_iteration: 374\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:17:56 (running for 00:32:10.38)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         1913.68</td><td style=\"text-align: right;\">1496000</td><td style=\"text-align: right;\"> 180.165</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1463.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-01\n",
      "  done: false\n",
      "  episode_len_mean: 1463.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 180.90748558006132\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1114\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2598828077316284\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01543782651424408\n",
      "          model: {}\n",
      "          policy_loss: -0.04256850481033325\n",
      "          total_loss: 6.210050106048584\n",
      "          vf_explained_var: 0.5353854298591614\n",
      "          vf_loss: 6.239430904388428\n",
      "    num_agent_steps_sampled: 1504000\n",
      "    num_agent_steps_trained: 1504000\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.333333333333336\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09083519579918718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31680321667288847\n",
      "    mean_inference_ms: 0.5271985924213787\n",
      "    mean_raw_obs_processing_ms: 0.0704793101812831\n",
      "  time_since_restore: 1921.6695806980133\n",
      "  time_this_iter_s: 3.947122097015381\n",
      "  time_total_s: 1921.6695806980133\n",
      "  timers:\n",
      "    learn_throughput: 2052.489\n",
      "    learn_time_ms: 1948.853\n",
      "    load_throughput: 32851411.788\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 996.029\n",
      "    sample_time_ms: 4015.947\n",
      "    update_time_ms: 1.316\n",
      "  timestamp: 1671819481\n",
      "  timesteps_since_restore: 1504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:02 (running for 00:32:15.45)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         1921.67</td><td style=\"text-align: right;\">1504000</td><td style=\"text-align: right;\"> 180.907</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1463.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:08 (running for 00:32:21.44)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         1925.68</td><td style=\"text-align: right;\">1508000</td><td style=\"text-align: right;\"> 181.228</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1463.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 1461.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 179.87395778521105\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1119\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29470086097717285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014020903035998344\n",
      "          model: {}\n",
      "          policy_loss: -0.039081282913684845\n",
      "          total_loss: 70.48729705810547\n",
      "          vf_explained_var: 0.46030181646347046\n",
      "          vf_loss: 70.51439666748047\n",
      "    num_agent_steps_sampled: 1512000\n",
      "    num_agent_steps_trained: 1512000\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.94\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09083268571330361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31678180955998386\n",
      "    mean_inference_ms: 0.5271807317021477\n",
      "    mean_raw_obs_processing_ms: 0.07047562309571828\n",
      "  time_since_restore: 1929.6303889751434\n",
      "  time_this_iter_s: 3.948120594024658\n",
      "  time_total_s: 1929.6303889751434\n",
      "  timers:\n",
      "    learn_throughput: 2055.589\n",
      "    learn_time_ms: 1945.914\n",
      "    load_throughput: 35567555.65\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 995.601\n",
      "    sample_time_ms: 4017.674\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671819489\n",
      "  timesteps_since_restore: 1512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:13 (running for 00:32:26.46)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         1929.63</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\"> 179.874</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1461.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 1453.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.6046741081671\n",
      "  episode_reward_mean: 177.13763946011295\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1125\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.19280239939689636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013195158913731575\n",
      "          model: {}\n",
      "          policy_loss: -0.05087345466017723\n",
      "          total_loss: 45.87799072265625\n",
      "          vf_explained_var: 0.23629264533519745\n",
      "          vf_loss: 45.9175910949707\n",
      "    num_agent_steps_sampled: 1520000\n",
      "    num_agent_steps_trained: 1520000\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082949931781047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3167555969921739\n",
      "    mean_inference_ms: 0.5271498297088583\n",
      "    mean_raw_obs_processing_ms: 0.0704713373868625\n",
      "  time_since_restore: 1937.530102968216\n",
      "  time_this_iter_s: 3.91471004486084\n",
      "  time_total_s: 1937.530102968216\n",
      "  timers:\n",
      "    learn_throughput: 2052.831\n",
      "    learn_time_ms: 1948.528\n",
      "    load_throughput: 35387504.746\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 996.932\n",
      "    sample_time_ms: 4012.311\n",
      "    update_time_ms: 1.311\n",
      "  timestamp: 1671819497\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:18 (running for 00:32:32.35)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         1937.53</td><td style=\"text-align: right;\">1520000</td><td style=\"text-align: right;\"> 177.138</td><td style=\"text-align: right;\">             236.605</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">            1453.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:24 (running for 00:32:37.46)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         1941.57</td><td style=\"text-align: right;\">1524000</td><td style=\"text-align: right;\">  180.76</td><td style=\"text-align: right;\">             241.359</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">            1468.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-25\n",
      "  done: false\n",
      "  episode_len_mean: 1468.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 241.35932424976878\n",
      "  episode_reward_mean: 180.95577189684138\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3196239769458771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01246718317270279\n",
      "          model: {}\n",
      "          policy_loss: -0.040440984070301056\n",
      "          total_loss: 7.4030442237854\n",
      "          vf_explained_var: 0.5614091157913208\n",
      "          vf_loss: 7.432834625244141\n",
      "    num_agent_steps_sampled: 1528000\n",
      "    num_agent_steps_trained: 1528000\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.86666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082475910039844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31671028957627706\n",
      "    mean_inference_ms: 0.5271454551471261\n",
      "    mean_raw_obs_processing_ms: 0.07046337319752456\n",
      "  time_since_restore: 1945.5381848812103\n",
      "  time_this_iter_s: 3.964606523513794\n",
      "  time_total_s: 1945.5381848812103\n",
      "  timers:\n",
      "    learn_throughput: 2051.693\n",
      "    learn_time_ms: 1949.609\n",
      "    load_throughput: 35439831.01\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 997.331\n",
      "    sample_time_ms: 4010.705\n",
      "    update_time_ms: 1.31\n",
      "  timestamp: 1671819505\n",
      "  timesteps_since_restore: 1528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:29 (running for 00:32:43.40)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         1945.54</td><td style=\"text-align: right;\">1528000</td><td style=\"text-align: right;\"> 180.956</td><td style=\"text-align: right;\">             241.359</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">            1468.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 1478.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 241.80527632712707\n",
      "  episode_reward_mean: 184.7057413327253\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1135\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.20881810784339905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013842918910086155\n",
      "          model: {}\n",
      "          policy_loss: -0.053593024611473083\n",
      "          total_loss: 7.845297813415527\n",
      "          vf_explained_var: 0.5755772590637207\n",
      "          vf_loss: 7.887065410614014\n",
      "    num_agent_steps_sampled: 1536000\n",
      "    num_agent_steps_trained: 1536000\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.633333333333326\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082259370871702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3166914914979221\n",
      "    mean_inference_ms: 0.5271232651483669\n",
      "    mean_raw_obs_processing_ms: 0.07045981420343814\n",
      "  time_since_restore: 1953.482356786728\n",
      "  time_this_iter_s: 3.938568592071533\n",
      "  time_total_s: 1953.482356786728\n",
      "  timers:\n",
      "    learn_throughput: 2055.917\n",
      "    learn_time_ms: 1945.604\n",
      "    load_throughput: 34239216.327\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 999.174\n",
      "    sample_time_ms: 4003.308\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671819513\n",
      "  timesteps_since_restore: 1536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:35 (running for 00:32:48.42)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         1953.48</td><td style=\"text-align: right;\">1536000</td><td style=\"text-align: right;\"> 184.706</td><td style=\"text-align: right;\">             241.805</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1478.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:40 (running for 00:32:53.43)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         1957.51</td><td style=\"text-align: right;\">1540000</td><td style=\"text-align: right;\">  184.74</td><td style=\"text-align: right;\">             241.805</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1478.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 1478.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 243.65963911059444\n",
      "  episode_reward_mean: 185.23084925402006\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2855587601661682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015180272981524467\n",
      "          model: {}\n",
      "          policy_loss: -0.03818613663315773\n",
      "          total_loss: 6.37295389175415\n",
      "          vf_explained_var: 0.6010345816612244\n",
      "          vf_loss: 6.398171901702881\n",
      "    num_agent_steps_sampled: 1544000\n",
      "    num_agent_steps_trained: 1544000\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.733333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09082061256686715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3166737529894522\n",
      "    mean_inference_ms: 0.5271023606815737\n",
      "    mean_raw_obs_processing_ms: 0.07045684407194185\n",
      "  time_since_restore: 1961.4557740688324\n",
      "  time_this_iter_s: 3.941232681274414\n",
      "  time_total_s: 1961.4557740688324\n",
      "  timers:\n",
      "    learn_throughput: 2060.106\n",
      "    learn_time_ms: 1941.648\n",
      "    load_throughput: 32152579.532\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1000.815\n",
      "    sample_time_ms: 3996.744\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1671819521\n",
      "  timesteps_since_restore: 1544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:46 (running for 00:32:59.44)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         1961.46</td><td style=\"text-align: right;\">1544000</td><td style=\"text-align: right;\"> 185.231</td><td style=\"text-align: right;\">              243.66</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1478.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-50\n",
      "  done: false\n",
      "  episode_len_mean: 1474.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 182.99955239884616\n",
      "  episode_reward_min: -129.09351925420202\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1145\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.44495889544487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013119051232933998\n",
      "          model: {}\n",
      "          policy_loss: -0.037942156195640564\n",
      "          total_loss: 85.70955657958984\n",
      "          vf_explained_var: 0.46599793434143066\n",
      "          vf_loss: 85.73628234863281\n",
      "    num_agent_steps_sampled: 1552000\n",
      "    num_agent_steps_trained: 1552000\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.06\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09081870557706934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316656842889085\n",
      "    mean_inference_ms: 0.5270830132713278\n",
      "    mean_raw_obs_processing_ms: 0.07045367975591167\n",
      "  time_since_restore: 1969.5042061805725\n",
      "  time_this_iter_s: 3.9824678897857666\n",
      "  time_total_s: 1969.5042061805725\n",
      "  timers:\n",
      "    learn_throughput: 2053.751\n",
      "    learn_time_ms: 1947.656\n",
      "    load_throughput: 32189593.246\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 997.734\n",
      "    sample_time_ms: 4009.085\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1671819530\n",
      "  timesteps_since_restore: 1552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:51 (running for 00:33:04.48)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">          1969.5</td><td style=\"text-align: right;\">1552000</td><td style=\"text-align: right;\">     183</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1474.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:18:56 (running for 00:33:09.55)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         1973.51</td><td style=\"text-align: right;\">1556000</td><td style=\"text-align: right;\"> 182.847</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.094</td><td style=\"text-align: right;\">           1474.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 1468.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 181.8693608584905\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1151\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.07713022828102112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01518199872225523\n",
      "          model: {}\n",
      "          policy_loss: -0.037269242107868195\n",
      "          total_loss: 36.00919723510742\n",
      "          vf_explained_var: 0.4997795820236206\n",
      "          vf_loss: 36.03350067138672\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.43333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09081476702841262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31661402706237135\n",
      "    mean_inference_ms: 0.5270816958819803\n",
      "    mean_raw_obs_processing_ms: 0.07044633787365774\n",
      "  time_since_restore: 1977.4667026996613\n",
      "  time_this_iter_s: 3.959257125854492\n",
      "  time_total_s: 1977.4667026996613\n",
      "  timers:\n",
      "    learn_throughput: 2054.951\n",
      "    learn_time_ms: 1946.519\n",
      "    load_throughput: 30267393.108\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 997.182\n",
      "    sample_time_ms: 4011.302\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1671819538\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:02 (running for 00:33:15.47)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         1981.44</td><td style=\"text-align: right;\">1564000</td><td style=\"text-align: right;\"> 182.037</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1468.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-19-06\n",
      "  done: false\n",
      "  episode_len_mean: 1475.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 184.1192975361487\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1156\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.21367786824703217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013745560310781002\n",
      "          model: {}\n",
      "          policy_loss: -0.0445413663983345\n",
      "          total_loss: 6.5188727378845215\n",
      "          vf_explained_var: 0.5205644369125366\n",
      "          vf_loss: 6.551671504974365\n",
      "    num_agent_steps_sampled: 1568000\n",
      "    num_agent_steps_trained: 1568000\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.52\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09081519278473932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31662159281570534\n",
      "    mean_inference_ms: 0.5270434920602388\n",
      "    mean_raw_obs_processing_ms: 0.07044791617626499\n",
      "  time_since_restore: 1985.398367881775\n",
      "  time_this_iter_s: 3.9560391902923584\n",
      "  time_total_s: 1985.398367881775\n",
      "  timers:\n",
      "    learn_throughput: 2059.702\n",
      "    learn_time_ms: 1942.029\n",
      "    load_throughput: 27003405.762\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 997.515\n",
      "    sample_time_ms: 4009.967\n",
      "    update_time_ms: 1.413\n",
      "  timestamp: 1671819546\n",
      "  timesteps_since_restore: 1568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:07 (running for 00:33:20.48)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">          1985.4</td><td style=\"text-align: right;\">1568000</td><td style=\"text-align: right;\"> 184.119</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1475.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:13 (running for 00:33:26.46)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         1989.39</td><td style=\"text-align: right;\">1572000</td><td style=\"text-align: right;\"> 183.966</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1475.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 1471.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 183.8286136221985\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1161\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.14540031552314758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011628138832747936\n",
      "          model: {}\n",
      "          policy_loss: -0.03581857308745384\n",
      "          total_loss: 29.86464500427246\n",
      "          vf_explained_var: 0.5438554883003235\n",
      "          vf_loss: 29.890527725219727\n",
      "    num_agent_steps_sampled: 1576000\n",
      "    num_agent_steps_trained: 1576000\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.81666666666666\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.090810899763872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31657848481832357\n",
      "    mean_inference_ms: 0.5270430196178006\n",
      "    mean_raw_obs_processing_ms: 0.07044025106613684\n",
      "  time_since_restore: 1993.3120505809784\n",
      "  time_this_iter_s: 3.9228317737579346\n",
      "  time_total_s: 1993.3120505809784\n",
      "  timers:\n",
      "    learn_throughput: 2057.614\n",
      "    learn_time_ms: 1943.999\n",
      "    load_throughput: 27685174.917\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 999.745\n",
      "    sample_time_ms: 4001.021\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671819553\n",
      "  timesteps_since_restore: 1576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:18 (running for 00:33:31.47)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         1997.35</td><td style=\"text-align: right;\">1580000</td><td style=\"text-align: right;\"> 180.935</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1457.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-19-22\n",
      "  done: false\n",
      "  episode_len_mean: 1451.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 179.16315375710954\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1167\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.21659711003303528\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012777388095855713\n",
      "          model: {}\n",
      "          policy_loss: -0.03922242671251297\n",
      "          total_loss: 33.01029968261719\n",
      "          vf_explained_var: 0.27298569679260254\n",
      "          vf_loss: 33.038604736328125\n",
      "    num_agent_steps_sampled: 1584000\n",
      "    num_agent_steps_trained: 1584000\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080902186564034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31656013833332364\n",
      "    mean_inference_ms: 0.5270228589889816\n",
      "    mean_raw_obs_processing_ms: 0.07043748138082352\n",
      "  time_since_restore: 2001.38388133049\n",
      "  time_this_iter_s: 4.029199123382568\n",
      "  time_total_s: 2001.38388133049\n",
      "  timers:\n",
      "    learn_throughput: 2059.087\n",
      "    learn_time_ms: 1942.609\n",
      "    load_throughput: 29248981.869\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 996.233\n",
      "    sample_time_ms: 4015.125\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671819562\n",
      "  timesteps_since_restore: 1584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:23 (running for 00:33:36.52)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         2001.38</td><td style=\"text-align: right;\">1584000</td><td style=\"text-align: right;\"> 179.163</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1451.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:28 (running for 00:33:41.62)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         2005.42</td><td style=\"text-align: right;\">1588000</td><td style=\"text-align: right;\"> 181.458</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1460.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 1486.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 187.52103896145135\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1173\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2223963588476181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015670547261834145\n",
      "          model: {}\n",
      "          policy_loss: -0.04854484647512436\n",
      "          total_loss: 9.35738468170166\n",
      "          vf_explained_var: 0.44346025586128235\n",
      "          vf_loss: 9.392542839050293\n",
      "    num_agent_steps_sampled: 1592000\n",
      "    num_agent_steps_trained: 1592000\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.41666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09081004113574273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31657013769403053\n",
      "    mean_inference_ms: 0.5269862309132318\n",
      "    mean_raw_obs_processing_ms: 0.07043939395551824\n",
      "  time_since_restore: 2009.356155872345\n",
      "  time_this_iter_s: 3.9381296634674072\n",
      "  time_total_s: 2009.356155872345\n",
      "  timers:\n",
      "    learn_throughput: 2064.357\n",
      "    learn_time_ms: 1937.65\n",
      "    load_throughput: 29233692.281\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 999.135\n",
      "    sample_time_ms: 4003.463\n",
      "    update_time_ms: 1.382\n",
      "  timestamp: 1671819570\n",
      "  timesteps_since_restore: 1592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:34 (running for 00:33:47.54)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         2009.36</td><td style=\"text-align: right;\">1592000</td><td style=\"text-align: right;\"> 187.521</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:39 (running for 00:33:52.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:44 (running for 00:33:57.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:49 (running for 00:34:02.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:54 (running for 00:34:07.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:19:59 (running for 00:34:12.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:04 (running for 00:34:17.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:09 (running for 00:34:22.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:14 (running for 00:34:27.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:19 (running for 00:34:32.67)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:24 (running for 00:34:37.67)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:29 (running for 00:34:42.68)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:34 (running for 00:34:47.68)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:39 (running for 00:34:52.69)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:44 (running for 00:34:57.69)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:49 (running for 00:35:02.69)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:54 (running for 00:35:07.70)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:20:59 (running for 00:35:12.70)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:04 (running for 00:35:17.71)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:09 (running for 00:35:22.71)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:14 (running for 00:35:27.72)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:19 (running for 00:35:32.72)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:24 (running for 00:35:37.72)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:29 (running for 00:35:42.73)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:34 (running for 00:35:47.73)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:39 (running for 00:35:52.74)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:44 (running for 00:35:57.74)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:49 (running for 00:36:02.74)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:54 (running for 00:36:07.75)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:21:59 (running for 00:36:12.75)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:04 (running for 00:36:17.76)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">          2013.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> 187.491</td><td style=\"text-align: right;\">             246.461</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-05\n",
      "  done: false\n",
      "  episode_len_mean: 1486.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.4605200384322\n",
      "  episode_reward_mean: 187.86883314671005\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1177\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1565.06\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 246.79568283682076\n",
      "    episode_reward_mean: 214.8244010438479\n",
      "    episode_reward_min: -126.43505428511588\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 120\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 131\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1461\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1194\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 224.3258952320296\n",
      "      - 205.8231772075522\n",
      "      - 199.80718898237043\n",
      "      - -101.51806423593499\n",
      "      - 230.92806826234397\n",
      "      - 224.7904864769035\n",
      "      - 219.43293507029264\n",
      "      - 223.5798573203489\n",
      "      - 222.61753550596555\n",
      "      - 230.26967551833536\n",
      "      - 215.63344299890008\n",
      "      - 225.94209871435848\n",
      "      - 208.97015150770088\n",
      "      - 206.06882673182272\n",
      "      - 208.3022180346914\n",
      "      - 231.14670981736595\n",
      "      - 222.88640045973494\n",
      "      - 223.33468192836628\n",
      "      - 232.494842941507\n",
      "      - 222.03811886425717\n",
      "      - 223.22062154453707\n",
      "      - 229.9366634936549\n",
      "      - 219.49337779488056\n",
      "      - 217.49008137296542\n",
      "      - 218.44301066727996\n",
      "      - 222.2802610692758\n",
      "      - 228.096090675947\n",
      "      - 225.25870991768548\n",
      "      - 237.7985703503764\n",
      "      - 216.71323957240944\n",
      "      - 228.11912361258626\n",
      "      - 214.7457358352341\n",
      "      - 233.85285595122485\n",
      "      - 230.28422372688513\n",
      "      - 217.92808956750207\n",
      "      - 246.79568283682076\n",
      "      - 213.74607303507952\n",
      "      - 215.82798305351807\n",
      "      - 217.47502376278035\n",
      "      - 235.14873803807316\n",
      "      - 222.5319926233702\n",
      "      - 240.9840653398858\n",
      "      - -126.43505428511588\n",
      "      - 226.90628821018007\n",
      "      - 213.89635446523678\n",
      "      - 206.9279465530709\n",
      "      - 212.91120377239514\n",
      "      - 223.8592540625987\n",
      "      - 226.8186325606969\n",
      "      - 234.09936969117712\n",
      "      - 225.41422237342343\n",
      "      - 209.091199417832\n",
      "      - 206.44886826274333\n",
      "      - 229.64512529416734\n",
      "      - 230.1930892675231\n",
      "      - 235.2576552698287\n",
      "      - 206.53788835256756\n",
      "      - 230.60936662265328\n",
      "      - 221.36989494859586\n",
      "      - 240.1252836582446\n",
      "      - 214.0686857354395\n",
      "      - 212.7174437858694\n",
      "      - 226.44379900618924\n",
      "      - 232.60056130032146\n",
      "      - 221.21645311169047\n",
      "      - 236.2685532030074\n",
      "      - 231.83633905881175\n",
      "      - 244.46866401959755\n",
      "      - 229.02832518092842\n",
      "      - 230.45655326198423\n",
      "      - 226.90351593747155\n",
      "      - 227.68080115851762\n",
      "      - 233.7767109866244\n",
      "      - 231.41433573607952\n",
      "      - 224.40710478092342\n",
      "      - 219.5952555510145\n",
      "      - 221.9808753180299\n",
      "      - 235.94924201409128\n",
      "      - 246.26822524751486\n",
      "      - 212.5356672714852\n",
      "      - 218.50162313580938\n",
      "      - 228.8774623920768\n",
      "      - 227.64109851588913\n",
      "      - 234.94121233811038\n",
      "      - 225.97708448986455\n",
      "      - 240.65569586993158\n",
      "      - 202.6050332666502\n",
      "      - 211.34062841769742\n",
      "      - 223.48467795327014\n",
      "      - 208.1308702728951\n",
      "      - 242.3153891386912\n",
      "      - 246.66429462751776\n",
      "      - 110.59177266293054\n",
      "      - 231.17711361855046\n",
      "      - 213.78395850966052\n",
      "      - 62.299536043260275\n",
      "      - 224.4837785869989\n",
      "      - 229.85827847351436\n",
      "      - 234.35567578976304\n",
      "      - 218.41675893740742\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08599200101551113\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.304797302422659\n",
      "      mean_inference_ms: 0.4954269648129756\n",
      "      mean_raw_obs_processing_ms: 0.05930454659455898\n",
      "    timesteps_this_iter: 156506\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.10102780163288116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013196238316595554\n",
      "          model: {}\n",
      "          policy_loss: -0.049688756465911865\n",
      "          total_loss: 4.789220333099365\n",
      "          vf_explained_var: 0.6507131457328796\n",
      "          vf_loss: 4.827635765075684\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_agent_steps_trained: 1600000\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.182407407407407\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080925010103172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3165603244258806\n",
      "    mean_inference_ms: 0.5269748701756718\n",
      "    mean_raw_obs_processing_ms: 0.07043745255025918\n",
      "  time_since_restore: 2164.365969181061\n",
      "  time_this_iter_s: 150.9683074951172\n",
      "  time_total_s: 2164.365969181061\n",
      "  timers:\n",
      "    learn_throughput: 2059.709\n",
      "    learn_time_ms: 1942.022\n",
      "    load_throughput: 30587449.407\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 995.692\n",
      "    sample_time_ms: 4017.308\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671819725\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:10 (running for 00:36:23.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         2168.34</td><td style=\"text-align: right;\">1604000</td><td style=\"text-align: right;\"> 187.961</td><td style=\"text-align: right;\">             248.649</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1486.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 1476.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 187.47796921682297\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1184\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.07326595485210419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010122076608240604\n",
      "          model: {}\n",
      "          policy_loss: -0.03192129358649254\n",
      "          total_loss: 54.82721710205078\n",
      "          vf_explained_var: 0.5359057188034058\n",
      "          vf_loss: 54.850494384765625\n",
      "    num_agent_steps_sampled: 1608000\n",
      "    num_agent_steps_trained: 1608000\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.63333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09081021211731016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31656880739269316\n",
      "    mean_inference_ms: 0.5269373073635727\n",
      "    mean_raw_obs_processing_ms: 0.0704388992294566\n",
      "  time_since_restore: 2172.293699979782\n",
      "  time_this_iter_s: 3.951657295227051\n",
      "  time_total_s: 2172.293699979782\n",
      "  timers:\n",
      "    learn_throughput: 2060.382\n",
      "    learn_time_ms: 1941.388\n",
      "    load_throughput: 33825032.258\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 213.776\n",
      "    sample_time_ms: 18711.206\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1671819733\n",
      "  timesteps_since_restore: 1608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 402\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:16 (running for 00:36:29.57)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         2172.29</td><td style=\"text-align: right;\">1608000</td><td style=\"text-align: right;\"> 187.478</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">            1476.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 1476.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 188.01469621328047\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1188\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.0980842337012291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015685344114899635\n",
      "          model: {}\n",
      "          policy_loss: -0.04452947899699211\n",
      "          total_loss: 3.9576497077941895\n",
      "          vf_explained_var: 0.6632851958274841\n",
      "          vf_loss: 3.988779306411743\n",
      "    num_agent_steps_sampled: 1616000\n",
      "    num_agent_steps_trained: 1616000\n",
      "    num_steps_sampled: 1616000\n",
      "    num_steps_trained: 1616000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.839999999999996\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080675531533336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31653259520495847\n",
      "    mean_inference_ms: 0.5269448707715615\n",
      "    mean_raw_obs_processing_ms: 0.07043252745576939\n",
      "  time_since_restore: 2180.2041754722595\n",
      "  time_this_iter_s: 3.947495460510254\n",
      "  time_total_s: 2180.2041754722595\n",
      "  timers:\n",
      "    learn_throughput: 2060.34\n",
      "    learn_time_ms: 1941.428\n",
      "    load_throughput: 33136906.972\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 213.754\n",
      "    sample_time_ms: 18713.08\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1671819741\n",
      "  timesteps_since_restore: 1616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1616000\n",
      "  training_iteration: 404\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:22 (running for 00:36:35.57)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">          2180.2</td><td style=\"text-align: right;\">1616000</td><td style=\"text-align: right;\"> 188.015</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">            1476.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:28 (running for 00:36:41.57)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         2184.23</td><td style=\"text-align: right;\">1620000</td><td style=\"text-align: right;\">  185.26</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1462.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 1462.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 186.5145817566107\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1195\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296863079071\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.20452024042606354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02016385644674301\n",
      "          model: {}\n",
      "          policy_loss: -0.04101961851119995\n",
      "          total_loss: 11.639078140258789\n",
      "          vf_explained_var: 0.5796318650245667\n",
      "          vf_loss: 11.662871360778809\n",
      "    num_agent_steps_sampled: 1624000\n",
      "    num_agent_steps_trained: 1624000\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1624000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.1\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080553014171194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3165167996762156\n",
      "    mean_inference_ms: 0.5269274404808435\n",
      "    mean_raw_obs_processing_ms: 0.07042973817525605\n",
      "  time_since_restore: 2188.2474658489227\n",
      "  time_this_iter_s: 4.0214502811431885\n",
      "  time_total_s: 2188.2474658489227\n",
      "  timers:\n",
      "    learn_throughput: 2049.338\n",
      "    learn_time_ms: 1951.85\n",
      "    load_throughput: 33065068.979\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 213.901\n",
      "    sample_time_ms: 18700.271\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671819749\n",
      "  timesteps_since_restore: 1624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 406\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:33 (running for 00:36:46.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         2192.27</td><td style=\"text-align: right;\">1628000</td><td style=\"text-align: right;\"> 190.311</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">            1476.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-37\n",
      "  done: false\n",
      "  episode_len_mean: 1476.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 190.7548908747735\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.14957721531391144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011126415804028511\n",
      "          model: {}\n",
      "          policy_loss: -0.03844499588012695\n",
      "          total_loss: 20.461130142211914\n",
      "          vf_explained_var: 0.5023085474967957\n",
      "          vf_loss: 20.48531723022461\n",
      "    num_agent_steps_sampled: 1632000\n",
      "    num_agent_steps_trained: 1632000\n",
      "    num_steps_sampled: 1632000\n",
      "    num_steps_trained: 1632000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.52\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080234153396953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164807310285512\n",
      "    mean_inference_ms: 0.5269344926617858\n",
      "    mean_raw_obs_processing_ms: 0.07042352432448774\n",
      "  time_since_restore: 2196.264320373535\n",
      "  time_this_iter_s: 3.9984095096588135\n",
      "  time_total_s: 2196.264320373535\n",
      "  timers:\n",
      "    learn_throughput: 2051.059\n",
      "    learn_time_ms: 1950.212\n",
      "    load_throughput: 33045530.825\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 213.735\n",
      "    sample_time_ms: 18714.736\n",
      "    update_time_ms: 1.335\n",
      "  timestamp: 1671819757\n",
      "  timesteps_since_restore: 1632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1632000\n",
      "  training_iteration: 408\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:38 (running for 00:36:51.67)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         2196.26</td><td style=\"text-align: right;\">1632000</td><td style=\"text-align: right;\"> 190.755</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1476.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:43 (running for 00:36:56.73)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         2200.26</td><td style=\"text-align: right;\">1636000</td><td style=\"text-align: right;\"> 192.286</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1479.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-45\n",
      "  done: false\n",
      "  episode_len_mean: 1494.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 195.88965006460455\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1205\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.0802040621638298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011895399540662766\n",
      "          model: {}\n",
      "          policy_loss: -0.04309162124991417\n",
      "          total_loss: 8.383708000183105\n",
      "          vf_explained_var: 0.5708418488502502\n",
      "          vf_loss: 8.411555290222168\n",
      "    num_agent_steps_sampled: 1640000\n",
      "    num_agent_steps_trained: 1640000\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.11666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080380557086637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31649598409019875\n",
      "    mean_inference_ms: 0.5269040516391136\n",
      "    mean_raw_obs_processing_ms: 0.07042584720866053\n",
      "  time_since_restore: 2204.21284031868\n",
      "  time_this_iter_s: 3.9536142349243164\n",
      "  time_total_s: 2204.21284031868\n",
      "  timers:\n",
      "    learn_throughput: 2057.42\n",
      "    learn_time_ms: 1944.183\n",
      "    load_throughput: 33588020.02\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 213.893\n",
      "    sample_time_ms: 18700.923\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1671819765\n",
      "  timesteps_since_restore: 1640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 410\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:49 (running for 00:37:02.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         2208.19</td><td style=\"text-align: right;\">1644000</td><td style=\"text-align: right;\"> 199.417</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1508.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 1508.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 199.8306971576303\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.11960366368293762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012174123898148537\n",
      "          model: {}\n",
      "          policy_loss: -0.04042758792638779\n",
      "          total_loss: 5.2645697593688965\n",
      "          vf_explained_var: 0.5749510526657104\n",
      "          vf_loss: 5.2893967628479\n",
      "    num_agent_steps_sampled: 1648000\n",
      "    num_agent_steps_trained: 1648000\n",
      "    num_steps_sampled: 1648000\n",
      "    num_steps_trained: 1648000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.65\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080274222621149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164832705699103\n",
      "    mean_inference_ms: 0.526890439176221\n",
      "    mean_raw_obs_processing_ms: 0.07042321199417255\n",
      "  time_since_restore: 2212.1529610157013\n",
      "  time_this_iter_s: 3.963070869445801\n",
      "  time_total_s: 2212.1529610157013\n",
      "  timers:\n",
      "    learn_throughput: 2058.555\n",
      "    learn_time_ms: 1943.11\n",
      "    load_throughput: 32419741.063\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 998.393\n",
      "    sample_time_ms: 4006.439\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671819773\n",
      "  timesteps_since_restore: 1648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1648000\n",
      "  training_iteration: 412\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:22:55 (running for 00:37:08.63)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         2212.15</td><td style=\"text-align: right;\">1648000</td><td style=\"text-align: right;\"> 199.831</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1508.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:00 (running for 00:37:13.69)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         2216.19</td><td style=\"text-align: right;\">1652000</td><td style=\"text-align: right;\"> 200.137</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1508.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 1508.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 200.24128017397396\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1215\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.028950724750757217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01009009126573801\n",
      "          model: {}\n",
      "          policy_loss: -0.039726920425891876\n",
      "          total_loss: 6.595548629760742\n",
      "          vf_explained_var: 0.696150541305542\n",
      "          vf_loss: 6.622345447540283\n",
      "    num_agent_steps_sampled: 1656000\n",
      "    num_agent_steps_trained: 1656000\n",
      "    num_steps_sampled: 1656000\n",
      "    num_steps_trained: 1656000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.75\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080378401297332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164950500974229\n",
      "    mean_inference_ms: 0.5268571493827681\n",
      "    mean_raw_obs_processing_ms: 0.07042461643951793\n",
      "  time_since_restore: 2220.1327896118164\n",
      "  time_this_iter_s: 3.942429542541504\n",
      "  time_total_s: 2220.1327896118164\n",
      "  timers:\n",
      "    learn_throughput: 2055.494\n",
      "    learn_time_ms: 1946.004\n",
      "    load_throughput: 29315421.981\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 996.845\n",
      "    sample_time_ms: 4012.659\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1671819781\n",
      "  timesteps_since_restore: 1656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1656000\n",
      "  training_iteration: 414\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:06 (running for 00:37:19.62)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         2224.08</td><td style=\"text-align: right;\">1660000</td><td style=\"text-align: right;\"> 200.617</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1508.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-09\n",
      "  done: false\n",
      "  episode_len_mean: 1510.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 202.60732250542958\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.28978589177131653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010444353334605694\n",
      "          model: {}\n",
      "          policy_loss: -0.040745679289102554\n",
      "          total_loss: 5.954684257507324\n",
      "          vf_explained_var: 0.6628394722938538\n",
      "          vf_loss: 5.982046604156494\n",
      "    num_agent_steps_sampled: 1664000\n",
      "    num_agent_steps_trained: 1664000\n",
      "    num_steps_sampled: 1664000\n",
      "    num_steps_trained: 1664000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.78333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080005070498368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31645524128708113\n",
      "    mean_inference_ms: 0.526860980991355\n",
      "    mean_raw_obs_processing_ms: 0.070417618062339\n",
      "  time_since_restore: 2228.033420085907\n",
      "  time_this_iter_s: 3.9582300186157227\n",
      "  time_total_s: 2228.033420085907\n",
      "  timers:\n",
      "    learn_throughput: 2065.992\n",
      "    learn_time_ms: 1936.116\n",
      "    load_throughput: 28197001.681\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 998.692\n",
      "    sample_time_ms: 4005.238\n",
      "    update_time_ms: 1.338\n",
      "  timestamp: 1671819789\n",
      "  timesteps_since_restore: 1664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1664000\n",
      "  training_iteration: 416\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:12 (running for 00:37:25.60)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         2228.03</td><td style=\"text-align: right;\">1664000</td><td style=\"text-align: right;\"> 202.607</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1510.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 1518.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 206.26320243278565\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.059132277965545654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01240724977105856\n",
      "          model: {}\n",
      "          policy_loss: -0.042437899857759476\n",
      "          total_loss: 6.389512062072754\n",
      "          vf_explained_var: 0.6460469961166382\n",
      "          vf_loss: 6.416050910949707\n",
      "    num_agent_steps_sampled: 1672000\n",
      "    num_agent_steps_trained: 1672000\n",
      "    num_steps_sampled: 1672000\n",
      "    num_steps_trained: 1672000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.14\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079881717428455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164422339275318\n",
      "    mean_inference_ms: 0.5268471846653675\n",
      "    mean_raw_obs_processing_ms: 0.07041501588190328\n",
      "  time_since_restore: 2235.988609790802\n",
      "  time_this_iter_s: 3.9628989696502686\n",
      "  time_total_s: 2235.988609790802\n",
      "  timers:\n",
      "    learn_throughput: 2067.782\n",
      "    learn_time_ms: 1934.44\n",
      "    load_throughput: 28197001.681\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 1001.475\n",
      "    sample_time_ms: 3994.109\n",
      "    update_time_ms: 1.306\n",
      "  timestamp: 1671819797\n",
      "  timesteps_since_restore: 1672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1672000\n",
      "  training_iteration: 418\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:18 (running for 00:37:31.59)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         2235.99</td><td style=\"text-align: right;\">1672000</td><td style=\"text-align: right;\"> 206.263</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:23 (running for 00:37:36.61)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         2239.99</td><td style=\"text-align: right;\">1676000</td><td style=\"text-align: right;\"> 206.523</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 1518.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.3799952819141\n",
      "  episode_reward_mean: 207.15185290031528\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.0344785675406456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00954403169453144\n",
      "          model: {}\n",
      "          policy_loss: -0.0352943055331707\n",
      "          total_loss: 6.113179683685303\n",
      "          vf_explained_var: 0.6331742405891418\n",
      "          vf_loss: 6.136244297027588\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_agent_steps_trained: 1680000\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.916666666666664\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079786181655485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31643025237819244\n",
      "    mean_inference_ms: 0.5268337579439598\n",
      "    mean_raw_obs_processing_ms: 0.07041293289428956\n",
      "  time_since_restore: 2243.952203989029\n",
      "  time_this_iter_s: 3.9660396575927734\n",
      "  time_total_s: 2243.952203989029\n",
      "  timers:\n",
      "    learn_throughput: 2066.219\n",
      "    learn_time_ms: 1935.903\n",
      "    load_throughput: 28069626.903\n",
      "    load_time_ms: 0.143\n",
      "    sample_throughput: 1001.498\n",
      "    sample_time_ms: 3994.018\n",
      "    update_time_ms: 1.295\n",
      "  timestamp: 1671819805\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 420\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:28 (running for 00:37:41.63)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         2243.95</td><td style=\"text-align: right;\">1680000</td><td style=\"text-align: right;\"> 207.152</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:33 (running for 00:37:46.63)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         2247.98</td><td style=\"text-align: right;\">1684000</td><td style=\"text-align: right;\"> 207.372</td><td style=\"text-align: right;\">              252.38</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 1518.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 207.99964363624284\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1235\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17865148186683655\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011906287632882595\n",
      "          model: {}\n",
      "          policy_loss: -0.0440652035176754\n",
      "          total_loss: 6.0577545166015625\n",
      "          vf_explained_var: 0.6708256602287292\n",
      "          vf_loss: 6.0865631103515625\n",
      "    num_agent_steps_sampled: 1688000\n",
      "    num_agent_steps_trained: 1688000\n",
      "    num_steps_sampled: 1688000\n",
      "    num_steps_trained: 1688000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.8\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907968701480268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164187784795773\n",
      "    mean_inference_ms: 0.5268211936422845\n",
      "    mean_raw_obs_processing_ms: 0.07041082859178807\n",
      "  time_since_restore: 2251.976188659668\n",
      "  time_this_iter_s: 3.998776912689209\n",
      "  time_total_s: 2251.976188659668\n",
      "  timers:\n",
      "    learn_throughput: 2061.083\n",
      "    learn_time_ms: 1940.727\n",
      "    load_throughput: 29599887.085\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 999.62\n",
      "    sample_time_ms: 4001.521\n",
      "    update_time_ms: 1.284\n",
      "  timestamp: 1671819813\n",
      "  timesteps_since_restore: 1688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1688000\n",
      "  training_iteration: 422\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:39 (running for 00:37:52.61)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         2255.92</td><td style=\"text-align: right;\">1692000</td><td style=\"text-align: right;\"> 208.521</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-41\n",
      "  done: false\n",
      "  episode_len_mean: 1518.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 208.7789163302891\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.05276143178343773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011117707937955856\n",
      "          model: {}\n",
      "          policy_loss: -0.03955533728003502\n",
      "          total_loss: 6.268741607666016\n",
      "          vf_explained_var: 0.59703528881073\n",
      "          vf_loss: 6.294050216674805\n",
      "    num_agent_steps_sampled: 1696000\n",
      "    num_agent_steps_trained: 1696000\n",
      "    num_steps_sampled: 1696000\n",
      "    num_steps_trained: 1696000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079578015595396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316406712527331\n",
      "    mean_inference_ms: 0.5268074921475071\n",
      "    mean_raw_obs_processing_ms: 0.07040878975961377\n",
      "  time_since_restore: 2259.8646368980408\n",
      "  time_this_iter_s: 3.949157238006592\n",
      "  time_total_s: 2259.8646368980408\n",
      "  timers:\n",
      "    learn_throughput: 2063.011\n",
      "    learn_time_ms: 1938.913\n",
      "    load_throughput: 33514214.942\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 1002.567\n",
      "    sample_time_ms: 3989.757\n",
      "    update_time_ms: 1.278\n",
      "  timestamp: 1671819821\n",
      "  timesteps_since_restore: 1696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1696000\n",
      "  training_iteration: 424\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:45 (running for 00:37:58.59)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         2263.85</td><td style=\"text-align: right;\">1700000</td><td style=\"text-align: right;\">  209.32</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1518.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-49\n",
      "  done: false\n",
      "  episode_len_mean: 1522.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 212.41541770349107\n",
      "  episode_reward_min: -129.47268585818324\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1245\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.03112599439918995\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011481568217277527\n",
      "          model: {}\n",
      "          policy_loss: -0.03793200105428696\n",
      "          total_loss: 7.759347915649414\n",
      "          vf_explained_var: 0.5622520446777344\n",
      "          vf_loss: 7.782566547393799\n",
      "    num_agent_steps_sampled: 1704000\n",
      "    num_agent_steps_trained: 1704000\n",
      "    num_steps_sampled: 1704000\n",
      "    num_steps_trained: 1704000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.983333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079449804774012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316393624245385\n",
      "    mean_inference_ms: 0.5267932770767518\n",
      "    mean_raw_obs_processing_ms: 0.07040622712492822\n",
      "  time_since_restore: 2267.8215165138245\n",
      "  time_this_iter_s: 3.9730875492095947\n",
      "  time_total_s: 2267.8215165138245\n",
      "  timers:\n",
      "    learn_throughput: 2063.479\n",
      "    learn_time_ms: 1938.474\n",
      "    load_throughput: 32469936.133\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 999.963\n",
      "    sample_time_ms: 4000.147\n",
      "    update_time_ms: 1.299\n",
      "  timestamp: 1671819829\n",
      "  timesteps_since_restore: 1704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1704000\n",
      "  training_iteration: 426\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:51 (running for 00:38:04.58)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         2267.82</td><td style=\"text-align: right;\">1704000</td><td style=\"text-align: right;\"> 212.415</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1522.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:23:56 (running for 00:38:09.60)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         2271.82</td><td style=\"text-align: right;\">1708000</td><td style=\"text-align: right;\"> 212.839</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -129.473</td><td style=\"text-align: right;\">           1522.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 1536.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 216.45738786903578\n",
      "  episode_reward_min: -126.18713623030544\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1250\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.03660297766327858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011831236071884632\n",
      "          model: {}\n",
      "          policy_loss: -0.043476756662130356\n",
      "          total_loss: 6.2044267654418945\n",
      "          vf_explained_var: 0.563670814037323\n",
      "          vf_loss: 6.2327423095703125\n",
      "    num_agent_steps_sampled: 1712000\n",
      "    num_agent_steps_trained: 1712000\n",
      "    num_steps_sampled: 1712000\n",
      "    num_steps_trained: 1712000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.82000000000001\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079520804094127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31640445822917834\n",
      "    mean_inference_ms: 0.5267582717245034\n",
      "    mean_raw_obs_processing_ms: 0.0704079084678925\n",
      "  time_since_restore: 2275.7511823177338\n",
      "  time_this_iter_s: 3.9348068237304688\n",
      "  time_total_s: 2275.7511823177338\n",
      "  timers:\n",
      "    learn_throughput: 2056.1\n",
      "    learn_time_ms: 1945.43\n",
      "    load_throughput: 32363456.79\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1001.782\n",
      "    sample_time_ms: 3992.883\n",
      "    update_time_ms: 1.317\n",
      "  timestamp: 1671819837\n",
      "  timesteps_since_restore: 1712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1712000\n",
      "  training_iteration: 428\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:02 (running for 00:38:15.59)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         2279.77</td><td style=\"text-align: right;\">1716000</td><td style=\"text-align: right;\"> 216.353</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -126.187</td><td style=\"text-align: right;\">           1536.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 1536.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 216.8492072003082\n",
      "  episode_reward_min: -126.18713623030544\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1255\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.15039241313934326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01143789105117321\n",
      "          model: {}\n",
      "          policy_loss: -0.03951180726289749\n",
      "          total_loss: 9.080097198486328\n",
      "          vf_explained_var: 0.606201171875\n",
      "          vf_loss: 9.104951858520508\n",
      "    num_agent_steps_sampled: 1720000\n",
      "    num_agent_steps_trained: 1720000\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079374786971439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31639179674824525\n",
      "    mean_inference_ms: 0.5267434376455477\n",
      "    mean_raw_obs_processing_ms: 0.07040515425804579\n",
      "  time_since_restore: 2283.7401995658875\n",
      "  time_this_iter_s: 3.9655725955963135\n",
      "  time_total_s: 2283.7401995658875\n",
      "  timers:\n",
      "    learn_throughput: 2058.805\n",
      "    learn_time_ms: 1942.875\n",
      "    load_throughput: 30570728.863\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 999.899\n",
      "    sample_time_ms: 4000.402\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1671819845\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 430\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:08 (running for 00:38:21.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         2283.74</td><td style=\"text-align: right;\">1720000</td><td style=\"text-align: right;\"> 216.849</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -126.187</td><td style=\"text-align: right;\">           1536.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-13\n",
      "  done: false\n",
      "  episode_len_mean: 1526.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 215.97848940126457\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1261\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.20597736537456512\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011850543320178986\n",
      "          model: {}\n",
      "          policy_loss: -0.038292620331048965\n",
      "          total_loss: 5.87966775894165\n",
      "          vf_explained_var: 0.6832624673843384\n",
      "          vf_loss: 5.902774333953857\n",
      "    num_agent_steps_sampled: 1728000\n",
      "    num_agent_steps_trained: 1728000\n",
      "    num_steps_sampled: 1728000\n",
      "    num_steps_trained: 1728000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.15\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907924829803852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163793862758548\n",
      "    mean_inference_ms: 0.5267291870011324\n",
      "    mean_raw_obs_processing_ms: 0.07040265004117108\n",
      "  time_since_restore: 2291.748503923416\n",
      "  time_this_iter_s: 3.977126359939575\n",
      "  time_total_s: 2291.748503923416\n",
      "  timers:\n",
      "    learn_throughput: 2063.515\n",
      "    learn_time_ms: 1938.44\n",
      "    load_throughput: 30207446.885\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1000.152\n",
      "    sample_time_ms: 3999.392\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1671819853\n",
      "  timesteps_since_restore: 1728000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1728000\n",
      "  training_iteration: 432\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:14 (running for 00:38:27.62)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         2291.75</td><td style=\"text-align: right;\">1728000</td><td style=\"text-align: right;\"> 215.978</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">            1526.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:19 (running for 00:38:32.71)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         2295.77</td><td style=\"text-align: right;\">1732000</td><td style=\"text-align: right;\"> 215.873</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1524.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-21\n",
      "  done: false\n",
      "  episode_len_mean: 1524.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 216.41628373039165\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1266\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.01986844278872013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01097392663359642\n",
      "          model: {}\n",
      "          policy_loss: -0.03755288943648338\n",
      "          total_loss: 6.531644821166992\n",
      "          vf_explained_var: 0.6560070514678955\n",
      "          vf_loss: 6.5551347732543945\n",
      "    num_agent_steps_sampled: 1736000\n",
      "    num_agent_steps_trained: 1736000\n",
      "    num_steps_sampled: 1736000\n",
      "    num_steps_trained: 1736000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.71666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09079124891899813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163682871480508\n",
      "    mean_inference_ms: 0.5267172274484316\n",
      "    mean_raw_obs_processing_ms: 0.07040044342659293\n",
      "  time_since_restore: 2299.701755285263\n",
      "  time_this_iter_s: 3.9306156635284424\n",
      "  time_total_s: 2299.701755285263\n",
      "  timers:\n",
      "    learn_throughput: 2065.588\n",
      "    learn_time_ms: 1936.495\n",
      "    load_throughput: 28008707.846\n",
      "    load_time_ms: 0.143\n",
      "    sample_throughput: 998.428\n",
      "    sample_time_ms: 4006.297\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1671819861\n",
      "  timesteps_since_restore: 1736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1736000\n",
      "  training_iteration: 434\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:25 (running for 00:38:38.62)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">          2299.7</td><td style=\"text-align: right;\">1736000</td><td style=\"text-align: right;\"> 216.416</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1524.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 1530.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 218.97614323778672\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1272\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.1111793965101242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010998949408531189\n",
      "          model: {}\n",
      "          policy_loss: -0.04152823984622955\n",
      "          total_loss: 7.172755718231201\n",
      "          vf_explained_var: 0.6420918107032776\n",
      "          vf_loss: 7.200189113616943\n",
      "    num_agent_steps_sampled: 1744000\n",
      "    num_agent_steps_trained: 1744000\n",
      "    num_steps_sampled: 1744000\n",
      "    num_steps_trained: 1744000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.21666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078938630630531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31635320305383835\n",
      "    mean_inference_ms: 0.5267021719362235\n",
      "    mean_raw_obs_processing_ms: 0.07039763838094\n",
      "  time_since_restore: 2307.7002952098846\n",
      "  time_this_iter_s: 3.9985129833221436\n",
      "  time_total_s: 2307.7002952098846\n",
      "  timers:\n",
      "    learn_throughput: 2067.966\n",
      "    learn_time_ms: 1934.268\n",
      "    load_throughput: 30029024.521\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 997.756\n",
      "    sample_time_ms: 4008.994\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1671819869\n",
      "  timesteps_since_restore: 1744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1744000\n",
      "  training_iteration: 436\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:30 (running for 00:38:43.68)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">          2307.7</td><td style=\"text-align: right;\">1744000</td><td style=\"text-align: right;\"> 218.976</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1530.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:35 (running for 00:38:48.70)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">          2311.7</td><td style=\"text-align: right;\">1748000</td><td style=\"text-align: right;\"> 219.702</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1530.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-37\n",
      "  done: false\n",
      "  episode_len_mean: 1530.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 220.030554625219\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1276\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.10318189114332199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009947296231985092\n",
      "          model: {}\n",
      "          policy_loss: -0.03499758243560791\n",
      "          total_loss: 5.665785789489746\n",
      "          vf_explained_var: 0.6370906233787537\n",
      "          vf_loss: 5.6880364418029785\n",
      "    num_agent_steps_sampled: 1752000\n",
      "    num_agent_steps_trained: 1752000\n",
      "    num_steps_sampled: 1752000\n",
      "    num_steps_trained: 1752000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.08\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078826205737801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163436159057425\n",
      "    mean_inference_ms: 0.5266927973255465\n",
      "    mean_raw_obs_processing_ms: 0.0703957361426007\n",
      "  time_since_restore: 2315.6033663749695\n",
      "  time_this_iter_s: 3.9053330421447754\n",
      "  time_total_s: 2315.6033663749695\n",
      "  timers:\n",
      "    learn_throughput: 2072.735\n",
      "    learn_time_ms: 1929.817\n",
      "    load_throughput: 30142321.236\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 998.535\n",
      "    sample_time_ms: 4005.868\n",
      "    update_time_ms: 1.391\n",
      "  timestamp: 1671819877\n",
      "  timesteps_since_restore: 1752000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1752000\n",
      "  training_iteration: 438\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:41 (running for 00:38:54.61)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         2319.57</td><td style=\"text-align: right;\">1756000</td><td style=\"text-align: right;\"> 216.971</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1515.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-45\n",
      "  done: false\n",
      "  episode_len_mean: 1515.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 217.18480128253256\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1283\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.19112315773963928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01172573957592249\n",
      "          model: {}\n",
      "          policy_loss: -0.03923973813652992\n",
      "          total_loss: 7.427542686462402\n",
      "          vf_explained_var: 0.6212834119796753\n",
      "          vf_loss: 7.451756954193115\n",
      "    num_agent_steps_sampled: 1760000\n",
      "    num_agent_steps_trained: 1760000\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.13333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078625388440206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31632613210389954\n",
      "    mean_inference_ms: 0.5266765897139907\n",
      "    mean_raw_obs_processing_ms: 0.07039229630600553\n",
      "  time_since_restore: 2323.596273422241\n",
      "  time_this_iter_s: 4.02586817741394\n",
      "  time_total_s: 2323.596273422241\n",
      "  timers:\n",
      "    learn_throughput: 2073.684\n",
      "    learn_time_ms: 1928.934\n",
      "    load_throughput: 30256476.105\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 998.005\n",
      "    sample_time_ms: 4007.994\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1671819885\n",
      "  timesteps_since_restore: 1760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 440\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:46 (running for 00:38:59.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">          2323.6</td><td style=\"text-align: right;\">1760000</td><td style=\"text-align: right;\"> 217.185</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1515.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:51 (running for 00:39:04.74)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         2327.62</td><td style=\"text-align: right;\">1764000</td><td style=\"text-align: right;\"> 221.142</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1530.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-24-53\n",
      "  done: false\n",
      "  episode_len_mean: 1530.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 221.41587005889883\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1287\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.18191538751125336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01020036917179823\n",
      "          model: {}\n",
      "          policy_loss: -0.036860231310129166\n",
      "          total_loss: 4.502618789672852\n",
      "          vf_explained_var: 0.6532856822013855\n",
      "          vf_loss: 4.526407241821289\n",
      "    num_agent_steps_sampled: 1768000\n",
      "    num_agent_steps_trained: 1768000\n",
      "    num_steps_sampled: 1768000\n",
      "    num_steps_trained: 1768000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.160000000000004\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078547738977041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163184589570321\n",
      "    mean_inference_ms: 0.5266703189477168\n",
      "    mean_raw_obs_processing_ms: 0.07039059935730797\n",
      "  time_since_restore: 2331.604027032852\n",
      "  time_this_iter_s: 3.984807014465332\n",
      "  time_total_s: 2331.604027032852\n",
      "  timers:\n",
      "    learn_throughput: 2071.476\n",
      "    learn_time_ms: 1930.99\n",
      "    load_throughput: 30671327.239\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 998.837\n",
      "    sample_time_ms: 4004.655\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1671819893\n",
      "  timesteps_since_restore: 1768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1768000\n",
      "  training_iteration: 442\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:24:57 (running for 00:39:10.72)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">          2335.6</td><td style=\"text-align: right;\">1772000</td><td style=\"text-align: right;\"> 225.526</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1545.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 1535.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 223.1433588132077\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1294\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17751656472682953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012392185628414154\n",
      "          model: {}\n",
      "          policy_loss: -0.036107707768678665\n",
      "          total_loss: 96.8570785522461\n",
      "          vf_explained_var: 0.450730562210083\n",
      "          vf_loss: 96.87730407714844\n",
      "    num_agent_steps_sampled: 1776000\n",
      "    num_agent_steps_trained: 1776000\n",
      "    num_steps_sampled: 1776000\n",
      "    num_steps_trained: 1776000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.46666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078647817749594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163308254634678\n",
      "    mean_inference_ms: 0.5266406503595507\n",
      "    mean_raw_obs_processing_ms: 0.07039217573747326\n",
      "  time_since_restore: 2339.569738149643\n",
      "  time_this_iter_s: 3.9724669456481934\n",
      "  time_total_s: 2339.569738149643\n",
      "  timers:\n",
      "    learn_throughput: 2072.545\n",
      "    learn_time_ms: 1929.995\n",
      "    load_throughput: 33301341.802\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 997.391\n",
      "    sample_time_ms: 4010.465\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1671819901\n",
      "  timesteps_since_restore: 1776000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1776000\n",
      "  training_iteration: 444\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:02 (running for 00:39:15.77)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         2339.57</td><td style=\"text-align: right;\">1776000</td><td style=\"text-align: right;\"> 223.143</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1535.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:08 (running for 00:39:21.74)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         2343.58</td><td style=\"text-align: right;\">1780000</td><td style=\"text-align: right;\"> 223.246</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1535.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 1535.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 223.1697298492518\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1298\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.18857477605342865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012192116118967533\n",
      "          model: {}\n",
      "          policy_loss: -0.042940475046634674\n",
      "          total_loss: 7.724398136138916\n",
      "          vf_explained_var: 0.5811437368392944\n",
      "          vf_loss: 7.751715660095215\n",
      "    num_agent_steps_sampled: 1784000\n",
      "    num_agent_steps_trained: 1784000\n",
      "    num_steps_sampled: 1784000\n",
      "    num_steps_trained: 1784000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.56\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078580189300423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163236258983061\n",
      "    mean_inference_ms: 0.5266350798445185\n",
      "    mean_raw_obs_processing_ms: 0.07039051538078019\n",
      "  time_since_restore: 2347.5169360637665\n",
      "  time_this_iter_s: 3.940321445465088\n",
      "  time_total_s: 2347.5169360637665\n",
      "  timers:\n",
      "    learn_throughput: 2069.41\n",
      "    learn_time_ms: 1932.918\n",
      "    load_throughput: 33327802.94\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 999.567\n",
      "    sample_time_ms: 4001.732\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1671819909\n",
      "  timesteps_since_restore: 1784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1784000\n",
      "  training_iteration: 446\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:14 (running for 00:39:27.70)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">          2351.5</td><td style=\"text-align: right;\">1788000</td><td style=\"text-align: right;\"> 225.697</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1544.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-17\n",
      "  done: false\n",
      "  episode_len_mean: 1544.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 226.33528391028685\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1304\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.2766588032245636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011343730613589287\n",
      "          model: {}\n",
      "          policy_loss: -0.03990667685866356\n",
      "          total_loss: 8.97516918182373\n",
      "          vf_explained_var: 0.5613688230514526\n",
      "          vf_loss: 9.000539779663086\n",
      "    num_agent_steps_sampled: 1792000\n",
      "    num_agent_steps_trained: 1792000\n",
      "    num_steps_sampled: 1792000\n",
      "    num_steps_trained: 1792000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.5\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078456474161317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163114185243808\n",
      "    mean_inference_ms: 0.5266252418473113\n",
      "    mean_raw_obs_processing_ms: 0.07038778538501787\n",
      "  time_since_restore: 2355.465303182602\n",
      "  time_this_iter_s: 3.960515260696411\n",
      "  time_total_s: 2355.465303182602\n",
      "  timers:\n",
      "    learn_throughput: 2069.069\n",
      "    learn_time_ms: 1933.236\n",
      "    load_throughput: 33084630.25\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 997.817\n",
      "    sample_time_ms: 4008.753\n",
      "    update_time_ms: 1.382\n",
      "  timestamp: 1671819917\n",
      "  timesteps_since_restore: 1792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1792000\n",
      "  training_iteration: 448\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:20 (running for 00:39:33.68)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         2355.47</td><td style=\"text-align: right;\">1792000</td><td style=\"text-align: right;\"> 226.335</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1544.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 1544.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 227.47071259637698\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1308\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.1969795823097229\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0108464565128088\n",
      "          model: {}\n",
      "          policy_loss: -0.04248655587434769\n",
      "          total_loss: 5.523355484008789\n",
      "          vf_explained_var: 0.6491113901138306\n",
      "          vf_loss: 5.551943778991699\n",
      "    num_agent_steps_sampled: 1800000\n",
      "    num_agent_steps_trained: 1800000\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.849999999999994\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09078357838175924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31630279976091236\n",
      "    mean_inference_ms: 0.5266180991306707\n",
      "    mean_raw_obs_processing_ms: 0.07038583852181268\n",
      "  time_since_restore: 2363.3713648319244\n",
      "  time_this_iter_s: 3.9335649013519287\n",
      "  time_total_s: 2363.3713648319244\n",
      "  timers:\n",
      "    learn_throughput: 2065.209\n",
      "    learn_time_ms: 1936.85\n",
      "    load_throughput: 33104214.68\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1001.111\n",
      "    sample_time_ms: 3995.561\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671819925\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 450\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:26 (running for 00:39:39.62)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         2363.37</td><td style=\"text-align: right;\">1800000</td><td style=\"text-align: right;\"> 227.471</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1544.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:31 (running for 00:39:44.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         2367.38</td><td style=\"text-align: right;\">1804000</td><td style=\"text-align: right;\"> 227.676</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1544.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1808000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-33\n",
      "  done: false\n",
      "  episode_len_mean: 1529.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 224.87146948818238\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1315\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.41096365451812744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012068714015185833\n",
      "          model: {}\n",
      "          policy_loss: -0.039094291627407074\n",
      "          total_loss: 45.693634033203125\n",
      "          vf_explained_var: 0.5119426846504211\n",
      "          vf_loss: 45.71725845336914\n",
      "    num_agent_steps_sampled: 1808000\n",
      "    num_agent_steps_trained: 1808000\n",
      "    num_steps_sampled: 1808000\n",
      "    num_steps_trained: 1808000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.82000000000001\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077946185393163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3162620793512506\n",
      "    mean_inference_ms: 0.5266248712196016\n",
      "    mean_raw_obs_processing_ms: 0.07037850924314572\n",
      "  time_since_restore: 2371.3000767230988\n",
      "  time_this_iter_s: 3.916757822036743\n",
      "  time_total_s: 2371.3000767230988\n",
      "  timers:\n",
      "    learn_throughput: 2066.486\n",
      "    learn_time_ms: 1935.653\n",
      "    load_throughput: 33071586.832\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1002.563\n",
      "    sample_time_ms: 3989.774\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671819933\n",
      "  timesteps_since_restore: 1808000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1808000\n",
      "  training_iteration: 452\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:37 (running for 00:39:50.55)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         2375.25</td><td style=\"text-align: right;\">1812000</td><td style=\"text-align: right;\">  224.99</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1816000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 1529.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.81050979803615\n",
      "  episode_reward_mean: 225.29284577853153\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1319\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.39299237728118896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012165771797299385\n",
      "          model: {}\n",
      "          policy_loss: -0.03973587974905968\n",
      "          total_loss: 8.2667236328125\n",
      "          vf_explained_var: 0.6673125624656677\n",
      "          vf_loss: 8.290868759155273\n",
      "    num_agent_steps_sampled: 1816000\n",
      "    num_agent_steps_trained: 1816000\n",
      "    num_steps_sampled: 1816000\n",
      "    num_steps_trained: 1816000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.550000000000004\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907806246644896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3162780004899173\n",
      "    mean_inference_ms: 0.5265993123206346\n",
      "    mean_raw_obs_processing_ms: 0.07038079324009995\n",
      "  time_since_restore: 2379.220116376877\n",
      "  time_this_iter_s: 3.9740023612976074\n",
      "  time_total_s: 2379.220116376877\n",
      "  timers:\n",
      "    learn_throughput: 2066.807\n",
      "    learn_time_ms: 1935.353\n",
      "    load_throughput: 33156553.36\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1004.042\n",
      "    sample_time_ms: 3983.898\n",
      "    update_time_ms: 1.363\n",
      "  timestamp: 1671819941\n",
      "  timesteps_since_restore: 1816000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1816000\n",
      "  training_iteration: 454\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:43 (running for 00:39:56.55)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         2379.22</td><td style=\"text-align: right;\">1816000</td><td style=\"text-align: right;\"> 225.293</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:48 (running for 00:40:01.75)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">          2383.4</td><td style=\"text-align: right;\">1820000</td><td style=\"text-align: right;\"> 225.617</td><td style=\"text-align: right;\">             261.811</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1824000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 1529.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 264.58789254408254\n",
      "  episode_reward_mean: 226.28448071064588\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1325\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.32304099202156067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011901439167559147\n",
      "          model: {}\n",
      "          policy_loss: -0.03967227041721344\n",
      "          total_loss: 9.514860153198242\n",
      "          vf_explained_var: 0.5476212501525879\n",
      "          vf_loss: 9.539281845092773\n",
      "    num_agent_steps_sampled: 1824000\n",
      "    num_agent_steps_trained: 1824000\n",
      "    num_steps_sampled: 1824000\n",
      "    num_steps_trained: 1824000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.18\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077710003763646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31624038617912165\n",
      "    mean_inference_ms: 0.5266094731511091\n",
      "    mean_raw_obs_processing_ms: 0.07037413993457886\n",
      "  time_since_restore: 2387.3568868637085\n",
      "  time_this_iter_s: 3.957707405090332\n",
      "  time_total_s: 2387.3568868637085\n",
      "  timers:\n",
      "    learn_throughput: 2048.866\n",
      "    learn_time_ms: 1952.3\n",
      "    load_throughput: 32023699.179\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 999.535\n",
      "    sample_time_ms: 4001.862\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1671819949\n",
      "  timesteps_since_restore: 1824000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1824000\n",
      "  training_iteration: 456\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:54 (running for 00:40:07.65)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         2391.27</td><td style=\"text-align: right;\">1828000</td><td style=\"text-align: right;\"> 226.846</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1832000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 1529.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 226.93388897562875\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1329\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.41594862937927246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011978373862802982\n",
      "          model: {}\n",
      "          policy_loss: -0.04113112390041351\n",
      "          total_loss: 7.634951591491699\n",
      "          vf_explained_var: 0.7038027048110962\n",
      "          vf_loss: 7.660733699798584\n",
      "    num_agent_steps_sampled: 1832000\n",
      "    num_agent_steps_trained: 1832000\n",
      "    num_steps_sampled: 1832000\n",
      "    num_steps_trained: 1832000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.6\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077815348407282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31625513994957044\n",
      "    mean_inference_ms: 0.5265828905240388\n",
      "    mean_raw_obs_processing_ms: 0.07037627313403293\n",
      "  time_since_restore: 2395.2620525360107\n",
      "  time_this_iter_s: 3.993016481399536\n",
      "  time_total_s: 2395.2620525360107\n",
      "  timers:\n",
      "    learn_throughput: 2050.821\n",
      "    learn_time_ms: 1950.439\n",
      "    load_throughput: 32257673.524\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1000.295\n",
      "    sample_time_ms: 3998.82\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1671819957\n",
      "  timesteps_since_restore: 1832000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1832000\n",
      "  training_iteration: 458\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:25:59 (running for 00:40:12.67)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         2395.26</td><td style=\"text-align: right;\">1832000</td><td style=\"text-align: right;\"> 226.934</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:04 (running for 00:40:17.68)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         2399.26</td><td style=\"text-align: right;\">1836000</td><td style=\"text-align: right;\"> 227.058</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-05\n",
      "  done: false\n",
      "  episode_len_mean: 1529.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 227.09220228701213\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1335\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4135962724685669\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01334218680858612\n",
      "          model: {}\n",
      "          policy_loss: -0.04457241669297218\n",
      "          total_loss: 8.725117683410645\n",
      "          vf_explained_var: 0.5594907402992249\n",
      "          vf_loss: 8.752593040466309\n",
      "    num_agent_steps_sampled: 1840000\n",
      "    num_agent_steps_trained: 1840000\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.083333333333336\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907741747246737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3162140872728379\n",
      "    mean_inference_ms: 0.5265896139441085\n",
      "    mean_raw_obs_processing_ms: 0.07036905064877495\n",
      "  time_since_restore: 2403.210469722748\n",
      "  time_this_iter_s: 3.954164981842041\n",
      "  time_total_s: 2403.210469722748\n",
      "  timers:\n",
      "    learn_throughput: 2047.154\n",
      "    learn_time_ms: 1953.932\n",
      "    load_throughput: 34295208.504\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 999.963\n",
      "    sample_time_ms: 4000.147\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1671819965\n",
      "  timesteps_since_restore: 1840000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 460\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:10 (running for 00:40:23.64)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         2407.18</td><td style=\"text-align: right;\">1844000</td><td style=\"text-align: right;\"> 227.399</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1848000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 1529.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 227.32111042803368\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1339\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17045772075653076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010573194362223148\n",
      "          model: {}\n",
      "          policy_loss: -0.038613952696323395\n",
      "          total_loss: 6.594333648681641\n",
      "          vf_explained_var: 0.6629331707954407\n",
      "          vf_loss: 6.619399070739746\n",
      "    num_agent_steps_sampled: 1848000\n",
      "    num_agent_steps_trained: 1848000\n",
      "    num_steps_sampled: 1848000\n",
      "    num_steps_trained: 1848000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.44\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077515627954537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3162275005678793\n",
      "    mean_inference_ms: 0.526562062211583\n",
      "    mean_raw_obs_processing_ms: 0.07037098610828667\n",
      "  time_since_restore: 2411.089352607727\n",
      "  time_this_iter_s: 3.9106171131134033\n",
      "  time_total_s: 2411.089352607727\n",
      "  timers:\n",
      "    learn_throughput: 2047.426\n",
      "    learn_time_ms: 1953.673\n",
      "    load_throughput: 32078806.883\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1000.242\n",
      "    sample_time_ms: 3999.034\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671819973\n",
      "  timesteps_since_restore: 1848000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1848000\n",
      "  training_iteration: 462\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:16 (running for 00:40:29.57)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         2411.09</td><td style=\"text-align: right;\">1848000</td><td style=\"text-align: right;\"> 227.321</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1856000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 1529.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 227.38813262894087\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1345\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.2586590349674225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013439629226922989\n",
      "          model: {}\n",
      "          policy_loss: -0.050603821873664856\n",
      "          total_loss: 11.2136812210083\n",
      "          vf_explained_var: 0.526397168636322\n",
      "          vf_loss: 11.247062683105469\n",
      "    num_agent_steps_sampled: 1856000\n",
      "    num_agent_steps_trained: 1856000\n",
      "    num_steps_sampled: 1856000\n",
      "    num_steps_trained: 1856000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.96666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077120840777819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3161860136451775\n",
      "    mean_inference_ms: 0.5265689871828362\n",
      "    mean_raw_obs_processing_ms: 0.07036391767228994\n",
      "  time_since_restore: 2419.01952791214\n",
      "  time_this_iter_s: 3.9648303985595703\n",
      "  time_total_s: 2419.01952791214\n",
      "  timers:\n",
      "    learn_throughput: 2047.665\n",
      "    learn_time_ms: 1953.445\n",
      "    load_throughput: 30223772.293\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1000.111\n",
      "    sample_time_ms: 3999.555\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671819981\n",
      "  timesteps_since_restore: 1856000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1856000\n",
      "  training_iteration: 464\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:22 (running for 00:40:35.54)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         2419.02</td><td style=\"text-align: right;\">1856000</td><td style=\"text-align: right;\"> 227.388</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:28 (running for 00:40:41.58)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         2422.98</td><td style=\"text-align: right;\">1860000</td><td style=\"text-align: right;\"> 227.526</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">           1529.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1864000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-29\n",
      "  done: false\n",
      "  episode_len_mean: 1528.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 228.28595753611688\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1349\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.335926353931427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014471394941210747\n",
      "          model: {}\n",
      "          policy_loss: -0.039090823382139206\n",
      "          total_loss: 5.221917629241943\n",
      "          vf_explained_var: 0.6428247690200806\n",
      "          vf_loss: 5.242464065551758\n",
      "    num_agent_steps_sampled: 1864000\n",
      "    num_agent_steps_trained: 1864000\n",
      "    num_steps_sampled: 1864000\n",
      "    num_steps_trained: 1864000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.199999999999996\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077210602834172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31619893218930817\n",
      "    mean_inference_ms: 0.5265415838431281\n",
      "    mean_raw_obs_processing_ms: 0.070365960451176\n",
      "  time_since_restore: 2426.900737285614\n",
      "  time_this_iter_s: 3.915916919708252\n",
      "  time_total_s: 2426.900737285614\n",
      "  timers:\n",
      "    learn_throughput: 2063.646\n",
      "    learn_time_ms: 1938.317\n",
      "    load_throughput: 31271604.846\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1006.57\n",
      "    sample_time_ms: 3973.891\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671819989\n",
      "  timesteps_since_restore: 1864000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1864000\n",
      "  training_iteration: 466\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:34 (running for 00:40:47.47)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         2430.89</td><td style=\"text-align: right;\">1868000</td><td style=\"text-align: right;\"> 228.271</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">            1528.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1872000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-37\n",
      "  done: false\n",
      "  episode_len_mean: 1528.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 228.7770543425036\n",
      "  episode_reward_min: -128.4091189315263\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1355\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.18827153742313385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010544066317379475\n",
      "          model: {}\n",
      "          policy_loss: -0.03972078487277031\n",
      "          total_loss: 9.841580390930176\n",
      "          vf_explained_var: 0.5011754035949707\n",
      "          vf_loss: 9.867790222167969\n",
      "    num_agent_steps_sampled: 1872000\n",
      "    num_agent_steps_trained: 1872000\n",
      "    num_steps_sampled: 1872000\n",
      "    num_steps_trained: 1872000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.68\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09076804578192356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31615604747305165\n",
      "    mean_inference_ms: 0.5265477049317684\n",
      "    mean_raw_obs_processing_ms: 0.07035901074109396\n",
      "  time_since_restore: 2434.8458075523376\n",
      "  time_this_iter_s: 3.953521251678467\n",
      "  time_total_s: 2434.8458075523376\n",
      "  timers:\n",
      "    learn_throughput: 2063.116\n",
      "    learn_time_ms: 1938.815\n",
      "    load_throughput: 31283266.828\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1005.816\n",
      "    sample_time_ms: 3976.871\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1671819997\n",
      "  timesteps_since_restore: 1872000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1872000\n",
      "  training_iteration: 468\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:39 (running for 00:40:52.48)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         2434.85</td><td style=\"text-align: right;\">1872000</td><td style=\"text-align: right;\"> 228.777</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -128.409</td><td style=\"text-align: right;\">            1528.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:44 (running for 00:40:57.50)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         2438.88</td><td style=\"text-align: right;\">1876000</td><td style=\"text-align: right;\"> 229.129</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1528.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-45\n",
      "  done: false\n",
      "  episode_len_mean: 1528.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 229.7008191274034\n",
      "  episode_reward_min: -121.54327716779832\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.42008453607559204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012744116596877575\n",
      "          model: {}\n",
      "          policy_loss: -0.04283154383301735\n",
      "          total_loss: 6.4205145835876465\n",
      "          vf_explained_var: 0.6534603834152222\n",
      "          vf_loss: 6.447014808654785\n",
      "    num_agent_steps_sampled: 1880000\n",
      "    num_agent_steps_trained: 1880000\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.71666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09076825972789285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3161641854339723\n",
      "    mean_inference_ms: 0.5265158051533819\n",
      "    mean_raw_obs_processing_ms: 0.07036054320453604\n",
      "  time_since_restore: 2442.8541231155396\n",
      "  time_this_iter_s: 3.9731807708740234\n",
      "  time_total_s: 2442.8541231155396\n",
      "  timers:\n",
      "    learn_throughput: 2065.311\n",
      "    learn_time_ms: 1936.754\n",
      "    load_throughput: 31283266.828\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1003.398\n",
      "    sample_time_ms: 3986.454\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1671820005\n",
      "  timesteps_since_restore: 1880000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 470\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:49 (running for 00:41:02.52)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         2442.85</td><td style=\"text-align: right;\">1880000</td><td style=\"text-align: right;\"> 229.701</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1528.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1888000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-26-53\n",
      "  done: false\n",
      "  episode_len_mean: 1543.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 234.31445793973407\n",
      "  episode_reward_min: -121.54327716779832\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1366\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3180694282054901\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012169046327471733\n",
      "          model: {}\n",
      "          policy_loss: -0.0407380573451519\n",
      "          total_loss: 11.491711616516113\n",
      "          vf_explained_var: 0.6123350262641907\n",
      "          vf_loss: 11.516855239868164\n",
      "    num_agent_steps_sampled: 1888000\n",
      "    num_agent_steps_trained: 1888000\n",
      "    num_steps_sampled: 1888000\n",
      "    num_steps_trained: 1888000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.260000000000005\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907662129770241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3161444843382101\n",
      "    mean_inference_ms: 0.526502592287009\n",
      "    mean_raw_obs_processing_ms: 0.07035751383216723\n",
      "  time_since_restore: 2450.8843619823456\n",
      "  time_this_iter_s: 4.030239105224609\n",
      "  time_total_s: 2450.8843619823456\n",
      "  timers:\n",
      "    learn_throughput: 2066.321\n",
      "    learn_time_ms: 1935.808\n",
      "    load_throughput: 33414092.81\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 999.611\n",
      "    sample_time_ms: 4001.558\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1671820013\n",
      "  timesteps_since_restore: 1888000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1888000\n",
      "  training_iteration: 472\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:54 (running for 00:41:07.59)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         2450.88</td><td style=\"text-align: right;\">1888000</td><td style=\"text-align: right;\"> 234.314</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1543.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:26:59 (running for 00:41:12.66)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         2454.88</td><td style=\"text-align: right;\">1892000</td><td style=\"text-align: right;\"> 234.664</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1543.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1896000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 1543.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 234.89462873289816\n",
      "  episode_reward_min: -121.54327716779832\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1370\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.35387998819351196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012961373664438725\n",
      "          model: {}\n",
      "          policy_loss: -0.04077131301164627\n",
      "          total_loss: 6.970323085784912\n",
      "          vf_explained_var: 0.6193326711654663\n",
      "          vf_loss: 6.994484901428223\n",
      "    num_agent_steps_sampled: 1896000\n",
      "    num_agent_steps_trained: 1896000\n",
      "    num_steps_sampled: 1896000\n",
      "    num_steps_trained: 1896000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.28333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09076497203283009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31613174336946614\n",
      "    mean_inference_ms: 0.5264940223091267\n",
      "    mean_raw_obs_processing_ms: 0.07035549113608093\n",
      "  time_since_restore: 2458.851181268692\n",
      "  time_this_iter_s: 3.9710004329681396\n",
      "  time_total_s: 2458.851181268692\n",
      "  timers:\n",
      "    learn_throughput: 2062.256\n",
      "    learn_time_ms: 1939.623\n",
      "    load_throughput: 32710501.072\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 999.557\n",
      "    sample_time_ms: 4001.771\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671820021\n",
      "  timesteps_since_restore: 1896000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1896000\n",
      "  training_iteration: 474\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:05 (running for 00:41:18.58)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         2462.82</td><td style=\"text-align: right;\">1900000</td><td style=\"text-align: right;\">  235.13</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1543.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1904000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 1543.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 235.77693266726948\n",
      "  episode_reward_min: -121.54327716779832\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1376\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.33109715580940247\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011822093278169632\n",
      "          model: {}\n",
      "          policy_loss: -0.04686351120471954\n",
      "          total_loss: 14.418116569519043\n",
      "          vf_explained_var: 0.5330999493598938\n",
      "          vf_loss: 14.449831008911133\n",
      "    num_agent_steps_sampled: 1904000\n",
      "    num_agent_steps_trained: 1904000\n",
      "    num_steps_sampled: 1904000\n",
      "    num_steps_trained: 1904000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.220000000000006\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09076292235429596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31611121748953275\n",
      "    mean_inference_ms: 0.5264794263781534\n",
      "    mean_raw_obs_processing_ms: 0.07035225789905528\n",
      "  time_since_restore: 2466.7854313850403\n",
      "  time_this_iter_s: 3.9688422679901123\n",
      "  time_total_s: 2466.7854313850403\n",
      "  timers:\n",
      "    learn_throughput: 2065.879\n",
      "    learn_time_ms: 1936.222\n",
      "    load_throughput: 32513984.496\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 997.076\n",
      "    sample_time_ms: 4011.732\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1671820029\n",
      "  timesteps_since_restore: 1904000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1904000\n",
      "  training_iteration: 476\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:10 (running for 00:41:23.61)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         2466.79</td><td style=\"text-align: right;\">1904000</td><td style=\"text-align: right;\"> 235.777</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1543.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:16 (running for 00:41:29.61)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         2470.81</td><td style=\"text-align: right;\">1908000</td><td style=\"text-align: right;\"> 236.101</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.543</td><td style=\"text-align: right;\">           1543.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1912000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 1558.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 240.02251341317614\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5053190588951111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011902358382940292\n",
      "          model: {}\n",
      "          policy_loss: -0.036575235426425934\n",
      "          total_loss: 7.376889228820801\n",
      "          vf_explained_var: 0.6058486104011536\n",
      "          vf_loss: 7.398212432861328\n",
      "    num_agent_steps_sampled: 1912000\n",
      "    num_agent_steps_trained: 1912000\n",
      "    num_steps_sampled: 1912000\n",
      "    num_steps_trained: 1912000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.85\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09075959481796328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3160743201509686\n",
      "    mean_inference_ms: 0.5264897678464877\n",
      "    mean_raw_obs_processing_ms: 0.07034607679403462\n",
      "  time_since_restore: 2474.7185986042023\n",
      "  time_this_iter_s: 3.9113802909851074\n",
      "  time_total_s: 2474.7185986042023\n",
      "  timers:\n",
      "    learn_throughput: 2065.6\n",
      "    learn_time_ms: 1936.483\n",
      "    load_throughput: 30772589.875\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 997.547\n",
      "    sample_time_ms: 4009.836\n",
      "    update_time_ms: 1.341\n",
      "  timestamp: 1671820037\n",
      "  timesteps_since_restore: 1912000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1912000\n",
      "  training_iteration: 478\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:22 (running for 00:41:35.56)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         2478.67</td><td style=\"text-align: right;\">1916000</td><td style=\"text-align: right;\"> 240.399</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1558.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 1556.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 241.008986894492\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1386\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.39392220973968506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012342577800154686\n",
      "          model: {}\n",
      "          policy_loss: -0.04519597813487053\n",
      "          total_loss: 11.993477821350098\n",
      "          vf_explained_var: 0.5507482290267944\n",
      "          vf_loss: 12.022857666015625\n",
      "    num_agent_steps_sampled: 1920000\n",
      "    num_agent_steps_trained: 1920000\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.93333333333334\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09075944907982575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3160766128445129\n",
      "    mean_inference_ms: 0.5264521826445356\n",
      "    mean_raw_obs_processing_ms: 0.07034666912854443\n",
      "  time_since_restore: 2482.6508173942566\n",
      "  time_this_iter_s: 3.9780590534210205\n",
      "  time_total_s: 2482.6508173942566\n",
      "  timers:\n",
      "    learn_throughput: 2070.483\n",
      "    learn_time_ms: 1931.917\n",
      "    load_throughput: 29081671.0\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 999.254\n",
      "    sample_time_ms: 4002.984\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671820045\n",
      "  timesteps_since_restore: 1920000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 480\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:28 (running for 00:41:41.51)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         2482.65</td><td style=\"text-align: right;\">1920000</td><td style=\"text-align: right;\"> 241.009</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1556.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1928000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 1556.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 241.5326801948339\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1390\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3179171681404114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011639094911515713\n",
      "          model: {}\n",
      "          policy_loss: -0.04199374467134476\n",
      "          total_loss: 6.197161674499512\n",
      "          vf_explained_var: 0.5568915009498596\n",
      "          vf_loss: 6.224240779876709\n",
      "    num_agent_steps_sampled: 1928000\n",
      "    num_agent_steps_trained: 1928000\n",
      "    num_steps_sampled: 1928000\n",
      "    num_steps_trained: 1928000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.52\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09075571504145064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3160370880658924\n",
      "    mean_inference_ms: 0.5264590695302094\n",
      "    mean_raw_obs_processing_ms: 0.0703401636439825\n",
      "  time_since_restore: 2490.6092340946198\n",
      "  time_this_iter_s: 3.9706850051879883\n",
      "  time_total_s: 2490.6092340946198\n",
      "  timers:\n",
      "    learn_throughput: 2068.075\n",
      "    learn_time_ms: 1934.166\n",
      "    load_throughput: 28392648.502\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 1002.003\n",
      "    sample_time_ms: 3992.005\n",
      "    update_time_ms: 1.327\n",
      "  timestamp: 1671820053\n",
      "  timesteps_since_restore: 1928000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1928000\n",
      "  training_iteration: 482\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:34 (running for 00:41:47.51)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         2490.61</td><td style=\"text-align: right;\">1928000</td><td style=\"text-align: right;\"> 241.533</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1556.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:39 (running for 00:41:52.54)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         2494.62</td><td style=\"text-align: right;\">1932000</td><td style=\"text-align: right;\"> 241.507</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1556.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1936000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 1567.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 245.11431825180327\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1396\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4261641204357147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012421778403222561\n",
      "          model: {}\n",
      "          policy_loss: -0.04160025715827942\n",
      "          total_loss: 10.900381088256836\n",
      "          vf_explained_var: 0.5034850835800171\n",
      "          vf_loss: 10.926064491271973\n",
      "    num_agent_steps_sampled: 1936000\n",
      "    num_agent_steps_trained: 1936000\n",
      "    num_steps_sampled: 1936000\n",
      "    num_steps_trained: 1936000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.349999999999994\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09075322627447151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3160137105483983\n",
      "    mean_inference_ms: 0.5264393871845438\n",
      "    mean_raw_obs_processing_ms: 0.07033632512417927\n",
      "  time_since_restore: 2498.532969713211\n",
      "  time_this_iter_s: 3.9098801612854004\n",
      "  time_total_s: 2498.532969713211\n",
      "  timers:\n",
      "    learn_throughput: 2071.034\n",
      "    learn_time_ms: 1931.402\n",
      "    load_throughput: 28173326.616\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 1002.316\n",
      "    sample_time_ms: 3990.759\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1671820061\n",
      "  timesteps_since_restore: 1936000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1936000\n",
      "  training_iteration: 484\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:45 (running for 00:41:58.45)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">          2502.5</td><td style=\"text-align: right;\">1940000</td><td style=\"text-align: right;\"> 242.835</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1557.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1944000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-48\n",
      "  done: false\n",
      "  episode_len_mean: 1556.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 243.39407314992786\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1401\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.47134897112846375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016045348718762398\n",
      "          model: {}\n",
      "          policy_loss: -0.04172414541244507\n",
      "          total_loss: 7.6542277336120605\n",
      "          vf_explained_var: 0.5711655616760254\n",
      "          vf_loss: 7.675390720367432\n",
      "    num_agent_steps_sampled: 1944000\n",
      "    num_agent_steps_trained: 1944000\n",
      "    num_steps_sampled: 1944000\n",
      "    num_steps_trained: 1944000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.56666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09075317741553404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3160184309046787\n",
      "    mean_inference_ms: 0.5264030173234773\n",
      "    mean_raw_obs_processing_ms: 0.07033750403300836\n",
      "  time_since_restore: 2506.431315422058\n",
      "  time_this_iter_s: 3.9358129501342773\n",
      "  time_total_s: 2506.431315422058\n",
      "  timers:\n",
      "    learn_throughput: 2070.685\n",
      "    learn_time_ms: 1931.727\n",
      "    load_throughput: 26921078.306\n",
      "    load_time_ms: 0.149\n",
      "    sample_throughput: 1003.721\n",
      "    sample_time_ms: 3985.171\n",
      "    update_time_ms: 1.283\n",
      "  timestamp: 1671820068\n",
      "  timesteps_since_restore: 1944000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1944000\n",
      "  training_iteration: 486\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:50 (running for 00:42:04.41)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         2506.43</td><td style=\"text-align: right;\">1944000</td><td style=\"text-align: right;\"> 243.394</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1556.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:27:56 (running for 00:42:09.45)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         2510.41</td><td style=\"text-align: right;\">1948000</td><td style=\"text-align: right;\"> 243.629</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1556.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1952000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 1549.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 242.34980545995145\n",
      "  episode_reward_min: -121.14141764620194\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1406\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.416751503944397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010440447367727757\n",
      "          model: {}\n",
      "          policy_loss: -0.035962872207164764\n",
      "          total_loss: 27.815549850463867\n",
      "          vf_explained_var: 0.5631352663040161\n",
      "          vf_loss: 27.838132858276367\n",
      "    num_agent_steps_sampled: 1952000\n",
      "    num_agent_steps_trained: 1952000\n",
      "    num_steps_sampled: 1952000\n",
      "    num_steps_trained: 1952000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.5\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074875655723215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31597415178321364\n",
      "    mean_inference_ms: 0.5264044100523724\n",
      "    mean_raw_obs_processing_ms: 0.07033001821430294\n",
      "  time_since_restore: 2514.325085401535\n",
      "  time_this_iter_s: 3.915205240249634\n",
      "  time_total_s: 2514.325085401535\n",
      "  timers:\n",
      "    learn_throughput: 2071.769\n",
      "    learn_time_ms: 1930.717\n",
      "    load_throughput: 28220716.569\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 1004.768\n",
      "    sample_time_ms: 3981.018\n",
      "    update_time_ms: 1.277\n",
      "  timestamp: 1671820076\n",
      "  timesteps_since_restore: 1952000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1952000\n",
      "  training_iteration: 488\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:01 (running for 00:42:15.25)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         2518.22</td><td style=\"text-align: right;\">1956000</td><td style=\"text-align: right;\"> 242.573</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -121.141</td><td style=\"text-align: right;\">           1548.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 1563.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 246.38578423123116\n",
      "  episode_reward_min: -120.98284123309391\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1412\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4720696210861206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013560605235397816\n",
      "          model: {}\n",
      "          policy_loss: -0.040962040424346924\n",
      "          total_loss: 8.301159858703613\n",
      "          vf_explained_var: 0.5487090945243835\n",
      "          vf_loss: 8.324745178222656\n",
      "    num_agent_steps_sampled: 1960000\n",
      "    num_agent_steps_trained: 1960000\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1960000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.05\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074793595053265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31597385879281537\n",
      "    mean_inference_ms: 0.526362026221755\n",
      "    mean_raw_obs_processing_ms: 0.07033012369885887\n",
      "  time_since_restore: 2522.1455907821655\n",
      "  time_this_iter_s: 3.923977851867676\n",
      "  time_total_s: 2522.1455907821655\n",
      "  timers:\n",
      "    learn_throughput: 2072.452\n",
      "    learn_time_ms: 1930.081\n",
      "    load_throughput: 29810262.971\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 1007.877\n",
      "    sample_time_ms: 3968.74\n",
      "    update_time_ms: 1.289\n",
      "  timestamp: 1671820084\n",
      "  timesteps_since_restore: 1960000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 490\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:07 (running for 00:42:21.24)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         2522.15</td><td style=\"text-align: right;\">1960000</td><td style=\"text-align: right;\"> 246.386</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -120.983</td><td style=\"text-align: right;\">           1563.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1968000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-28-12\n",
      "  done: false\n",
      "  episode_len_mean: 1563.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 246.41035703191392\n",
      "  episode_reward_min: -120.98284123309391\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1416\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17871835827827454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012060015462338924\n",
      "          model: {}\n",
      "          policy_loss: -0.04161639139056206\n",
      "          total_loss: 6.587676525115967\n",
      "          vf_explained_var: 0.6268178224563599\n",
      "          vf_loss: 6.613837718963623\n",
      "    num_agent_steps_sampled: 1968000\n",
      "    num_agent_steps_trained: 1968000\n",
      "    num_steps_sampled: 1968000\n",
      "    num_steps_trained: 1968000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.18333333333333\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074575201373429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3159564160722311\n",
      "    mean_inference_ms: 0.5263451798503355\n",
      "    mean_raw_obs_processing_ms: 0.07032709136029835\n",
      "  time_since_restore: 2529.970956325531\n",
      "  time_this_iter_s: 3.9258859157562256\n",
      "  time_total_s: 2529.970956325531\n",
      "  timers:\n",
      "    learn_throughput: 2075.101\n",
      "    learn_time_ms: 1927.617\n",
      "    load_throughput: 30593026.988\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1011.204\n",
      "    sample_time_ms: 3955.679\n",
      "    update_time_ms: 1.341\n",
      "  timestamp: 1671820092\n",
      "  timesteps_since_restore: 1968000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1968000\n",
      "  training_iteration: 492\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:13 (running for 00:42:27.06)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         2529.97</td><td style=\"text-align: right;\">1968000</td><td style=\"text-align: right;\">  246.41</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -120.983</td><td style=\"text-align: right;\">           1563.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:19 (running for 00:42:33.06)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         2533.95</td><td style=\"text-align: right;\">1972000</td><td style=\"text-align: right;\"> 246.384</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -120.983</td><td style=\"text-align: right;\">            1562.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1976000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-28-20\n",
      "  done: false\n",
      "  episode_len_mean: 1562.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 246.3873376740867\n",
      "  episode_reward_min: -120.98284123309391\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1422\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.22213324904441833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013791537843644619\n",
      "          model: {}\n",
      "          policy_loss: -0.0404994934797287\n",
      "          total_loss: 10.105560302734375\n",
      "          vf_explained_var: 0.6062010526657104\n",
      "          vf_loss: 10.128385543823242\n",
      "    num_agent_steps_sampled: 1976000\n",
      "    num_agent_steps_trained: 1976000\n",
      "    num_steps_sampled: 1976000\n",
      "    num_steps_trained: 1976000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.35\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074243901646288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31593113171598175\n",
      "    mean_inference_ms: 0.5263199133080243\n",
      "    mean_raw_obs_processing_ms: 0.0703227157812892\n",
      "  time_since_restore: 2537.941018819809\n",
      "  time_this_iter_s: 3.9902913570404053\n",
      "  time_total_s: 2537.941018819809\n",
      "  timers:\n",
      "    learn_throughput: 2077.041\n",
      "    learn_time_ms: 1925.817\n",
      "    load_throughput: 31371009.723\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1009.949\n",
      "    sample_time_ms: 3960.597\n",
      "    update_time_ms: 1.341\n",
      "  timestamp: 1671820100\n",
      "  timesteps_since_restore: 1976000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1976000\n",
      "  training_iteration: 494\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:25 (running for 00:42:39.04)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">          2541.9</td><td style=\"text-align: right;\">1980000</td><td style=\"text-align: right;\">  246.79</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -120.983</td><td style=\"text-align: right;\">           1562.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1984000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 1561.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.5681203482074\n",
      "  episode_reward_mean: 247.04251569238474\n",
      "  episode_reward_min: -120.98284123309391\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1426\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.27960309386253357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011727213859558105\n",
      "          model: {}\n",
      "          policy_loss: -0.035470977425575256\n",
      "          total_loss: 5.494680881500244\n",
      "          vf_explained_var: 0.5899887084960938\n",
      "          vf_loss: 5.515124320983887\n",
      "    num_agent_steps_sampled: 1984000\n",
      "    num_agent_steps_trained: 1984000\n",
      "    num_steps_sampled: 1984000\n",
      "    num_steps_trained: 1984000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.56666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09074029114843274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.315914713347874\n",
      "    mean_inference_ms: 0.5263037653527242\n",
      "    mean_raw_obs_processing_ms: 0.07031958060736204\n",
      "  time_since_restore: 2545.8351838588715\n",
      "  time_this_iter_s: 3.93723464012146\n",
      "  time_total_s: 2545.8351838588715\n",
      "  timers:\n",
      "    learn_throughput: 2078.742\n",
      "    learn_time_ms: 1924.241\n",
      "    load_throughput: 31034435.812\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1010.239\n",
      "    sample_time_ms: 3959.459\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1671820108\n",
      "  timesteps_since_restore: 1984000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1984000\n",
      "  training_iteration: 496\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:31 (running for 00:42:45.00)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         2545.84</td><td style=\"text-align: right;\">1984000</td><td style=\"text-align: right;\"> 247.043</td><td style=\"text-align: right;\">             271.568</td><td style=\"text-align: right;\">            -120.983</td><td style=\"text-align: right;\">           1561.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 1992000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 1547.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.56342755043363\n",
      "  episode_reward_mean: 243.85276867077056\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1433\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4043789803981781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011946499347686768\n",
      "          model: {}\n",
      "          policy_loss: -0.025402933359146118\n",
      "          total_loss: 69.20802307128906\n",
      "          vf_explained_var: 0.5872021317481995\n",
      "          vf_loss: 69.21810913085938\n",
      "    num_agent_steps_sampled: 1992000\n",
      "    num_agent_steps_trained: 1992000\n",
      "    num_steps_sampled: 1992000\n",
      "    num_steps_trained: 1992000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.01666666666667\n",
      "    ram_util_percent: 30.0\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09073854516706426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31591105138034115\n",
      "    mean_inference_ms: 0.5262564106129559\n",
      "    mean_raw_obs_processing_ms: 0.07031847210211829\n",
      "  time_since_restore: 2553.719183921814\n",
      "  time_this_iter_s: 3.948514699935913\n",
      "  time_total_s: 2553.719183921814\n",
      "  timers:\n",
      "    learn_throughput: 2079.699\n",
      "    learn_time_ms: 1923.356\n",
      "    load_throughput: 31074673.088\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1010.623\n",
      "    sample_time_ms: 3957.955\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1671820116\n",
      "  timesteps_since_restore: 1992000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1992000\n",
      "  training_iteration: 498\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:37 (running for 00:42:50.96)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         2553.72</td><td style=\"text-align: right;\">1992000</td><td style=\"text-align: right;\"> 243.853</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1547.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:42 (running for 00:42:56.11)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:47 (running for 00:43:01.15)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:52 (running for 00:43:06.15)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:28:57 (running for 00:43:11.16)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:02 (running for 00:43:16.16)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:07 (running for 00:43:21.17)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:12 (running for 00:43:26.17)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:17 (running for 00:43:31.17)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:22 (running for 00:43:36.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:27 (running for 00:43:41.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:32 (running for 00:43:46.19)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:37 (running for 00:43:51.19)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:42 (running for 00:43:56.20)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:47 (running for 00:44:01.20)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:52 (running for 00:44:06.20)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:29:57 (running for 00:44:11.21)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:02 (running for 00:44:16.21)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:07 (running for 00:44:21.22)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:12 (running for 00:44:26.22)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:17 (running for 00:44:31.23)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:22 (running for 00:44:36.23)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:27 (running for 00:44:41.23)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:32 (running for 00:44:46.24)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:37 (running for 00:44:51.24)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:42 (running for 00:44:56.25)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:47 (running for 00:45:01.25)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:52 (running for 00:45:06.26)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:30:57 (running for 00:45:11.26)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:02 (running for 00:45:16.26)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         2557.89</td><td style=\"text-align: right;\">1996000</td><td style=\"text-align: right;\"> 241.818</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1540.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-06\n",
      "  done: false\n",
      "  episode_len_mean: 1540.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.56342755043363\n",
      "  episode_reward_mean: 242.33806496891197\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1438\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1508.36\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 275.0898076907855\n",
      "    episode_reward_mean: 244.70181296939515\n",
      "    episode_reward_min: -128.95638703465153\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1588\n",
      "      - 1549\n",
      "      - 1490\n",
      "      - 1470\n",
      "      - 1600\n",
      "      - 1514\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1583\n",
      "      - 1582\n",
      "      - 1579\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1562\n",
      "      - 1586\n",
      "      - 1599\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1545\n",
      "      - 1600\n",
      "      - 1591\n",
      "      - 1570\n",
      "      - 1584\n",
      "      - 1572\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 177\n",
      "      - 1600\n",
      "      - 1599\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1572\n",
      "      - 1600\n",
      "      - 1599\n",
      "      - 1557\n",
      "      - 1600\n",
      "      - 1554\n",
      "      - 1600\n",
      "      - 1595\n",
      "      - 1600\n",
      "      - 1526\n",
      "      - 1600\n",
      "      - 1576\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1593\n",
      "      - 262\n",
      "      - 1578\n",
      "      - 384\n",
      "      - 526\n",
      "      - 1580\n",
      "      - 1569\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1555\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1530\n",
      "      - 1548\n",
      "      - 1596\n",
      "      - 1560\n",
      "      - 1600\n",
      "      - 1588\n",
      "      - 1600\n",
      "      - 64\n",
      "      - 1522\n",
      "      - 1525\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1558\n",
      "      - 1568\n",
      "      - 1564\n",
      "      - 1600\n",
      "      - 1496\n",
      "      - 1568\n",
      "      - 1597\n",
      "      - 1600\n",
      "      - 1576\n",
      "      - 1600\n",
      "      - 1574\n",
      "      - 1572\n",
      "      - 1565\n",
      "      - 1552\n",
      "      - 1567\n",
      "      - 1573\n",
      "      - 833\n",
      "      - 1600\n",
      "      - 1574\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 253.41245090179373\n",
      "      - 262.0895843823549\n",
      "      - 267.85534455437937\n",
      "      - 262.54507174042527\n",
      "      - 269.47202605859246\n",
      "      - 269.5670826704886\n",
      "      - 274.9297807300974\n",
      "      - 274.9950048760384\n",
      "      - 264.12160790744605\n",
      "      - 275.0898076907855\n",
      "      - 267.00475472733734\n",
      "      - 251.32299217055734\n",
      "      - 268.49961347459276\n",
      "      - 270.4622855308296\n",
      "      - 268.48018015348265\n",
      "      - 259.96040633844854\n",
      "      - 265.8884153779914\n",
      "      - 248.68139336019314\n",
      "      - 243.6797137434567\n",
      "      - 270.19631841625016\n",
      "      - 265.4474304676307\n",
      "      - 265.63634300759696\n",
      "      - 247.19589511223785\n",
      "      - 253.4664421896468\n",
      "      - 261.4363100535846\n",
      "      - 267.1270293894712\n",
      "      - 266.9756167327895\n",
      "      - 265.6938646600373\n",
      "      - 267.01846915843123\n",
      "      - 264.56707004670625\n",
      "      - 269.1035563845996\n",
      "      - 252.98331646972602\n",
      "      - 265.4647415244119\n",
      "      - -128.95638703465153\n",
      "      - 259.32096081275444\n",
      "      - 267.64430155067424\n",
      "      - 259.89201888524343\n",
      "      - 252.28171242913658\n",
      "      - 269.8380978199554\n",
      "      - 261.16233252393545\n",
      "      - 263.53602614876\n",
      "      - 270.78829150071175\n",
      "      - 254.82638050234817\n",
      "      - 266.9358414459459\n",
      "      - 245.30621748628522\n",
      "      - 267.7682789286708\n",
      "      - 248.2000578657699\n",
      "      - 269.1066256889577\n",
      "      - 265.7807997703687\n",
      "      - 265.24152775553\n",
      "      - 264.8077304258565\n",
      "      - 259.69049834938653\n",
      "      - 267.92232555532433\n",
      "      - -65.48586956294503\n",
      "      - 268.36822474271185\n",
      "      - -39.3346776070576\n",
      "      - -28.63018289526549\n",
      "      - 268.30844124888586\n",
      "      - 267.40160133916794\n",
      "      - 262.77928262996096\n",
      "      - 252.7628736474049\n",
      "      - 248.73858407209153\n",
      "      - 268.42541579047713\n",
      "      - 267.3657701083741\n",
      "      - 263.5889301967241\n",
      "      - 257.7934569178187\n",
      "      - 273.6079174939157\n",
      "      - 270.9045163257304\n",
      "      - 266.591543763775\n",
      "      - 268.4418421635127\n",
      "      - 255.35405643882714\n",
      "      - 264.5176714799744\n",
      "      - 241.9182392814427\n",
      "      - -109.35420024603978\n",
      "      - 268.1423951401943\n",
      "      - 272.0030237095539\n",
      "      - 263.97055244683\n",
      "      - 260.5219322600252\n",
      "      - 267.71225054121345\n",
      "      - 269.47190898947923\n",
      "      - 267.40283893751626\n",
      "      - 258.5840950821775\n",
      "      - 272.74544309027925\n",
      "      - 270.85091749441114\n",
      "      - 268.5788309003371\n",
      "      - 265.3034991568958\n",
      "      - 269.1623020784922\n",
      "      - 265.6626058066588\n",
      "      - 267.8738434089546\n",
      "      - 271.1096258591473\n",
      "      - 269.0802426743802\n",
      "      - 268.1477163783376\n",
      "      - 268.35685746981005\n",
      "      - 266.93752470819095\n",
      "      - 40.484491928841976\n",
      "      - 252.71841845278178\n",
      "      - 267.6829011580422\n",
      "      - 262.60778510995607\n",
      "      - 258.34440507333284\n",
      "      - 261.1978933408144\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08583736748978169\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.3039167282483736\n",
      "      mean_inference_ms: 0.49460894154354895\n",
      "      mean_raw_obs_processing_ms: 0.05918962456883598\n",
      "    timesteps_this_iter: 150836\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4468013048171997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011357275769114494\n",
      "          model: {}\n",
      "          policy_loss: -0.03890091925859451\n",
      "          total_loss: 10.961398124694824\n",
      "          vf_explained_var: 0.47329986095428467\n",
      "          vf_loss: 10.985746383666992\n",
      "    num_agent_steps_sampled: 2000000\n",
      "    num_agent_steps_trained: 2000000\n",
      "    num_steps_sampled: 2000000\n",
      "    num_steps_trained: 2000000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.96153846153846\n",
      "    ram_util_percent: 30.23798076923077\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09073463171076065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3158707597737634\n",
      "    mean_inference_ms: 0.5262616738638277\n",
      "    mean_raw_obs_processing_ms: 0.07031120895825245\n",
      "  time_since_restore: 2703.793564558029\n",
      "  time_this_iter_s: 145.9011971950531\n",
      "  time_total_s: 2703.793564558029\n",
      "  timers:\n",
      "    learn_throughput: 2053.135\n",
      "    learn_time_ms: 1948.24\n",
      "    load_throughput: 29599887.085\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 996.439\n",
      "    sample_time_ms: 4014.294\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1671820266\n",
      "  timesteps_since_restore: 2000000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2000000\n",
      "  training_iteration: 500\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:08 (running for 00:45:22.11)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         2703.79</td><td style=\"text-align: right;\">2000000</td><td style=\"text-align: right;\"> 242.338</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1540.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2008000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 1523.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.56342755043363\n",
      "  episode_reward_mean: 239.6732259245048\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1444\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6448416709899902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01438465341925621\n",
      "          model: {}\n",
      "          policy_loss: -0.047195207327604294\n",
      "          total_loss: 4.534719944000244\n",
      "          vf_explained_var: 0.6552066206932068\n",
      "          vf_loss: 4.56348180770874\n",
      "    num_agent_steps_sampled: 2008000\n",
      "    num_agent_steps_trained: 2008000\n",
      "    num_steps_sampled: 2008000\n",
      "    num_steps_trained: 2008000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.86666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09073476189133188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31587659119543554\n",
      "    mean_inference_ms: 0.5262274238832524\n",
      "    mean_raw_obs_processing_ms: 0.07031237458836193\n",
      "  time_since_restore: 2711.610191345215\n",
      "  time_this_iter_s: 3.9090495109558105\n",
      "  time_total_s: 2711.610191345215\n",
      "  timers:\n",
      "    learn_throughput: 2051.98\n",
      "    learn_time_ms: 1949.337\n",
      "    load_throughput: 29547756.252\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 219.977\n",
      "    sample_time_ms: 18183.736\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1671820274\n",
      "  timesteps_since_restore: 2008000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2008000\n",
      "  training_iteration: 502\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:14 (running for 00:45:27.91)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         2711.61</td><td style=\"text-align: right;\">2008000</td><td style=\"text-align: right;\"> 239.673</td><td style=\"text-align: right;\">             270.563</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1523.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:20 (running for 00:45:33.91)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         2715.53</td><td style=\"text-align: right;\">2012000</td><td style=\"text-align: right;\"> 240.235</td><td style=\"text-align: right;\">             272.492</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1522.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2016000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 1522.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.4915382893133\n",
      "  episode_reward_mean: 240.1386778567998\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1449\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5913389325141907\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01208160538226366\n",
      "          model: {}\n",
      "          policy_loss: -0.04239170625805855\n",
      "          total_loss: 4.887250900268555\n",
      "          vf_explained_var: 0.7709038257598877\n",
      "          vf_loss: 4.914160251617432\n",
      "    num_agent_steps_sampled: 2016000\n",
      "    num_agent_steps_trained: 2016000\n",
      "    num_steps_sampled: 2016000\n",
      "    num_steps_trained: 2016000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.120000000000005\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907308842185934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3158360972047497\n",
      "    mean_inference_ms: 0.5262328893598124\n",
      "    mean_raw_obs_processing_ms: 0.07030514992336817\n",
      "  time_since_restore: 2719.437795162201\n",
      "  time_this_iter_s: 3.9029414653778076\n",
      "  time_total_s: 2719.437795162201\n",
      "  timers:\n",
      "    learn_throughput: 2051.373\n",
      "    learn_time_ms: 1949.914\n",
      "    load_throughput: 30710627.86\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 220.132\n",
      "    sample_time_ms: 18170.955\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671820282\n",
      "  timesteps_since_restore: 2016000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2016000\n",
      "  training_iteration: 504\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:26 (running for 00:45:39.72)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         2723.36</td><td style=\"text-align: right;\">2020000</td><td style=\"text-align: right;\"> 240.728</td><td style=\"text-align: right;\">             272.492</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1521.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2024000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 1521.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.4915382893133\n",
      "  episode_reward_mean: 240.98436125166944\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1454\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5076879262924194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011745073832571507\n",
      "          model: {}\n",
      "          policy_loss: -0.03668351471424103\n",
      "          total_loss: 6.420053958892822\n",
      "          vf_explained_var: 0.7157717347145081\n",
      "          vf_loss: 6.44168758392334\n",
      "    num_agent_steps_sampled: 2024000\n",
      "    num_agent_steps_trained: 2024000\n",
      "    num_steps_sampled: 2024000\n",
      "    num_steps_trained: 2024000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.419999999999995\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09073077781951118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31584304187806556\n",
      "    mean_inference_ms: 0.5261983665601526\n",
      "    mean_raw_obs_processing_ms: 0.07030623948799641\n",
      "  time_since_restore: 2727.267544746399\n",
      "  time_this_iter_s: 3.903846025466919\n",
      "  time_total_s: 2727.267544746399\n",
      "  timers:\n",
      "    learn_throughput: 2051.039\n",
      "    learn_time_ms: 1950.231\n",
      "    load_throughput: 32091078.806\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 220.21\n",
      "    sample_time_ms: 18164.489\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671820290\n",
      "  timesteps_since_restore: 2024000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2024000\n",
      "  training_iteration: 506\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:32 (running for 00:45:45.68)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         2727.27</td><td style=\"text-align: right;\">2024000</td><td style=\"text-align: right;\"> 240.984</td><td style=\"text-align: right;\">             272.492</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1521.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2032000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 1535.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.4915382893133\n",
      "  episode_reward_mean: 245.1237069137673\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1459\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5031458139419556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012767736800014973\n",
      "          model: {}\n",
      "          policy_loss: -0.04307451844215393\n",
      "          total_loss: 6.527493476867676\n",
      "          vf_explained_var: 0.6366647481918335\n",
      "          vf_loss: 6.554206371307373\n",
      "    num_agent_steps_sampled: 2032000\n",
      "    num_agent_steps_trained: 2032000\n",
      "    num_steps_sampled: 2032000\n",
      "    num_steps_trained: 2032000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.15\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09072612180092272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31579861640589457\n",
      "    mean_inference_ms: 0.526199749801117\n",
      "    mean_raw_obs_processing_ms: 0.07029811485529545\n",
      "  time_since_restore: 2735.0722041130066\n",
      "  time_this_iter_s: 3.9261038303375244\n",
      "  time_total_s: 2735.0722041130066\n",
      "  timers:\n",
      "    learn_throughput: 2049.843\n",
      "    learn_time_ms: 1951.369\n",
      "    load_throughput: 32042047.364\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 220.321\n",
      "    sample_time_ms: 18155.321\n",
      "    update_time_ms: 1.383\n",
      "  timestamp: 1671820298\n",
      "  timesteps_since_restore: 2032000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2032000\n",
      "  training_iteration: 508\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:38 (running for 00:45:51.49)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         2735.07</td><td style=\"text-align: right;\">2032000</td><td style=\"text-align: right;\"> 245.124</td><td style=\"text-align: right;\">             272.492</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1535.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:43 (running for 00:45:56.93)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">          2739.5</td><td style=\"text-align: right;\">2036000</td><td style=\"text-align: right;\"> 245.336</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1533.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-46\n",
      "  done: false\n",
      "  episode_len_mean: 1532.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.70240465328408\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1464\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.60101318359375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011015627533197403\n",
      "          model: {}\n",
      "          policy_loss: -0.03618525341153145\n",
      "          total_loss: 7.213654041290283\n",
      "          vf_explained_var: 0.6724618673324585\n",
      "          vf_loss: 7.235723972320557\n",
      "    num_agent_steps_sampled: 2040000\n",
      "    num_agent_steps_trained: 2040000\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.916666666666664\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09072639740790348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3158066274557867\n",
      "    mean_inference_ms: 0.5261671060659633\n",
      "    mean_raw_obs_processing_ms: 0.07029976510177931\n",
      "  time_since_restore: 2743.5302493572235\n",
      "  time_this_iter_s: 4.028048276901245\n",
      "  time_total_s: 2743.5302493572235\n",
      "  timers:\n",
      "    learn_throughput: 2055.81\n",
      "    learn_time_ms: 1945.705\n",
      "    load_throughput: 31720960.484\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 220.212\n",
      "    sample_time_ms: 18164.314\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1671820306\n",
      "  timesteps_since_restore: 2040000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 510\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:48 (running for 00:46:01.99)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         2743.53</td><td style=\"text-align: right;\">2040000</td><td style=\"text-align: right;\"> 245.702</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1532.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:53 (running for 00:46:06.99)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         2747.48</td><td style=\"text-align: right;\">2044000</td><td style=\"text-align: right;\"> 246.064</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1531.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2048000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-31-54\n",
      "  done: false\n",
      "  episode_len_mean: 1530.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 246.3574897998934\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1469\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.28559690713882446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010938596911728382\n",
      "          model: {}\n",
      "          policy_loss: -0.035323988646268845\n",
      "          total_loss: 4.73429536819458\n",
      "          vf_explained_var: 0.7169284224510193\n",
      "          vf_loss: 4.755601406097412\n",
      "    num_agent_steps_sampled: 2048000\n",
      "    num_agent_steps_trained: 2048000\n",
      "    num_steps_sampled: 2048000\n",
      "    num_steps_trained: 2048000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.50000000000001\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09072253461547714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3157662096846235\n",
      "    mean_inference_ms: 0.5261723058335632\n",
      "    mean_raw_obs_processing_ms: 0.07029238467548124\n",
      "  time_since_restore: 2751.460248708725\n",
      "  time_this_iter_s: 3.98140025138855\n",
      "  time_total_s: 2751.460248708725\n",
      "  timers:\n",
      "    learn_throughput: 2055.876\n",
      "    learn_time_ms: 1945.642\n",
      "    load_throughput: 31835324.478\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 998.864\n",
      "    sample_time_ms: 4004.548\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1671820314\n",
      "  timesteps_since_restore: 2048000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2048000\n",
      "  training_iteration: 512\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:31:59 (running for 00:46:12.93)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">         2755.41</td><td style=\"text-align: right;\">2052000</td><td style=\"text-align: right;\"> 246.529</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1529.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2056000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 1528.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 246.81918177178235\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1475\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5491740703582764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01570003479719162\n",
      "          model: {}\n",
      "          policy_loss: -0.04418882727622986\n",
      "          total_loss: 10.556838035583496\n",
      "          vf_explained_var: 0.5551080703735352\n",
      "          vf_loss: 10.580907821655273\n",
      "    num_agent_steps_sampled: 2056000\n",
      "    num_agent_steps_trained: 2056000\n",
      "    num_steps_sampled: 2056000\n",
      "    num_steps_trained: 2056000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.55\n",
      "    ram_util_percent: 30.216666666666665\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907204149761433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31574845752132724\n",
      "    mean_inference_ms: 0.5261562634625682\n",
      "    mean_raw_obs_processing_ms: 0.0702892297443241\n",
      "  time_since_restore: 2759.465124130249\n",
      "  time_this_iter_s: 4.050835132598877\n",
      "  time_total_s: 2759.465124130249\n",
      "  timers:\n",
      "    learn_throughput: 2054.207\n",
      "    learn_time_ms: 1947.223\n",
      "    load_throughput: 32526591.702\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 994.96\n",
      "    sample_time_ms: 4020.263\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1671820322\n",
      "  timesteps_since_restore: 2056000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2056000\n",
      "  training_iteration: 514\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:04 (running for 00:46:18.01)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         2759.47</td><td style=\"text-align: right;\">2056000</td><td style=\"text-align: right;\"> 246.819</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1528.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2064000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 1524.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.6910854351809\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4249041676521301\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010872066020965576\n",
      "          model: {}\n",
      "          policy_loss: -0.04118189588189125\n",
      "          total_loss: 18.359195709228516\n",
      "          vf_explained_var: 0.5974103808403015\n",
      "          vf_loss: 18.386445999145508\n",
      "    num_agent_steps_sampled: 2064000\n",
      "    num_agent_steps_trained: 2064000\n",
      "    num_steps_sampled: 2064000\n",
      "    num_steps_trained: 2064000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09071834666413006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3157329783867613\n",
      "    mean_inference_ms: 0.5261414276135323\n",
      "    mean_raw_obs_processing_ms: 0.07028647045466034\n",
      "  time_since_restore: 2767.2701330184937\n",
      "  time_this_iter_s: 3.921231746673584\n",
      "  time_total_s: 2767.2701330184937\n",
      "  timers:\n",
      "    learn_throughput: 2053.529\n",
      "    learn_time_ms: 1947.867\n",
      "    load_throughput: 31277434.75\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 995.145\n",
      "    sample_time_ms: 4019.514\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1671820330\n",
      "  timesteps_since_restore: 2064000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2064000\n",
      "  training_iteration: 516\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:10 (running for 00:46:23.85)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">         2767.27</td><td style=\"text-align: right;\">2064000</td><td style=\"text-align: right;\"> 245.691</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1524.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:16 (running for 00:46:29.80)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">          2771.2</td><td style=\"text-align: right;\">2068000</td><td style=\"text-align: right;\"> 245.768</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1524.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2072000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-18\n",
      "  done: false\n",
      "  episode_len_mean: 1524.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.75784521642316\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1485\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3305768370628357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012132475152611732\n",
      "          model: {}\n",
      "          policy_loss: -0.038220979273319244\n",
      "          total_loss: 11.86292839050293\n",
      "          vf_explained_var: 0.5585455894470215\n",
      "          vf_loss: 11.885602951049805\n",
      "    num_agent_steps_sampled: 2072000\n",
      "    num_agent_steps_trained: 2072000\n",
      "    num_steps_sampled: 2072000\n",
      "    num_steps_trained: 2072000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09071604034612248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3157170651863302\n",
      "    mean_inference_ms: 0.5261254751042583\n",
      "    mean_raw_obs_processing_ms: 0.07028340167916106\n",
      "  time_since_restore: 2775.1353664398193\n",
      "  time_this_iter_s: 3.932464838027954\n",
      "  time_total_s: 2775.1353664398193\n",
      "  timers:\n",
      "    learn_throughput: 2053.547\n",
      "    learn_time_ms: 1947.85\n",
      "    load_throughput: 29873960.114\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 993.18\n",
      "    sample_time_ms: 4027.465\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671820338\n",
      "  timesteps_since_restore: 2072000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2072000\n",
      "  training_iteration: 518\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:22 (running for 00:46:35.73)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         2779.09</td><td style=\"text-align: right;\">2076000</td><td style=\"text-align: right;\"> 246.106</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1522.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 1508.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 242.63580564756856\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1491\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3322851061820984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019113440066576004\n",
      "          model: {}\n",
      "          policy_loss: -0.02992910146713257\n",
      "          total_loss: 42.19788360595703\n",
      "          vf_explained_var: 0.5422886610031128\n",
      "          vf_loss: 42.20331954956055\n",
      "    num_agent_steps_sampled: 2080000\n",
      "    num_agent_steps_trained: 2080000\n",
      "    num_steps_sampled: 2080000\n",
      "    num_steps_trained: 2080000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09071299196723827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3156969371668011\n",
      "    mean_inference_ms: 0.526105043061802\n",
      "    mean_raw_obs_processing_ms: 0.07027965141968058\n",
      "  time_since_restore: 2783.0067546367645\n",
      "  time_this_iter_s: 3.9189412593841553\n",
      "  time_total_s: 2783.0067546367645\n",
      "  timers:\n",
      "    learn_throughput: 2072.128\n",
      "    learn_time_ms: 1930.383\n",
      "    load_throughput: 31225043.737\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1007.959\n",
      "    sample_time_ms: 3968.415\n",
      "    update_time_ms: 1.409\n",
      "  timestamp: 1671820346\n",
      "  timesteps_since_restore: 2080000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2080000\n",
      "  training_iteration: 520\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:28 (running for 00:46:41.67)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         2783.01</td><td style=\"text-align: right;\">2080000</td><td style=\"text-align: right;\"> 242.636</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1508.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:33 (running for 00:46:46.68)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">            2787</td><td style=\"text-align: right;\">2084000</td><td style=\"text-align: right;\"> 242.649</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1508.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2088000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 1507.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 242.83632601649344\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1496\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4463486671447754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012170586735010147\n",
      "          model: {}\n",
      "          policy_loss: -0.04238118976354599\n",
      "          total_loss: 9.347101211547852\n",
      "          vf_explained_var: 0.6654196977615356\n",
      "          vf_loss: 9.373886108398438\n",
      "    num_agent_steps_sampled: 2088000\n",
      "    num_agent_steps_trained: 2088000\n",
      "    num_steps_sampled: 2088000\n",
      "    num_steps_trained: 2088000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.083333333333336\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070844188349665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31565548884902483\n",
      "    mean_inference_ms: 0.5261081930051129\n",
      "    mean_raw_obs_processing_ms: 0.07027207080841444\n",
      "  time_since_restore: 2790.950057029724\n",
      "  time_this_iter_s: 3.954505443572998\n",
      "  time_total_s: 2790.950057029724\n",
      "  timers:\n",
      "    learn_throughput: 2072.812\n",
      "    learn_time_ms: 1929.746\n",
      "    load_throughput: 31236671.011\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1007.455\n",
      "    sample_time_ms: 3970.401\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1671820354\n",
      "  timesteps_since_restore: 2088000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2088000\n",
      "  training_iteration: 522\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:39 (running for 00:46:52.61)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         2794.89</td><td style=\"text-align: right;\">2092000</td><td style=\"text-align: right;\"> 245.693</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1517.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2096000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 1518.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.6112045961749\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1501\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5073626041412354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011273679323494434\n",
      "          model: {}\n",
      "          policy_loss: -0.03894861787557602\n",
      "          total_loss: 7.44150972366333\n",
      "          vf_explained_var: 0.5746992826461792\n",
      "          vf_loss: 7.4660115242004395\n",
      "    num_agent_steps_sampled: 2096000\n",
      "    num_agent_steps_trained: 2096000\n",
      "    num_steps_sampled: 2096000\n",
      "    num_steps_trained: 2096000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.0\n",
      "    ram_util_percent: 30.21428571428571\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907062994666407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31564023644013245\n",
      "    mean_inference_ms: 0.5260947117008219\n",
      "    mean_raw_obs_processing_ms: 0.07026945742291638\n",
      "  time_since_restore: 2799.2734937667847\n",
      "  time_this_iter_s: 4.3867528438568115\n",
      "  time_total_s: 2799.2734937667847\n",
      "  timers:\n",
      "    learn_throughput: 2057.72\n",
      "    learn_time_ms: 1943.899\n",
      "    load_throughput: 28315976.371\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 1003.234\n",
      "    sample_time_ms: 3987.106\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671820362\n",
      "  timesteps_since_restore: 2096000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2096000\n",
      "  training_iteration: 524\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:44 (running for 00:46:58.02)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         2799.27</td><td style=\"text-align: right;\">2096000</td><td style=\"text-align: right;\"> 245.611</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1518.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:49 (running for 00:47:03.12)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         2803.36</td><td style=\"text-align: right;\">2100000</td><td style=\"text-align: right;\"> 245.833</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1517.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-50\n",
      "  done: false\n",
      "  episode_len_mean: 1524.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 247.70181176290345\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1506\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4677038788795471\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014629926532506943\n",
      "          model: {}\n",
      "          policy_loss: -0.040632523596286774\n",
      "          total_loss: 10.465893745422363\n",
      "          vf_explained_var: 0.5472458600997925\n",
      "          vf_loss: 10.48777961730957\n",
      "    num_agent_steps_sampled: 2104000\n",
      "    num_agent_steps_trained: 2104000\n",
      "    num_steps_sampled: 2104000\n",
      "    num_steps_trained: 2104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.040000000000006\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0907054374912324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31562917548942976\n",
      "    mean_inference_ms: 0.5260871364981569\n",
      "    mean_raw_obs_processing_ms: 0.07026771415621778\n",
      "  time_since_restore: 2807.316358566284\n",
      "  time_this_iter_s: 3.9556689262390137\n",
      "  time_total_s: 2807.316358566284\n",
      "  timers:\n",
      "    learn_throughput: 2055.852\n",
      "    learn_time_ms: 1945.665\n",
      "    load_throughput: 29526955.297\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 994.105\n",
      "    sample_time_ms: 4023.719\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1671820370\n",
      "  timesteps_since_restore: 2104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2104000\n",
      "  training_iteration: 526\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:32:55 (running for 00:47:09.14)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">          2811.3</td><td style=\"text-align: right;\">2108000</td><td style=\"text-align: right;\"> 247.949</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1524.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-32-58\n",
      "  done: false\n",
      "  episode_len_mean: 1523.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 247.89701471741836\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1511\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.20922306180000305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01291588880121708\n",
      "          model: {}\n",
      "          policy_loss: -0.042073071002960205\n",
      "          total_loss: 7.101780891418457\n",
      "          vf_explained_var: 0.5945934653282166\n",
      "          vf_loss: 7.127302646636963\n",
      "    num_agent_steps_sampled: 2112000\n",
      "    num_agent_steps_trained: 2112000\n",
      "    num_steps_sampled: 2112000\n",
      "    num_steps_trained: 2112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.949999999999996\n",
      "    ram_util_percent: 30.28333333333333\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070683340275339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31564464813969195\n",
      "    mean_inference_ms: 0.526062675110348\n",
      "    mean_raw_obs_processing_ms: 0.07027084215291415\n",
      "  time_since_restore: 2815.2448596954346\n",
      "  time_this_iter_s: 3.9426522254943848\n",
      "  time_total_s: 2815.2448596954346\n",
      "  timers:\n",
      "    learn_throughput: 2052.572\n",
      "    learn_time_ms: 1948.775\n",
      "    load_throughput: 29970017.864\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 993.239\n",
      "    sample_time_ms: 4027.228\n",
      "    update_time_ms: 1.343\n",
      "  timestamp: 1671820378\n",
      "  timesteps_since_restore: 2112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2112000\n",
      "  training_iteration: 528\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:01 (running for 00:47:15.06)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         2815.24</td><td style=\"text-align: right;\">2112000</td><td style=\"text-align: right;\"> 247.897</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1523.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-06\n",
      "  done: false\n",
      "  episode_len_mean: 1509.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.60260454577582\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1517\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.13311220705509186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011317422613501549\n",
      "          model: {}\n",
      "          policy_loss: -0.041802797466516495\n",
      "          total_loss: 5.739504814147949\n",
      "          vf_explained_var: 0.5548152327537537\n",
      "          vf_loss: 5.766804218292236\n",
      "    num_agent_steps_sampled: 2120000\n",
      "    num_agent_steps_trained: 2120000\n",
      "    num_steps_sampled: 2120000\n",
      "    num_steps_trained: 2120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.23333333333334\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070623726076343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31563410668977954\n",
      "    mean_inference_ms: 0.5260566291804888\n",
      "    mean_raw_obs_processing_ms: 0.07026954820526375\n",
      "  time_since_restore: 2823.13125872612\n",
      "  time_this_iter_s: 3.9645445346832275\n",
      "  time_total_s: 2823.13125872612\n",
      "  timers:\n",
      "    learn_throughput: 2052.818\n",
      "    learn_time_ms: 1948.541\n",
      "    load_throughput: 29254081.953\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 992.115\n",
      "    sample_time_ms: 4031.79\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1671820386\n",
      "  timesteps_since_restore: 2120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2120000\n",
      "  training_iteration: 530\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:07 (running for 00:47:21.03)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         2823.13</td><td style=\"text-align: right;\">2120000</td><td style=\"text-align: right;\"> 245.603</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1509.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:13 (running for 00:47:26.93)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         2827.06</td><td style=\"text-align: right;\">2124000</td><td style=\"text-align: right;\"> 245.958</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1510.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-14\n",
      "  done: false\n",
      "  episode_len_mean: 1510.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.98333087143357\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1522\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.36257216334342957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012851900421082973\n",
      "          model: {}\n",
      "          policy_loss: -0.044145893305540085\n",
      "          total_loss: 6.075835704803467\n",
      "          vf_explained_var: 0.7088767290115356\n",
      "          vf_loss: 6.103512287139893\n",
      "    num_agent_steps_sampled: 2128000\n",
      "    num_agent_steps_trained: 2128000\n",
      "    num_steps_sampled: 2128000\n",
      "    num_steps_trained: 2128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.06666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070543347196011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.315623809587518\n",
      "    mean_inference_ms: 0.5260504087986084\n",
      "    mean_raw_obs_processing_ms: 0.07026829407877692\n",
      "  time_since_restore: 2830.955823659897\n",
      "  time_this_iter_s: 3.8974926471710205\n",
      "  time_total_s: 2830.955823659897\n",
      "  timers:\n",
      "    learn_throughput: 2052.909\n",
      "    learn_time_ms: 1948.455\n",
      "    load_throughput: 29162551.712\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 995.275\n",
      "    sample_time_ms: 4018.988\n",
      "    update_time_ms: 1.344\n",
      "  timestamp: 1671820394\n",
      "  timesteps_since_restore: 2128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2128000\n",
      "  training_iteration: 532\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:19 (running for 00:47:32.79)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         2834.88</td><td style=\"text-align: right;\">2132000</td><td style=\"text-align: right;\"> 245.884</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">           1510.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-22\n",
      "  done: false\n",
      "  episode_len_mean: 1509.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.799639033936\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1527\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5034754872322083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011857641860842705\n",
      "          model: {}\n",
      "          policy_loss: -0.04157682880759239\n",
      "          total_loss: 6.157357692718506\n",
      "          vf_explained_var: 0.7112464904785156\n",
      "          vf_loss: 6.18373966217041\n",
      "    num_agent_steps_sampled: 2136000\n",
      "    num_agent_steps_trained: 2136000\n",
      "    num_steps_sampled: 2136000\n",
      "    num_steps_trained: 2136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.6\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070433580439097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3156110047199138\n",
      "    mean_inference_ms: 0.5260418072168137\n",
      "    mean_raw_obs_processing_ms: 0.0702666776252041\n",
      "  time_since_restore: 2838.808718919754\n",
      "  time_this_iter_s: 3.93165922164917\n",
      "  time_total_s: 2838.808718919754\n",
      "  timers:\n",
      "    learn_throughput: 2068.23\n",
      "    learn_time_ms: 1934.021\n",
      "    load_throughput: 31312459.873\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1002.862\n",
      "    sample_time_ms: 3988.585\n",
      "    update_time_ms: 1.359\n",
      "  timestamp: 1671820402\n",
      "  timesteps_since_restore: 2136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2136000\n",
      "  training_iteration: 534\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:25 (running for 00:47:38.74)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         2838.81</td><td style=\"text-align: right;\">2136000</td><td style=\"text-align: right;\">   245.8</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1509.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 1509.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 245.8908013970697\n",
      "  episode_reward_min: -130.41757306685545\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1532\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.33801257610321045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011809982359409332\n",
      "          model: {}\n",
      "          policy_loss: -0.04205784201622009\n",
      "          total_loss: 7.123560428619385\n",
      "          vf_explained_var: 0.6172761917114258\n",
      "          vf_loss: 7.150484085083008\n",
      "    num_agent_steps_sampled: 2144000\n",
      "    num_agent_steps_trained: 2144000\n",
      "    num_steps_sampled: 2144000\n",
      "    num_steps_trained: 2144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070318760730667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31559865005453286\n",
      "    mean_inference_ms: 0.5260336362951492\n",
      "    mean_raw_obs_processing_ms: 0.07026512866590545\n",
      "  time_since_restore: 2846.730208873749\n",
      "  time_this_iter_s: 3.96695876121521\n",
      "  time_total_s: 2846.730208873749\n",
      "  timers:\n",
      "    learn_throughput: 2070.863\n",
      "    learn_time_ms: 1931.562\n",
      "    load_throughput: 31726959.153\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 1009.427\n",
      "    sample_time_ms: 3962.645\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1671820410\n",
      "  timesteps_since_restore: 2144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2144000\n",
      "  training_iteration: 536\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:31 (running for 00:47:44.70)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         2846.73</td><td style=\"text-align: right;\">2144000</td><td style=\"text-align: right;\"> 245.891</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -130.418</td><td style=\"text-align: right;\">            1509.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:37 (running for 00:47:50.61)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         2850.62</td><td style=\"text-align: right;\">2148000</td><td style=\"text-align: right;\"> 252.139</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -110.209</td><td style=\"text-align: right;\">           1528.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-38\n",
      "  done: false\n",
      "  episode_len_mean: 1528.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 252.12085895854565\n",
      "  episode_reward_min: -110.20889201186225\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1537\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.40942221879959106\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013644970953464508\n",
      "          model: {}\n",
      "          policy_loss: -0.03954961895942688\n",
      "          total_loss: 6.082894802093506\n",
      "          vf_explained_var: 0.6411556005477905\n",
      "          vf_loss: 6.104959487915039\n",
      "    num_agent_steps_sampled: 2152000\n",
      "    num_agent_steps_trained: 2152000\n",
      "    num_steps_sampled: 2152000\n",
      "    num_steps_trained: 2152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.01666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09070135373361458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.315582924008743\n",
      "    mean_inference_ms: 0.5260200042452743\n",
      "    mean_raw_obs_processing_ms: 0.07026264349996249\n",
      "  time_since_restore: 2854.546984910965\n",
      "  time_this_iter_s: 3.9238345623016357\n",
      "  time_total_s: 2854.546984910965\n",
      "  timers:\n",
      "    learn_throughput: 2072.709\n",
      "    learn_time_ms: 1929.841\n",
      "    load_throughput: 32793620.016\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 1012.093\n",
      "    sample_time_ms: 3952.205\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671820418\n",
      "  timesteps_since_restore: 2152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2152000\n",
      "  training_iteration: 538\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:42 (running for 00:47:56.00)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         2858.97</td><td style=\"text-align: right;\">2156000</td><td style=\"text-align: right;\"> 251.993</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -110.209</td><td style=\"text-align: right;\">           1528.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 1543.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 255.5925002129443\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1543\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4556455612182617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011854630894958973\n",
      "          model: {}\n",
      "          policy_loss: -0.044351592659950256\n",
      "          total_loss: 11.045762062072754\n",
      "          vf_explained_var: 0.5768114328384399\n",
      "          vf_loss: 11.074922561645508\n",
      "    num_agent_steps_sampled: 2160000\n",
      "    num_agent_steps_trained: 2160000\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.53333333333333\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0906979429350415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31554440642013054\n",
      "    mean_inference_ms: 0.5260285195588525\n",
      "    mean_raw_obs_processing_ms: 0.07025643491463769\n",
      "  time_since_restore: 2863.0903248786926\n",
      "  time_this_iter_s: 4.120253324508667\n",
      "  time_total_s: 2863.0903248786926\n",
      "  timers:\n",
      "    learn_throughput: 2053.806\n",
      "    learn_time_ms: 1947.603\n",
      "    load_throughput: 34155570.033\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 996.157\n",
      "    sample_time_ms: 4015.43\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1671820426\n",
      "  timesteps_since_restore: 2160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 540\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:47 (running for 00:48:01.14)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         2863.09</td><td style=\"text-align: right;\">2160000</td><td style=\"text-align: right;\"> 255.593</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1543.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:53 (running for 00:48:07.09)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         2867.03</td><td style=\"text-align: right;\">2164000</td><td style=\"text-align: right;\"> 255.851</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1543.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-33-54\n",
      "  done: false\n",
      "  episode_len_mean: 1544.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 255.76004732549623\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1547\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3381553292274475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010088120587170124\n",
      "          model: {}\n",
      "          policy_loss: -0.033771056681871414\n",
      "          total_loss: 6.693124294281006\n",
      "          vf_explained_var: 0.6816941499710083\n",
      "          vf_loss: 6.713967800140381\n",
      "    num_agent_steps_sampled: 2168000\n",
      "    num_agent_steps_trained: 2168000\n",
      "    num_steps_sampled: 2168000\n",
      "    num_steps_trained: 2168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.0\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069733925502892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31553652224209794\n",
      "    mean_inference_ms: 0.5260234323940055\n",
      "    mean_raw_obs_processing_ms: 0.07025542617430763\n",
      "  time_since_restore: 2870.9785566329956\n",
      "  time_this_iter_s: 3.952535390853882\n",
      "  time_total_s: 2870.9785566329956\n",
      "  timers:\n",
      "    learn_throughput: 2052.347\n",
      "    learn_time_ms: 1948.988\n",
      "    load_throughput: 34197342.03\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 994.4\n",
      "    sample_time_ms: 4022.528\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671820434\n",
      "  timesteps_since_restore: 2168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2168000\n",
      "  training_iteration: 542\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:33:59 (running for 00:48:13.06)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         2874.95</td><td style=\"text-align: right;\">2172000</td><td style=\"text-align: right;\">  255.88</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1543.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-02\n",
      "  done: false\n",
      "  episode_len_mean: 1542.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 256.2200034779516\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1553\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5834650993347168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01206531934440136\n",
      "          model: {}\n",
      "          policy_loss: -0.04355129599571228\n",
      "          total_loss: 9.159843444824219\n",
      "          vf_explained_var: 0.6579323410987854\n",
      "          vf_loss: 9.187934875488281\n",
      "    num_agent_steps_sampled: 2176000\n",
      "    num_agent_steps_trained: 2176000\n",
      "    num_steps_sampled: 2176000\n",
      "    num_steps_trained: 2176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.4\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069685043280307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3155268988772927\n",
      "    mean_inference_ms: 0.5260183265668604\n",
      "    mean_raw_obs_processing_ms: 0.07025441462082303\n",
      "  time_since_restore: 2878.8940618038177\n",
      "  time_this_iter_s: 3.9433834552764893\n",
      "  time_total_s: 2878.8940618038177\n",
      "  timers:\n",
      "    learn_throughput: 2052.461\n",
      "    learn_time_ms: 1948.88\n",
      "    load_throughput: 33000031.471\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 993.059\n",
      "    sample_time_ms: 4027.958\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671820442\n",
      "  timesteps_since_restore: 2176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2176000\n",
      "  training_iteration: 544\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:05 (running for 00:48:19.02)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">         2878.89</td><td style=\"text-align: right;\">2176000</td><td style=\"text-align: right;\">  256.22</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1542.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 1544.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.3946313765815\n",
      "  episode_reward_mean: 255.81091539462966\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1558\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.33774834871292114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012485831044614315\n",
      "          model: {}\n",
      "          policy_loss: -0.04641279950737953\n",
      "          total_loss: 9.755097389221191\n",
      "          vf_explained_var: 0.5986400246620178\n",
      "          vf_loss: 9.785511016845703\n",
      "    num_agent_steps_sampled: 2184000\n",
      "    num_agent_steps_trained: 2184000\n",
      "    num_steps_sampled: 2184000\n",
      "    num_steps_trained: 2184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.46666666666667\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069879804486115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3155446798083884\n",
      "    mean_inference_ms: 0.5259965895756589\n",
      "    mean_raw_obs_processing_ms: 0.07025814790676727\n",
      "  time_since_restore: 2886.765580177307\n",
      "  time_this_iter_s: 3.9659318923950195\n",
      "  time_total_s: 2886.765580177307\n",
      "  timers:\n",
      "    learn_throughput: 2052.209\n",
      "    learn_time_ms: 1949.119\n",
      "    load_throughput: 31063166.08\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 994.55\n",
      "    sample_time_ms: 4021.919\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671820450\n",
      "  timesteps_since_restore: 2184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2184000\n",
      "  training_iteration: 546\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:11 (running for 00:48:24.97)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         2886.77</td><td style=\"text-align: right;\">2184000</td><td style=\"text-align: right;\"> 255.811</td><td style=\"text-align: right;\">             273.395</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1544.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:17 (running for 00:48:30.85)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         2890.67</td><td style=\"text-align: right;\">2188000</td><td style=\"text-align: right;\"> 255.854</td><td style=\"text-align: right;\">             272.713</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1544.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-18\n",
      "  done: false\n",
      "  episode_len_mean: 1545.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.6739399987399\n",
      "  episode_reward_mean: 255.77049585539203\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1563\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.48700854182243347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01388530619442463\n",
      "          model: {}\n",
      "          policy_loss: -0.043463483452796936\n",
      "          total_loss: 8.739420890808105\n",
      "          vf_explained_var: 0.6144900321960449\n",
      "          vf_loss: 8.765091896057129\n",
      "    num_agent_steps_sampled: 2192000\n",
      "    num_agent_steps_trained: 2192000\n",
      "    num_steps_sampled: 2192000\n",
      "    num_steps_trained: 2192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.49999999999999\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069574425643397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31550931850557795\n",
      "    mean_inference_ms: 0.526006530585192\n",
      "    mean_raw_obs_processing_ms: 0.07025169682949466\n",
      "  time_since_restore: 2894.5737493038177\n",
      "  time_this_iter_s: 3.9043238162994385\n",
      "  time_total_s: 2894.5737493038177\n",
      "  timers:\n",
      "    learn_throughput: 2054.72\n",
      "    learn_time_ms: 1946.737\n",
      "    load_throughput: 31057415.772\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 994.272\n",
      "    sample_time_ms: 4023.044\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1671820458\n",
      "  timesteps_since_restore: 2192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2192000\n",
      "  training_iteration: 548\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:23 (running for 00:48:36.81)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         2898.55</td><td style=\"text-align: right;\">2196000</td><td style=\"text-align: right;\"> 255.794</td><td style=\"text-align: right;\">             272.674</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">            1544.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-26\n",
      "  done: false\n",
      "  episode_len_mean: 1545.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.6739399987399\n",
      "  episode_reward_mean: 255.66356888228268\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1568\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.35172298550605774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012027161195874214\n",
      "          model: {}\n",
      "          policy_loss: -0.04129659757018089\n",
      "          total_loss: 5.21282434463501\n",
      "          vf_explained_var: 0.6530688405036926\n",
      "          vf_loss: 5.23870849609375\n",
      "    num_agent_steps_sampled: 2200000\n",
      "    num_agent_steps_trained: 2200000\n",
      "    num_steps_sampled: 2200000\n",
      "    num_steps_trained: 2200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.88333333333333\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069627466363743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31552145293256467\n",
      "    mean_inference_ms: 0.5259757597213907\n",
      "    mean_raw_obs_processing_ms: 0.07025377274518937\n",
      "  time_since_restore: 2902.51242852211\n",
      "  time_this_iter_s: 3.9608633518218994\n",
      "  time_total_s: 2902.51242852211\n",
      "  timers:\n",
      "    learn_throughput: 2073.836\n",
      "    learn_time_ms: 1928.793\n",
      "    load_throughput: 30693772.411\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1009.751\n",
      "    sample_time_ms: 3961.374\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1671820466\n",
      "  timesteps_since_restore: 2200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2200000\n",
      "  training_iteration: 550\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:29 (running for 00:48:42.75)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         2902.51</td><td style=\"text-align: right;\">2200000</td><td style=\"text-align: right;\"> 255.664</td><td style=\"text-align: right;\">             272.674</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1545.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-34\n",
      "  done: false\n",
      "  episode_len_mean: 1545.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.6739399987399\n",
      "  episode_reward_mean: 255.69412615123292\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1573\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4653387665748596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01209268718957901\n",
      "          model: {}\n",
      "          policy_loss: -0.037984803318977356\n",
      "          total_loss: 6.312283515930176\n",
      "          vf_explained_var: 0.7403766512870789\n",
      "          vf_loss: 6.334772109985352\n",
      "    num_agent_steps_sampled: 2208000\n",
      "    num_agent_steps_trained: 2208000\n",
      "    num_steps_sampled: 2208000\n",
      "    num_steps_trained: 2208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.68333333333334\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069267258025487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3154833222014118\n",
      "    mean_inference_ms: 0.5259819445434413\n",
      "    mean_raw_obs_processing_ms: 0.0702464589360035\n",
      "  time_since_restore: 2910.3652477264404\n",
      "  time_this_iter_s: 3.916476249694824\n",
      "  time_total_s: 2910.3652477264404\n",
      "  timers:\n",
      "    learn_throughput: 2075.685\n",
      "    learn_time_ms: 1927.075\n",
      "    load_throughput: 28460078.032\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 1010.894\n",
      "    sample_time_ms: 3956.893\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671820474\n",
      "  timesteps_since_restore: 2208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2208000\n",
      "  training_iteration: 552\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:35 (running for 00:48:48.64)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         2910.37</td><td style=\"text-align: right;\">2208000</td><td style=\"text-align: right;\"> 255.694</td><td style=\"text-align: right;\">             272.674</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1545.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:41 (running for 00:48:54.60)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         2914.31</td><td style=\"text-align: right;\">2212000</td><td style=\"text-align: right;\"> 255.672</td><td style=\"text-align: right;\">             272.674</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">            1545.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 1545.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.6739399987399\n",
      "  episode_reward_mean: 255.45883217280553\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1578\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5194047093391418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018765028566122055\n",
      "          model: {}\n",
      "          policy_loss: -0.04245607927441597\n",
      "          total_loss: 9.051204681396484\n",
      "          vf_explained_var: 0.65180504322052\n",
      "          vf_loss: 9.06961441040039\n",
      "    num_agent_steps_sampled: 2216000\n",
      "    num_agent_steps_trained: 2216000\n",
      "    num_steps_sampled: 2216000\n",
      "    num_steps_trained: 2216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.85\n",
      "    ram_util_percent: 30.283333333333335\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069351596053064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31549523701050086\n",
      "    mean_inference_ms: 0.5259523918594411\n",
      "    mean_raw_obs_processing_ms: 0.0702486608433812\n",
      "  time_since_restore: 2918.745368719101\n",
      "  time_this_iter_s: 4.43543815612793\n",
      "  time_total_s: 2918.745368719101\n",
      "  timers:\n",
      "    learn_throughput: 2055.366\n",
      "    learn_time_ms: 1946.125\n",
      "    load_throughput: 27418231.737\n",
      "    load_time_ms: 0.146\n",
      "    sample_throughput: 1003.621\n",
      "    sample_time_ms: 3985.567\n",
      "    update_time_ms: 1.408\n",
      "  timestamp: 1671820482\n",
      "  timesteps_since_restore: 2216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2216000\n",
      "  training_iteration: 554\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:46 (running for 00:49:00.10)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         2918.75</td><td style=\"text-align: right;\">2216000</td><td style=\"text-align: right;\"> 255.459</td><td style=\"text-align: right;\">             272.674</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1545.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 1546.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 257.16822264529293\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1583\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4419143497943878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012964313849806786\n",
      "          model: {}\n",
      "          policy_loss: -0.041031770408153534\n",
      "          total_loss: 7.203097820281982\n",
      "          vf_explained_var: 0.6592997908592224\n",
      "          vf_loss: 7.2275166511535645\n",
      "    num_agent_steps_sampled: 2224000\n",
      "    num_agent_steps_trained: 2224000\n",
      "    num_steps_sampled: 2224000\n",
      "    num_steps_trained: 2224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.21666666666667\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069155249035349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31546284640824135\n",
      "    mean_inference_ms: 0.5259683004722063\n",
      "    mean_raw_obs_processing_ms: 0.070243008465454\n",
      "  time_since_restore: 2926.8694133758545\n",
      "  time_this_iter_s: 3.9853315353393555\n",
      "  time_total_s: 2926.8694133758545\n",
      "  timers:\n",
      "    learn_throughput: 2045.234\n",
      "    learn_time_ms: 1955.766\n",
      "    load_throughput: 28966187.845\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 994.661\n",
      "    sample_time_ms: 4021.469\n",
      "    update_time_ms: 1.412\n",
      "  timestamp: 1671820490\n",
      "  timesteps_since_restore: 2224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2224000\n",
      "  training_iteration: 556\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:51 (running for 00:49:05.23)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         2926.87</td><td style=\"text-align: right;\">2224000</td><td style=\"text-align: right;\"> 257.168</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1546.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:34:57 (running for 00:49:11.16)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">         2930.78</td><td style=\"text-align: right;\">2228000</td><td style=\"text-align: right;\"> 257.088</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -92.1828</td><td style=\"text-align: right;\">           1547.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 1547.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 257.0765793421214\n",
      "  episode_reward_min: -92.18284308271917\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1589\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4810073673725128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012646308168768883\n",
      "          model: {}\n",
      "          policy_loss: -0.04333972558379173\n",
      "          total_loss: 9.382917404174805\n",
      "          vf_explained_var: 0.6341535449028015\n",
      "          vf_loss: 9.410051345825195\n",
      "    num_agent_steps_sampled: 2232000\n",
      "    num_agent_steps_trained: 2232000\n",
      "    num_steps_sampled: 2232000\n",
      "    num_steps_trained: 2232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069142364549672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31545183602685567\n",
      "    mean_inference_ms: 0.5259631026880839\n",
      "    mean_raw_obs_processing_ms: 0.07024169567958181\n",
      "  time_since_restore: 2934.688641309738\n",
      "  time_this_iter_s: 3.910466194152832\n",
      "  time_total_s: 2934.688641309738\n",
      "  timers:\n",
      "    learn_throughput: 2044.404\n",
      "    learn_time_ms: 1956.56\n",
      "    load_throughput: 28951192.407\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 992.145\n",
      "    sample_time_ms: 4031.668\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1671820498\n",
      "  timesteps_since_restore: 2232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2232000\n",
      "  training_iteration: 558\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:03 (running for 00:49:17.05)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         2938.63</td><td style=\"text-align: right;\">2236000</td><td style=\"text-align: right;\"> 260.725</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1560.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 1559.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 260.81372862819467\n",
      "  episode_reward_min: -61.37939871022665\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1593\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.48130539059638977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012081927619874477\n",
      "          model: {}\n",
      "          policy_loss: -0.04172438383102417\n",
      "          total_loss: 6.07268762588501\n",
      "          vf_explained_var: 0.7521888613700867\n",
      "          vf_loss: 6.0989298820495605\n",
      "    num_agent_steps_sampled: 2240000\n",
      "    num_agent_steps_trained: 2240000\n",
      "    num_steps_sampled: 2240000\n",
      "    num_steps_trained: 2240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.75\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069122118092411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3154439702461941\n",
      "    mean_inference_ms: 0.5259587090710088\n",
      "    mean_raw_obs_processing_ms: 0.0702406660877041\n",
      "  time_since_restore: 2942.53648853302\n",
      "  time_this_iter_s: 3.908947706222534\n",
      "  time_total_s: 2942.53648853302\n",
      "  timers:\n",
      "    learn_throughput: 2044.775\n",
      "    learn_time_ms: 1956.206\n",
      "    load_throughput: 29490624.011\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 994.46\n",
      "    sample_time_ms: 4022.283\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671820506\n",
      "  timesteps_since_restore: 2240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2240000\n",
      "  training_iteration: 560\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:09 (running for 00:49:22.98)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         2942.54</td><td style=\"text-align: right;\">2240000</td><td style=\"text-align: right;\"> 260.814</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1559.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 1558.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 260.9015171894781\n",
      "  episode_reward_min: -61.37939871022665\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1599\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5411120653152466\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0122554125264287\n",
      "          model: {}\n",
      "          policy_loss: -0.04504931718111038\n",
      "          total_loss: 9.440035820007324\n",
      "          vf_explained_var: 0.5763307213783264\n",
      "          vf_loss: 9.469381332397461\n",
      "    num_agent_steps_sampled: 2248000\n",
      "    num_agent_steps_trained: 2248000\n",
      "    num_steps_sampled: 2248000\n",
      "    num_steps_trained: 2248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.44\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069074653081803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3154311183827967\n",
      "    mean_inference_ms: 0.5259507610411431\n",
      "    mean_raw_obs_processing_ms: 0.07023895048871047\n",
      "  time_since_restore: 2950.439685344696\n",
      "  time_this_iter_s: 3.9815526008605957\n",
      "  time_total_s: 2950.439685344696\n",
      "  timers:\n",
      "    learn_throughput: 2043.752\n",
      "    learn_time_ms: 1957.185\n",
      "    load_throughput: 31987065.777\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 992.998\n",
      "    sample_time_ms: 4028.205\n",
      "    update_time_ms: 1.413\n",
      "  timestamp: 1671820514\n",
      "  timesteps_since_restore: 2248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2248000\n",
      "  training_iteration: 562\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:15 (running for 00:49:28.97)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         2950.44</td><td style=\"text-align: right;\">2248000</td><td style=\"text-align: right;\"> 260.902</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1558.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:21 (running for 00:49:34.88)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         2954.38</td><td style=\"text-align: right;\">2252000</td><td style=\"text-align: right;\"> 260.881</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1558.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 1559.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 260.8157268689824\n",
      "  episode_reward_min: -61.37939871022665\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1604\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5497042536735535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012486857362091541\n",
      "          model: {}\n",
      "          policy_loss: -0.07053080946207047\n",
      "          total_loss: 6.7642903327941895\n",
      "          vf_explained_var: 0.660175085067749\n",
      "          vf_loss: 6.818820476531982\n",
      "    num_agent_steps_sampled: 2256000\n",
      "    num_agent_steps_trained: 2256000\n",
      "    num_steps_sampled: 2256000\n",
      "    num_steps_trained: 2256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.733333333333334\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09069093353960826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3154401285327342\n",
      "    mean_inference_ms: 0.5259177172290638\n",
      "    mean_raw_obs_processing_ms: 0.07024093779402013\n",
      "  time_since_restore: 2958.3169226646423\n",
      "  time_this_iter_s: 3.93281888961792\n",
      "  time_total_s: 2958.3169226646423\n",
      "  timers:\n",
      "    learn_throughput: 2065.772\n",
      "    learn_time_ms: 1936.322\n",
      "    load_throughput: 35552481.458\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1000.773\n",
      "    sample_time_ms: 3996.911\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1671820522\n",
      "  timesteps_since_restore: 2256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2256000\n",
      "  training_iteration: 564\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:27 (running for 00:49:40.87)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         2962.29</td><td style=\"text-align: right;\">2260000</td><td style=\"text-align: right;\"> 260.863</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1559.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-30\n",
      "  done: false\n",
      "  episode_len_mean: 1560.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.70127649121974\n",
      "  episode_reward_mean: 260.84782155425876\n",
      "  episode_reward_min: -61.37939871022665\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1609\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4058605432510376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012094411998987198\n",
      "          model: {}\n",
      "          policy_loss: -0.04070441052317619\n",
      "          total_loss: 9.769804000854492\n",
      "          vf_explained_var: 0.6175194978713989\n",
      "          vf_loss: 9.795011520385742\n",
      "    num_agent_steps_sampled: 2264000\n",
      "    num_agent_steps_trained: 2264000\n",
      "    num_steps_sampled: 2264000\n",
      "    num_steps_trained: 2264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.88333333333333\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068693184233015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.315399345890342\n",
      "    mean_inference_ms: 0.5259220419015326\n",
      "    mean_raw_obs_processing_ms: 0.0702334704237624\n",
      "  time_since_restore: 2966.2547845840454\n",
      "  time_this_iter_s: 3.9683263301849365\n",
      "  time_total_s: 2966.2547845840454\n",
      "  timers:\n",
      "    learn_throughput: 2075.245\n",
      "    learn_time_ms: 1927.483\n",
      "    load_throughput: 35544949.153\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1008.18\n",
      "    sample_time_ms: 3967.546\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1671820530\n",
      "  timesteps_since_restore: 2264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2264000\n",
      "  training_iteration: 566\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:33 (running for 00:49:46.82)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         2966.25</td><td style=\"text-align: right;\">2264000</td><td style=\"text-align: right;\"> 260.848</td><td style=\"text-align: right;\">             272.701</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">           1560.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 1558.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 272.74967103639693\n",
      "  episode_reward_mean: 261.49415759125174\n",
      "  episode_reward_min: -61.37939871022665\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1614\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5662333965301514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012792843393981457\n",
      "          model: {}\n",
      "          policy_loss: -0.042824625968933105\n",
      "          total_loss: 10.015908241271973\n",
      "          vf_explained_var: 0.5524118542671204\n",
      "          vf_loss: 10.042341232299805\n",
      "    num_agent_steps_sampled: 2272000\n",
      "    num_agent_steps_trained: 2272000\n",
      "    num_steps_sampled: 2272000\n",
      "    num_steps_trained: 2272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0906869981282383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31540849813172056\n",
      "    mean_inference_ms: 0.5258889501592283\n",
      "    mean_raw_obs_processing_ms: 0.0702352789764048\n",
      "  time_since_restore: 2974.148954629898\n",
      "  time_this_iter_s: 3.963515281677246\n",
      "  time_total_s: 2974.148954629898\n",
      "  timers:\n",
      "    learn_throughput: 2072.963\n",
      "    learn_time_ms: 1929.605\n",
      "    load_throughput: 35076763.538\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1009.164\n",
      "    sample_time_ms: 3963.677\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1671820538\n",
      "  timesteps_since_restore: 2272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2272000\n",
      "  training_iteration: 568\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:39 (running for 00:49:52.75)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         2974.15</td><td style=\"text-align: right;\">2272000</td><td style=\"text-align: right;\"> 261.494</td><td style=\"text-align: right;\">              272.75</td><td style=\"text-align: right;\">            -61.3794</td><td style=\"text-align: right;\">            1558.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:44 (running for 00:49:58.26)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         2978.64</td><td style=\"text-align: right;\">2276000</td><td style=\"text-align: right;\"> 264.948</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1568.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 1568.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 264.91084129071095\n",
      "  episode_reward_min: 242.81506707155222\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1619\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3588114082813263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011205199174582958\n",
      "          model: {}\n",
      "          policy_loss: -0.03916781768202782\n",
      "          total_loss: 7.300157070159912\n",
      "          vf_explained_var: 0.6048543453216553\n",
      "          vf_loss: 7.3249664306640625\n",
      "    num_agent_steps_sampled: 2280000\n",
      "    num_agent_steps_trained: 2280000\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96666666666667\n",
      "    ram_util_percent: 30.36666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068471633201618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31537525474701267\n",
      "    mean_inference_ms: 0.5259057332061446\n",
      "    mean_raw_obs_processing_ms: 0.07022936209075309\n",
      "  time_since_restore: 2982.9189858436584\n",
      "  time_this_iter_s: 4.281495571136475\n",
      "  time_total_s: 2982.9189858436584\n",
      "  timers:\n",
      "    learn_throughput: 2054.512\n",
      "    learn_time_ms: 1946.935\n",
      "    load_throughput: 32413477.589\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 985.663\n",
      "    sample_time_ms: 4058.18\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671820547\n",
      "  timesteps_since_restore: 2280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 570\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:50 (running for 00:50:03.60)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         2982.92</td><td style=\"text-align: right;\">2280000</td><td style=\"text-align: right;\"> 264.911</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1568.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 1567.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.23534422217364\n",
      "  episode_reward_min: 242.81506707155222\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1624\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4023451805114746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013174680061638355\n",
      "          model: {}\n",
      "          policy_loss: -0.04573962837457657\n",
      "          total_loss: 4.957964897155762\n",
      "          vf_explained_var: 0.6152583956718445\n",
      "          vf_loss: 4.98682165145874\n",
      "    num_agent_steps_sampled: 2288000\n",
      "    num_agent_steps_trained: 2288000\n",
      "    num_steps_sampled: 2288000\n",
      "    num_steps_trained: 2288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.72\n",
      "    ram_util_percent: 30.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068719603413518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153947732667222\n",
      "    mean_inference_ms: 0.5258918486949007\n",
      "    mean_raw_obs_processing_ms: 0.07023339298202004\n",
      "  time_since_restore: 2990.8256471157074\n",
      "  time_this_iter_s: 3.9495885372161865\n",
      "  time_total_s: 2990.8256471157074\n",
      "  timers:\n",
      "    learn_throughput: 2056.208\n",
      "    learn_time_ms: 1945.329\n",
      "    load_throughput: 32482509.197\n",
      "    load_time_ms: 0.123\n",
      "    sample_throughput: 985.431\n",
      "    sample_time_ms: 4059.139\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671820555\n",
      "  timesteps_since_restore: 2288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2288000\n",
      "  training_iteration: 572\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:35:56 (running for 00:50:09.50)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         2990.83</td><td style=\"text-align: right;\">2288000</td><td style=\"text-align: right;\"> 265.235</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1567.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:01 (running for 00:50:14.56)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">         2994.85</td><td style=\"text-align: right;\">2292000</td><td style=\"text-align: right;\"> 265.334</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1567.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 1566.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.5191895606025\n",
      "  episode_reward_min: 242.81506707155222\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1629\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4874095618724823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011093709617853165\n",
      "          model: {}\n",
      "          policy_loss: -0.04219987243413925\n",
      "          total_loss: 7.831262111663818\n",
      "          vf_explained_var: 0.6206256747245789\n",
      "          vf_loss: 7.859245777130127\n",
      "    num_agent_steps_sampled: 2296000\n",
      "    num_agent_steps_trained: 2296000\n",
      "    num_steps_sampled: 2296000\n",
      "    num_steps_trained: 2296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.28333333333334\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068598809301724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153669308144155\n",
      "    mean_inference_ms: 0.5259180747498625\n",
      "    mean_raw_obs_processing_ms: 0.07022865844869107\n",
      "  time_since_restore: 2998.7855491638184\n",
      "  time_this_iter_s: 3.9322657585144043\n",
      "  time_total_s: 2998.7855491638184\n",
      "  timers:\n",
      "    learn_throughput: 2051.781\n",
      "    learn_time_ms: 1949.526\n",
      "    load_throughput: 31022958.58\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 983.522\n",
      "    sample_time_ms: 4067.016\n",
      "    update_time_ms: 1.398\n",
      "  timestamp: 1671820563\n",
      "  timesteps_since_restore: 2296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2296000\n",
      "  training_iteration: 574\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:07 (running for 00:50:20.44)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">          3002.7</td><td style=\"text-align: right;\">2300000</td><td style=\"text-align: right;\"> 265.552</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1565.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-10\n",
      "  done: false\n",
      "  episode_len_mean: 1566.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.4681078918835\n",
      "  episode_reward_min: 242.81506707155222\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1635\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4603472650051117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011907338164746761\n",
      "          model: {}\n",
      "          policy_loss: -0.04474818333983421\n",
      "          total_loss: 9.274749755859375\n",
      "          vf_explained_var: 0.6056967377662659\n",
      "          vf_loss: 9.304240226745605\n",
      "    num_agent_steps_sampled: 2304000\n",
      "    num_agent_steps_trained: 2304000\n",
      "    num_steps_sampled: 2304000\n",
      "    num_steps_trained: 2304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068679888664401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153616670669597\n",
      "    mean_inference_ms: 0.5259258718783618\n",
      "    mean_raw_obs_processing_ms: 0.07022846065113862\n",
      "  time_since_restore: 3006.610061645508\n",
      "  time_this_iter_s: 3.9128494262695312\n",
      "  time_total_s: 3006.610061645508\n",
      "  timers:\n",
      "    learn_throughput: 2052.883\n",
      "    learn_time_ms: 1948.48\n",
      "    load_throughput: 31017223.147\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 986.384\n",
      "    sample_time_ms: 4055.217\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1671820570\n",
      "  timesteps_since_restore: 2304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2304000\n",
      "  training_iteration: 576\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:12 (running for 00:50:26.37)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         3006.61</td><td style=\"text-align: right;\">2304000</td><td style=\"text-align: right;\"> 265.468</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             242.815</td><td style=\"text-align: right;\">           1566.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-18\n",
      "  done: false\n",
      "  episode_len_mean: 1567.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.4838576273888\n",
      "  episode_reward_min: 245.19739659914578\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1639\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4695899188518524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01241994183510542\n",
      "          model: {}\n",
      "          policy_loss: -0.04966266453266144\n",
      "          total_loss: 6.427957534790039\n",
      "          vf_explained_var: 0.629348635673523\n",
      "          vf_loss: 6.461704730987549\n",
      "    num_agent_steps_sampled: 2312000\n",
      "    num_agent_steps_trained: 2312000\n",
      "    num_steps_sampled: 2312000\n",
      "    num_steps_trained: 2312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.949999999999996\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068696190418829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31535652498218103\n",
      "    mean_inference_ms: 0.5259286148611693\n",
      "    mean_raw_obs_processing_ms: 0.07022788075434867\n",
      "  time_since_restore: 3014.431273460388\n",
      "  time_this_iter_s: 3.9100000858306885\n",
      "  time_total_s: 3014.431273460388\n",
      "  timers:\n",
      "    learn_throughput: 2055.638\n",
      "    learn_time_ms: 1945.868\n",
      "    load_throughput: 31506508.92\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 987.723\n",
      "    sample_time_ms: 4049.718\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1671820578\n",
      "  timesteps_since_restore: 2312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2312000\n",
      "  training_iteration: 578\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:18 (running for 00:50:32.22)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         3014.43</td><td style=\"text-align: right;\">2312000</td><td style=\"text-align: right;\"> 265.484</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             245.197</td><td style=\"text-align: right;\">           1567.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:24 (running for 00:50:38.15)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         3018.34</td><td style=\"text-align: right;\">2316000</td><td style=\"text-align: right;\">   265.4</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             245.197</td><td style=\"text-align: right;\">           1568.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 1566.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.52801913787965\n",
      "  episode_reward_min: 245.19739659914578\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1645\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5391771793365479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0121662188321352\n",
      "          model: {}\n",
      "          policy_loss: -0.048184268176555634\n",
      "          total_loss: 10.132344245910645\n",
      "          vf_explained_var: 0.5358518958091736\n",
      "          vf_loss: 10.164937973022461\n",
      "    num_agent_steps_sampled: 2320000\n",
      "    num_agent_steps_trained: 2320000\n",
      "    num_steps_sampled: 2320000\n",
      "    num_steps_trained: 2320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.62\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0906858436919053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31534277783085457\n",
      "    mean_inference_ms: 0.5259244567001212\n",
      "    mean_raw_obs_processing_ms: 0.07022562823648565\n",
      "  time_since_restore: 3022.3082447052\n",
      "  time_this_iter_s: 3.9651379585266113\n",
      "  time_total_s: 3022.3082447052\n",
      "  timers:\n",
      "    learn_throughput: 2072.822\n",
      "    learn_time_ms: 1929.736\n",
      "    load_throughput: 31793094.561\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 1010.879\n",
      "    sample_time_ms: 3956.951\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1671820586\n",
      "  timesteps_since_restore: 2320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2320000\n",
      "  training_iteration: 580\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:30 (running for 00:50:44.08)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">         3026.24</td><td style=\"text-align: right;\">2324000</td><td style=\"text-align: right;\"> 265.775</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             245.197</td><td style=\"text-align: right;\">           1564.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 1565.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.76930880590453\n",
      "  episode_reward_min: 245.19739659914578\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1650\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4371970593929291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010720444843173027\n",
      "          model: {}\n",
      "          policy_loss: -0.05207153782248497\n",
      "          total_loss: 7.493518829345703\n",
      "          vf_explained_var: 0.6728188991546631\n",
      "          vf_loss: 7.531852722167969\n",
      "    num_agent_steps_sampled: 2328000\n",
      "    num_agent_steps_trained: 2328000\n",
      "    num_steps_sampled: 2328000\n",
      "    num_steps_trained: 2328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.800000000000004\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068484517109762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31533024232208906\n",
      "    mean_inference_ms: 0.525919941544073\n",
      "    mean_raw_obs_processing_ms: 0.07022384230806779\n",
      "  time_since_restore: 3030.142392396927\n",
      "  time_this_iter_s: 3.9024770259857178\n",
      "  time_total_s: 3030.142392396927\n",
      "  timers:\n",
      "    learn_throughput: 2071.789\n",
      "    learn_time_ms: 1930.698\n",
      "    load_throughput: 31418007.491\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1012.828\n",
      "    sample_time_ms: 3949.338\n",
      "    update_time_ms: 1.318\n",
      "  timestamp: 1671820594\n",
      "  timesteps_since_restore: 2328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2328000\n",
      "  training_iteration: 582\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:36 (running for 00:50:50.00)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         3030.14</td><td style=\"text-align: right;\">2328000</td><td style=\"text-align: right;\"> 265.769</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             245.197</td><td style=\"text-align: right;\">           1565.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:42 (running for 00:50:56.00)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         3034.07</td><td style=\"text-align: right;\">2332000</td><td style=\"text-align: right;\">  265.85</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             245.197</td><td style=\"text-align: right;\">           1565.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 1564.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 266.15842416312256\n",
      "  episode_reward_min: 245.19739659914578\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1655\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.46064186096191406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011611399240791798\n",
      "          model: {}\n",
      "          policy_loss: -0.04877990484237671\n",
      "          total_loss: 6.509220600128174\n",
      "          vf_explained_var: 0.6746517419815063\n",
      "          vf_loss: 6.543121337890625\n",
      "    num_agent_steps_sampled: 2336000\n",
      "    num_agent_steps_trained: 2336000\n",
      "    num_steps_sampled: 2336000\n",
      "    num_steps_trained: 2336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.82857142857143\n",
      "    ram_util_percent: 30.314285714285717\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068416495993685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153190273857642\n",
      "    mean_inference_ms: 0.5259181242732276\n",
      "    mean_raw_obs_processing_ms: 0.07022230511277235\n",
      "  time_since_restore: 3038.523747444153\n",
      "  time_this_iter_s: 4.455322742462158\n",
      "  time_total_s: 3038.523747444153\n",
      "  timers:\n",
      "    learn_throughput: 2053.948\n",
      "    learn_time_ms: 1947.469\n",
      "    load_throughput: 33156553.36\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1007.325\n",
      "    sample_time_ms: 3970.913\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671820602\n",
      "  timesteps_since_restore: 2336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2336000\n",
      "  training_iteration: 584\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:48 (running for 00:51:01.59)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">         3042.68</td><td style=\"text-align: right;\">2340000</td><td style=\"text-align: right;\"> 265.139</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             143.521</td><td style=\"text-align: right;\">           1561.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-51\n",
      "  done: false\n",
      "  episode_len_mean: 1561.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.08889617364264\n",
      "  episode_reward_min: 143.52086908108026\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1661\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.49539223313331604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015086282975971699\n",
      "          model: {}\n",
      "          policy_loss: -0.043064165860414505\n",
      "          total_loss: 13.2528076171875\n",
      "          vf_explained_var: 0.5324665904045105\n",
      "          vf_loss: 13.276540756225586\n",
      "    num_agent_steps_sampled: 2344000\n",
      "    num_agent_steps_trained: 2344000\n",
      "    num_steps_sampled: 2344000\n",
      "    num_steps_trained: 2344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.019999999999996\n",
      "    ram_util_percent: 30.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068495839332837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31531198272139327\n",
      "    mean_inference_ms: 0.5259243619519912\n",
      "    mean_raw_obs_processing_ms: 0.07022242799942273\n",
      "  time_since_restore: 3046.674367904663\n",
      "  time_this_iter_s: 3.996990919113159\n",
      "  time_total_s: 3046.674367904663\n",
      "  timers:\n",
      "    learn_throughput: 2046.749\n",
      "    learn_time_ms: 1954.318\n",
      "    load_throughput: 32832125.245\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 995.14\n",
      "    sample_time_ms: 4019.534\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1671820611\n",
      "  timesteps_since_restore: 2344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2344000\n",
      "  training_iteration: 586\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:53 (running for 00:51:06.66)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         3046.67</td><td style=\"text-align: right;\">2344000</td><td style=\"text-align: right;\"> 265.089</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             143.521</td><td style=\"text-align: right;\">           1561.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 1561.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.20524933143224\n",
      "  episode_reward_mean: 265.1721073737541\n",
      "  episode_reward_min: 143.52086908108026\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1665\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3644111156463623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013018472120165825\n",
      "          model: {}\n",
      "          policy_loss: -0.04097072407603264\n",
      "          total_loss: 6.073967933654785\n",
      "          vf_explained_var: 0.7220771312713623\n",
      "          vf_loss: 6.098255634307861\n",
      "    num_agent_steps_sampled: 2352000\n",
      "    num_agent_steps_trained: 2352000\n",
      "    num_steps_sampled: 2352000\n",
      "    num_steps_trained: 2352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7\n",
      "    ram_util_percent: 30.380000000000003\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068548412704296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153071548790949\n",
      "    mean_inference_ms: 0.5259285948029498\n",
      "    mean_raw_obs_processing_ms: 0.07022249990928289\n",
      "  time_since_restore: 3054.518194913864\n",
      "  time_this_iter_s: 3.926746129989624\n",
      "  time_total_s: 3054.518194913864\n",
      "  timers:\n",
      "    learn_throughput: 2042.984\n",
      "    learn_time_ms: 1957.92\n",
      "    load_throughput: 32710501.072\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 994.294\n",
      "    sample_time_ms: 4022.957\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1671820619\n",
      "  timesteps_since_restore: 2352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2352000\n",
      "  training_iteration: 588\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:36:59 (running for 00:51:12.48)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         3054.52</td><td style=\"text-align: right;\">2352000</td><td style=\"text-align: right;\"> 265.172</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             143.521</td><td style=\"text-align: right;\">           1561.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:04 (running for 00:51:17.52)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">          3058.5</td><td style=\"text-align: right;\">2356000</td><td style=\"text-align: right;\"> 265.335</td><td style=\"text-align: right;\">             276.205</td><td style=\"text-align: right;\">             143.521</td><td style=\"text-align: right;\">           1561.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 1557.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 263.6745830667194\n",
      "  episode_reward_min: 85.37441955943939\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1671\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4331170916557312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009227932430803776\n",
      "          model: {}\n",
      "          policy_loss: -0.036431532353162766\n",
      "          total_loss: 113.50200653076172\n",
      "          vf_explained_var: 0.2894187569618225\n",
      "          vf_loss: 113.526611328125\n",
      "    num_agent_steps_sampled: 2360000\n",
      "    num_agent_steps_trained: 2360000\n",
      "    num_steps_sampled: 2360000\n",
      "    num_steps_trained: 2360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068641875868075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3153002281574365\n",
      "    mean_inference_ms: 0.5259360747307009\n",
      "    mean_raw_obs_processing_ms: 0.07022300292610385\n",
      "  time_since_restore: 3062.5019025802612\n",
      "  time_this_iter_s: 4.001644849777222\n",
      "  time_total_s: 3062.5019025802612\n",
      "  timers:\n",
      "    learn_throughput: 2044.689\n",
      "    learn_time_ms: 1956.288\n",
      "    load_throughput: 33763767.358\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 990.654\n",
      "    sample_time_ms: 4037.735\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1671820627\n",
      "  timesteps_since_restore: 2360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2360000\n",
      "  training_iteration: 590\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:10 (running for 00:51:23.50)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">          3062.5</td><td style=\"text-align: right;\">2360000</td><td style=\"text-align: right;\"> 263.675</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             85.3744</td><td style=\"text-align: right;\">           1557.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 1552.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 262.38464210639273\n",
      "  episode_reward_min: 85.37441955943939\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1676\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.47123342752456665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0131680928170681\n",
      "          model: {}\n",
      "          policy_loss: -0.05114666372537613\n",
      "          total_loss: 7.282747745513916\n",
      "          vf_explained_var: 0.4898144006729126\n",
      "          vf_loss: 7.317020416259766\n",
      "    num_agent_steps_sampled: 2368000\n",
      "    num_agent_steps_trained: 2368000\n",
      "    num_steps_sampled: 2368000\n",
      "    num_steps_trained: 2368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.72\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0906872377145625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31529487939397055\n",
      "    mean_inference_ms: 0.525942770156395\n",
      "    mean_raw_obs_processing_ms: 0.07022360884590668\n",
      "  time_since_restore: 3070.3954136371613\n",
      "  time_this_iter_s: 3.959482431411743\n",
      "  time_total_s: 3070.3954136371613\n",
      "  timers:\n",
      "    learn_throughput: 2043.901\n",
      "    learn_time_ms: 1957.042\n",
      "    load_throughput: 33982612.923\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 989.584\n",
      "    sample_time_ms: 4042.103\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1671820634\n",
      "  timesteps_since_restore: 2368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2368000\n",
      "  training_iteration: 592\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:16 (running for 00:51:29.48)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">          3070.4</td><td style=\"text-align: right;\">2368000</td><td style=\"text-align: right;\"> 262.385</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             85.3744</td><td style=\"text-align: right;\">            1552.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:21 (running for 00:51:35.40)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">         3074.35</td><td style=\"text-align: right;\">2372000</td><td style=\"text-align: right;\"> 260.077</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1545.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 1544.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 260.16704249797624\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1682\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.44573527574539185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01345149613916874\n",
      "          model: {}\n",
      "          policy_loss: -0.04557841271162033\n",
      "          total_loss: 11.632561683654785\n",
      "          vf_explained_var: 0.4865749478340149\n",
      "          vf_loss: 11.660902976989746\n",
      "    num_agent_steps_sampled: 2376000\n",
      "    num_agent_steps_trained: 2376000\n",
      "    num_steps_sampled: 2376000\n",
      "    num_steps_trained: 2376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.93333333333333\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068673949466848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31528285514759175\n",
      "    mean_inference_ms: 0.5259416650630634\n",
      "    mean_raw_obs_processing_ms: 0.07022292762950796\n",
      "  time_since_restore: 3078.272708415985\n",
      "  time_this_iter_s: 3.9180610179901123\n",
      "  time_total_s: 3078.272708415985\n",
      "  timers:\n",
      "    learn_throughput: 2064.325\n",
      "    learn_time_ms: 1937.68\n",
      "    load_throughput: 33968851.994\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 997.17\n",
      "    sample_time_ms: 4011.352\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1671820642\n",
      "  timesteps_since_restore: 2376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2376000\n",
      "  training_iteration: 594\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:27 (running for 00:51:41.29)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         3082.21</td><td style=\"text-align: right;\">2380000</td><td style=\"text-align: right;\"> 260.126</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1545.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 1546.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 260.12960407708795\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1687\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5414290428161621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013872052542865276\n",
      "          model: {}\n",
      "          policy_loss: -0.046820010989904404\n",
      "          total_loss: 10.945013999938965\n",
      "          vf_explained_var: 0.5099969506263733\n",
      "          vf_loss: 10.974058151245117\n",
      "    num_agent_steps_sampled: 2384000\n",
      "    num_agent_steps_trained: 2384000\n",
      "    num_steps_sampled: 2384000\n",
      "    num_steps_trained: 2384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068626623826796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31527317471814137\n",
      "    mean_inference_ms: 0.5259406941347944\n",
      "    mean_raw_obs_processing_ms: 0.07022224660284065\n",
      "  time_since_restore: 3086.145318031311\n",
      "  time_this_iter_s: 3.934508800506592\n",
      "  time_total_s: 3086.145318031311\n",
      "  timers:\n",
      "    learn_throughput: 2069.636\n",
      "    learn_time_ms: 1932.707\n",
      "    load_throughput: 34400688.948\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1008.182\n",
      "    sample_time_ms: 3967.538\n",
      "    update_time_ms: 1.344\n",
      "  timestamp: 1671820650\n",
      "  timesteps_since_restore: 2384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2384000\n",
      "  training_iteration: 596\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:33 (running for 00:51:47.24)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         3086.15</td><td style=\"text-align: right;\">2384000</td><td style=\"text-align: right;\">  260.13</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-37-38\n",
      "  done: false\n",
      "  episode_len_mean: 1546.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 260.21347457335156\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1692\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3967040777206421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012221160344779491\n",
      "          model: {}\n",
      "          policy_loss: -0.040425438433885574\n",
      "          total_loss: 7.929361820220947\n",
      "          vf_explained_var: 0.6315520405769348\n",
      "          vf_loss: 7.954126834869385\n",
      "    num_agent_steps_sampled: 2392000\n",
      "    num_agent_steps_trained: 2392000\n",
      "    num_steps_sampled: 2392000\n",
      "    num_steps_trained: 2392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.2\n",
      "    ram_util_percent: 30.3\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068589358760684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31526405349395753\n",
      "    mean_inference_ms: 0.5259404872345155\n",
      "    mean_raw_obs_processing_ms: 0.07022164948788932\n",
      "  time_since_restore: 3094.0537197589874\n",
      "  time_this_iter_s: 3.9907360076904297\n",
      "  time_total_s: 3094.0537197589874\n",
      "  timers:\n",
      "    learn_throughput: 2069.941\n",
      "    learn_time_ms: 1932.423\n",
      "    load_throughput: 32948185.389\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1007.396\n",
      "    sample_time_ms: 3970.635\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1671820658\n",
      "  timesteps_since_restore: 2392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2392000\n",
      "  training_iteration: 598\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:39 (running for 00:51:53.19)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         3094.05</td><td style=\"text-align: right;\">2392000</td><td style=\"text-align: right;\"> 260.213</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:45 (running for 00:51:58.53)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:50 (running for 00:52:03.58)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:37:55 (running for 00:52:08.58)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:00 (running for 00:52:13.59)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:05 (running for 00:52:18.59)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:10 (running for 00:52:23.60)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:15 (running for 00:52:28.60)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:20 (running for 00:52:33.60)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:25 (running for 00:52:38.61)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:30 (running for 00:52:43.61)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:35 (running for 00:52:48.62)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:40 (running for 00:52:53.62)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:45 (running for 00:52:58.63)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:50 (running for 00:53:03.63)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:38:55 (running for 00:53:08.63)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:00 (running for 00:53:13.64)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:05 (running for 00:53:18.64)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:10 (running for 00:53:23.65)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:15 (running for 00:53:28.65)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:20 (running for 00:53:33.66)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:25 (running for 00:53:38.66)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:30 (running for 00:53:43.67)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:35 (running for 00:53:48.67)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:40 (running for 00:53:53.67)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:45 (running for 00:53:58.68)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:50 (running for 00:54:03.68)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:39:55 (running for 00:54:08.69)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:00 (running for 00:54:13.69)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:05 (running for 00:54:18.70)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         3098.38</td><td style=\"text-align: right;\">2396000</td><td style=\"text-align: right;\"> 260.251</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1546.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 1547.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 260.38878759118523\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1697\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1507.03\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 277.02345077242234\n",
      "    episode_reward_mean: 254.37686608094492\n",
      "    episode_reward_min: -124.59017066359893\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1563\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1572\n",
      "      - 1529\n",
      "      - 1585\n",
      "      - 1540\n",
      "      - 1543\n",
      "      - 1600\n",
      "      - 1561\n",
      "      - 1580\n",
      "      - 1519\n",
      "      - 1514\n",
      "      - 1538\n",
      "      - 1600\n",
      "      - 1508\n",
      "      - 1548\n",
      "      - 1517\n",
      "      - 1556\n",
      "      - 1600\n",
      "      - 1521\n",
      "      - 1546\n",
      "      - 506\n",
      "      - 1600\n",
      "      - 1527\n",
      "      - 1508\n",
      "      - 1574\n",
      "      - 1467\n",
      "      - 1520\n",
      "      - 1591\n",
      "      - 1600\n",
      "      - 1539\n",
      "      - 1540\n",
      "      - 1530\n",
      "      - 1511\n",
      "      - 1567\n",
      "      - 1531\n",
      "      - 1598\n",
      "      - 1600\n",
      "      - 1526\n",
      "      - 1492\n",
      "      - 1595\n",
      "      - 1444\n",
      "      - 1502\n",
      "      - 1600\n",
      "      - 122\n",
      "      - 1596\n",
      "      - 1594\n",
      "      - 1517\n",
      "      - 1585\n",
      "      - 1581\n",
      "      - 1600\n",
      "      - 1567\n",
      "      - 1578\n",
      "      - 1545\n",
      "      - 1512\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1533\n",
      "      - 1569\n",
      "      - 1559\n",
      "      - 1545\n",
      "      - 1558\n",
      "      - 1539\n",
      "      - 1537\n",
      "      - 1600\n",
      "      - 1528\n",
      "      - 1536\n",
      "      - 1501\n",
      "      - 1575\n",
      "      - 1560\n",
      "      - 1597\n",
      "      - 1534\n",
      "      - 1481\n",
      "      - 1553\n",
      "      - 1550\n",
      "      - 1556\n",
      "      - 1600\n",
      "      - 1574\n",
      "      - 1592\n",
      "      - 1600\n",
      "      - 1591\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 518\n",
      "      - 1526\n",
      "      - 1559\n",
      "      - 1600\n",
      "      - 1543\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1508\n",
      "      - 123\n",
      "      - 1469\n",
      "      - 1560\n",
      "      - 1550\n",
      "      - 1534\n",
      "      - 1540\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 270.29717936558745\n",
      "      - 264.03163336930703\n",
      "      - 264.61385194790233\n",
      "      - 269.08288858396094\n",
      "      - 273.0658194571894\n",
      "      - 270.5465376884975\n",
      "      - 270.75959099944976\n",
      "      - 271.49137451655963\n",
      "      - 263.82005837531085\n",
      "      - 270.5189876342814\n",
      "      - 268.8588148491741\n",
      "      - 271.2470791494253\n",
      "      - 273.63342034233494\n",
      "      - 272.61930248152146\n",
      "      - 265.5113531178205\n",
      "      - 272.34929654269337\n",
      "      - 270.5202769774822\n",
      "      - 271.9020449869273\n",
      "      - 268.5943314370852\n",
      "      - 251.94021508687504\n",
      "      - 272.3546889744476\n",
      "      - 268.47450071461844\n",
      "      - -31.87479988334387\n",
      "      - 240.49938653588643\n",
      "      - 269.6559209331581\n",
      "      - 270.18452649025346\n",
      "      - 269.81920470914054\n",
      "      - 277.02345077242234\n",
      "      - 270.3850081014688\n",
      "      - 268.6351962275369\n",
      "      - 256.70301338346\n",
      "      - 271.5418317341804\n",
      "      - 271.49785053572515\n",
      "      - 272.4623413785276\n",
      "      - 272.28976014073\n",
      "      - 269.64710731777086\n",
      "      - 270.0331390471011\n",
      "      - 270.12959246977346\n",
      "      - 264.798713044228\n",
      "      - 271.2815687770708\n",
      "      - 272.9174213531961\n",
      "      - 269.14423217323395\n",
      "      - 276.7931640471414\n",
      "      - 273.1897291226323\n",
      "      - 258.6726865078722\n",
      "      - -121.53697218300216\n",
      "      - 265.94576794692574\n",
      "      - 268.287327917042\n",
      "      - 271.5915154115296\n",
      "      - 270.09542531862746\n",
      "      - 266.9140413196128\n",
      "      - 265.9374849541483\n",
      "      - 270.01500422547895\n",
      "      - 267.9697074570106\n",
      "      - 271.24602228278053\n",
      "      - 272.2928351671691\n",
      "      - 254.05230766074146\n",
      "      - 228.70674341257484\n",
      "      - 274.27269243463496\n",
      "      - 268.84612002819847\n",
      "      - 267.90400337662584\n",
      "      - 270.9539298089444\n",
      "      - 270.83775844993863\n",
      "      - 270.6945066567349\n",
      "      - 270.10794681795346\n",
      "      - 264.2787645916266\n",
      "      - 271.2915324896905\n",
      "      - 269.9678638151852\n",
      "      - 275.2433675323991\n",
      "      - 268.6939813851173\n",
      "      - 266.8156742878565\n",
      "      - 268.7640454645452\n",
      "      - 274.8372244045123\n",
      "      - 275.20270496681024\n",
      "      - 269.4302589037386\n",
      "      - 269.7637650314579\n",
      "      - 271.41347076126385\n",
      "      - 233.2695839153772\n",
      "      - 269.7082453863338\n",
      "      - 269.7859814200582\n",
      "      - 260.6531724900256\n",
      "      - 267.7045954736523\n",
      "      - 265.7394864007049\n",
      "      - 262.3330874531394\n",
      "      - -20.135785428707692\n",
      "      - 272.73958101111066\n",
      "      - 271.42312053154365\n",
      "      - 263.061383642146\n",
      "      - 268.41879724489576\n",
      "      - 265.16039882386804\n",
      "      - 263.75372240431136\n",
      "      - 274.55592225258187\n",
      "      - -124.59017066359893\n",
      "      - 273.9976286626208\n",
      "      - 271.2147421817955\n",
      "      - 271.7075915591093\n",
      "      - 271.84071223166103\n",
      "      - 268.3457638781791\n",
      "      - 267.41461989989705\n",
      "      - 267.08431370829976\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08576608799085181\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.30340464304502396\n",
      "      mean_inference_ms: 0.4941921123140023\n",
      "      mean_raw_obs_processing_ms: 0.05912254792502829\n",
      "    timesteps_this_iter: 150703\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.367525577545166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01287129893898964\n",
      "          model: {}\n",
      "          policy_loss: -0.04444114863872528\n",
      "          total_loss: 9.591339111328125\n",
      "          vf_explained_var: 0.5722971558570862\n",
      "          vf_loss: 9.619285583496094\n",
      "    num_agent_steps_sampled: 2400000\n",
      "    num_agent_steps_trained: 2400000\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.669230769230772\n",
      "    ram_util_percent: 30.33221153846154\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068658837434979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3152596385745769\n",
      "    mean_inference_ms: 0.5259459023374078\n",
      "    mean_raw_obs_processing_ms: 0.07022209030210531\n",
      "  time_since_restore: 3243.9897010326385\n",
      "  time_this_iter_s: 145.60915350914001\n",
      "  time_total_s: 3243.9897010326385\n",
      "  timers:\n",
      "    learn_throughput: 2058.469\n",
      "    learn_time_ms: 1943.192\n",
      "    load_throughput: 32023699.179\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1000.932\n",
      "    sample_time_ms: 3996.275\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1671820808\n",
      "  timesteps_since_restore: 2400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 600\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:10 (running for 00:54:24.18)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         3243.99</td><td style=\"text-align: right;\">2400000</td><td style=\"text-align: right;\"> 260.389</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1547.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:15 (running for 00:54:29.22)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">         3248.02</td><td style=\"text-align: right;\">2404000</td><td style=\"text-align: right;\"> 260.483</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1547.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-40-16\n",
      "  done: false\n",
      "  episode_len_mean: 1546.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 260.7032062102491\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1702\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.35431498289108276\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01372537948191166\n",
      "          model: {}\n",
      "          policy_loss: -0.045542094856500626\n",
      "          total_loss: 6.441131114959717\n",
      "          vf_explained_var: 0.5284246802330017\n",
      "          vf_loss: 6.4690842628479\n",
      "    num_agent_steps_sampled: 2408000\n",
      "    num_agent_steps_trained: 2408000\n",
      "    num_steps_sampled: 2408000\n",
      "    num_steps_trained: 2408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.44\n",
      "    ram_util_percent: 30.18\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068730112485467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31525458017861757\n",
      "    mean_inference_ms: 0.5259513473747609\n",
      "    mean_raw_obs_processing_ms: 0.07022259811870932\n",
      "  time_since_restore: 3251.966140985489\n",
      "  time_this_iter_s: 3.9436488151550293\n",
      "  time_total_s: 3251.966140985489\n",
      "  timers:\n",
      "    learn_throughput: 2050.541\n",
      "    learn_time_ms: 1950.705\n",
      "    load_throughput: 30147737.646\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 220.104\n",
      "    sample_time_ms: 18173.243\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1671820816\n",
      "  timesteps_since_restore: 2408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2408000\n",
      "  training_iteration: 602\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:20 (running for 00:54:34.23)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         3255.99</td><td style=\"text-align: right;\">2412000</td><td style=\"text-align: right;\"> 260.924</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1544.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 1544.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 261.02672528838104\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1707\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.2426237165927887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015535356476902962\n",
      "          model: {}\n",
      "          policy_loss: -0.04133625701069832\n",
      "          total_loss: 5.680990695953369\n",
      "          vf_explained_var: 0.664831280708313\n",
      "          vf_loss: 5.702418327331543\n",
      "    num_agent_steps_sampled: 2416000\n",
      "    num_agent_steps_trained: 2416000\n",
      "    num_steps_sampled: 2416000\n",
      "    num_steps_trained: 2416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.25000000000001\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068823150864774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3152500964165206\n",
      "    mean_inference_ms: 0.5259579402559091\n",
      "    mean_raw_obs_processing_ms: 0.0702233004882061\n",
      "  time_since_restore: 3259.9593029022217\n",
      "  time_this_iter_s: 3.968470335006714\n",
      "  time_total_s: 3259.9593029022217\n",
      "  timers:\n",
      "    learn_throughput: 2050.562\n",
      "    learn_time_ms: 1950.685\n",
      "    load_throughput: 30147737.646\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 219.935\n",
      "    sample_time_ms: 18187.221\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671820824\n",
      "  timesteps_since_restore: 2416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2416000\n",
      "  training_iteration: 604\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:26 (running for 00:54:40.21)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         3259.96</td><td style=\"text-align: right;\">2416000</td><td style=\"text-align: right;\"> 261.027</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1544.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:31 (running for 00:54:45.27)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">         3263.96</td><td style=\"text-align: right;\">2420000</td><td style=\"text-align: right;\"> 261.133</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1543.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 1543.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 261.17512472232033\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1712\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4409061372280121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012382074259221554\n",
      "          model: {}\n",
      "          policy_loss: -0.041835688054561615\n",
      "          total_loss: 8.942277908325195\n",
      "          vf_explained_var: 0.6545931100845337\n",
      "          vf_loss: 8.968245506286621\n",
      "    num_agent_steps_sampled: 2424000\n",
      "    num_agent_steps_trained: 2424000\n",
      "    num_steps_sampled: 2424000\n",
      "    num_steps_trained: 2424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.44\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068934778294882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3152452316782156\n",
      "    mean_inference_ms: 0.5259653871726424\n",
      "    mean_raw_obs_processing_ms: 0.0702241564157929\n",
      "  time_since_restore: 3267.966337442398\n",
      "  time_this_iter_s: 4.0036680698394775\n",
      "  time_total_s: 3267.966337442398\n",
      "  timers:\n",
      "    learn_throughput: 2041.493\n",
      "    learn_time_ms: 1959.35\n",
      "    load_throughput: 28508438.403\n",
      "    load_time_ms: 0.14\n",
      "    sample_throughput: 219.789\n",
      "    sample_time_ms: 18199.302\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671820832\n",
      "  timesteps_since_restore: 2424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2424000\n",
      "  training_iteration: 606\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:36 (running for 00:54:50.29)<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">         3267.97</td><td style=\"text-align: right;\">2424000</td><td style=\"text-align: right;\"> 261.175</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1543.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 2432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-40-40\n",
      "  done: false\n",
      "  episode_len_mean: 1545.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 276.2108643610097\n",
      "  episode_reward_mean: 261.1388104087341\n",
      "  episode_reward_min: 29.744334103788532\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1718\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2814452648162842\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3608458936214447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013033121824264526\n",
      "          model: {}\n",
      "          policy_loss: -0.04927125200629234\n",
      "          total_loss: 9.596880912780762\n",
      "          vf_explained_var: 0.631805419921875\n",
      "          vf_loss: 9.629449844360352\n",
      "    num_agent_steps_sampled: 2432000\n",
      "    num_agent_steps_trained: 2432000\n",
      "    num_steps_sampled: 2432000\n",
      "    num_steps_trained: 2432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.31666666666666\n",
      "    ram_util_percent: 30.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09068990172354013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31523618324732416\n",
      "    mean_inference_ms: 0.5259684924319461\n",
      "    mean_raw_obs_processing_ms: 0.07022451920563845\n",
      "  time_since_restore: 3275.996797323227\n",
      "  time_this_iter_s: 4.020996809005737\n",
      "  time_total_s: 3275.996797323227\n",
      "  timers:\n",
      "    learn_throughput: 2042.172\n",
      "    learn_time_ms: 1958.699\n",
      "    load_throughput: 29652202.192\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 219.563\n",
      "    sample_time_ms: 18218.025\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1671820840\n",
      "  timesteps_since_restore: 2432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2432000\n",
      "  training_iteration: 608\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:41 (running for 00:54:55.36)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">            3276</td><td style=\"text-align: right;\">2432000</td><td style=\"text-align: right;\"> 261.139</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1545.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 18:40:43,952\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:40:44 (running for 00:54:57.43)<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">            3276</td><td style=\"text-align: right;\">2432000</td><td style=\"text-align: right;\"> 261.139</td><td style=\"text-align: right;\">             276.211</td><td style=\"text-align: right;\">             29.7443</td><td style=\"text-align: right;\">           1545.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 18:40:44,227\tERROR tune.py:635 -- Trials did not complete: [PPO_BipedalWalker-v3_a08d1_00000]\n",
      "2022-12-23 18:40:44,228\tINFO tune.py:639 -- Total run time: 3299.13 seconds (3297.36 seconds for the tuning loop).\n",
      "2022-12-23 18:40:44,229\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f5da41559a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Fill in the blanks with the correct settings and run the cell\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"BipedalWalker-v3\",\n",
    "                 \"evaluation_interval\": 100,\n",
    "                 \"evaluation_num_episodes\": 100\n",
    "                 },\n",
    "         local_dir=\"bipedal_walker_v3\",    # Save results to the relative path bipedal_walker_v3\n",
    "         checkpoint_freq=100,    # Save the agent at every evaluation\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fb4b1-2985-47b4-a15c-bc9baaa316cc",
   "metadata": {},
   "source": [
    "Open `tensorboard` in a terminal to visualize the results. Stop the experiment once the evaluation performance has reached 250 cumulative rewards per episode.\n",
    "\n",
    "A performance of > 250 typically means that the robot has learned how to walk! \n",
    "\n",
    "I am sure you want to see the robot walking. In the next lesson, we will learn how to see the walking robot in action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdeeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
