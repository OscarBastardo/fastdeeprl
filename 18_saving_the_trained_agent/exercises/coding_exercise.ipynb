{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d192556-0e8b-4ee7-aff7-f161a8c67a2c",
   "metadata": {},
   "source": [
    "# Save the trained robot\n",
    "\n",
    "Run the same experiment (using PPO in the `BipedalWalker-v3` environment) again. But this time, store the results in the relative path `bipedal_walker_v3` and save the agent at every evaluation.  \n",
    "\n",
    "Please stop any previous experiments (if they are still running) before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e03d10-671e-4b98-85ff-f8669ae3a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,618\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:48,619\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:52,094\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:52 (running for 00:00:06.22)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:52,806\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:53 (running for 00:00:07.23)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:45:54,960\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 480.375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -103.69946023898075\n",
      "  episode_reward_mean: -114.8549389703056\n",
      "  episode_reward_min: -124.98623578542097\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 8\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6258463859558105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012235086411237717\n",
      "          model: {}\n",
      "          policy_loss: -0.01618567667901516\n",
      "          total_loss: 623.4373779296875\n",
      "          vf_explained_var: -0.006057567894458771\n",
      "          vf_loss: 623.4510498046875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.214285714285715\n",
      "    ram_util_percent: 21.342857142857145\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09166318616052081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.331095222947837\n",
      "    mean_inference_ms: 0.552110333016132\n",
      "    mean_raw_obs_processing_ms: 0.07754194921162771\n",
      "  time_since_restore: 4.269518852233887\n",
      "  time_this_iter_s: 4.269518852233887\n",
      "  time_total_s: 4.269518852233887\n",
      "  timers:\n",
      "    learn_throughput: 1891.704\n",
      "    learn_time_ms: 2114.495\n",
      "    load_throughput: 34379540.984\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1395.154\n",
      "    sample_time_ms: 2867.068\n",
      "    update_time_ms: 1.819\n",
      "  timestamp: 1671817557\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:59 (running for 00:00:12.51)<br>Memory usage on this node: 6.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.26952</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-114.855</td><td style=\"text-align: right;\">            -103.699</td><td style=\"text-align: right;\">            -124.986</td><td style=\"text-align: right;\">           480.375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:04 (running for 00:00:17.52)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.26381</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -115.2</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           447.143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 566.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.31078195821432\n",
      "  episode_reward_mean: -115.73399798446101\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 20\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.721611022949219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020989786833524704\n",
      "          model: {}\n",
      "          policy_loss: -0.028235970064997673\n",
      "          total_loss: 209.29212951660156\n",
      "          vf_explained_var: -0.003048080950975418\n",
      "          vf_loss: 209.316162109375\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105358281828876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32962731473015977\n",
      "    mean_inference_ms: 0.5419344102446331\n",
      "    mean_raw_obs_processing_ms: 0.07582435689889211\n",
      "  time_since_restore: 12.235905647277832\n",
      "  time_this_iter_s: 3.9720983505249023\n",
      "  time_total_s: 12.235905647277832\n",
      "  timers:\n",
      "    learn_throughput: 2018.524\n",
      "    learn_time_ms: 1981.646\n",
      "    load_throughput: 31165107.121\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1085.207\n",
      "    sample_time_ms: 3685.931\n",
      "    update_time_ms: 1.53\n",
      "  timestamp: 1671817565\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:10 (running for 00:00:23.48)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         16.1943</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-116.154</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           570.462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 483.43589743589746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.31078195821432\n",
      "  episode_reward_mean: -115.04463941887997\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 39\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.342600345611572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014165914617478848\n",
      "          model: {}\n",
      "          policy_loss: -0.024110201746225357\n",
      "          total_loss: 699.7661743164062\n",
      "          vf_explained_var: -0.31886932253837585\n",
      "          vf_loss: 699.7860107421875\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.28\n",
      "    ram_util_percent: 20.9\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904261450275846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.327223236207678\n",
      "    mean_inference_ms: 0.5336058620450683\n",
      "    mean_raw_obs_processing_ms: 0.0751198450935775\n",
      "  time_since_restore: 20.16917085647583\n",
      "  time_this_iter_s: 3.9748873710632324\n",
      "  time_total_s: 20.16917085647583\n",
      "  timers:\n",
      "    learn_throughput: 2038.794\n",
      "    learn_time_ms: 1961.944\n",
      "    load_throughput: 28083722.799\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 1051.148\n",
      "    sample_time_ms: 3805.365\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1671817573\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:16 (running for 00:00:29.48)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.1692</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-115.045</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           483.436</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 533.1041666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -99.16475845614572\n",
      "  episode_reward_mean: -114.00788426825024\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 48\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.1425371170043945\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012110638432204723\n",
      "          model: {}\n",
      "          policy_loss: -0.027374638244509697\n",
      "          total_loss: 211.97628784179688\n",
      "          vf_explained_var: -0.3948518633842468\n",
      "          vf_loss: 212.00003051757812\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.86666666666667\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09023781161029043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3265487691918853\n",
      "    mean_inference_ms: 0.5313916133439255\n",
      "    mean_raw_obs_processing_ms: 0.07477130896606463\n",
      "  time_since_restore: 28.067641496658325\n",
      "  time_this_iter_s: 3.969888687133789\n",
      "  time_total_s: 28.067641496658325\n",
      "  timers:\n",
      "    learn_throughput: 2051.306\n",
      "    learn_time_ms: 1949.977\n",
      "    load_throughput: 29913528.273\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 1038.491\n",
      "    sample_time_ms: 3851.743\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671817580\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:22 (running for 00:00:35.42)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         28.0676</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-114.008</td><td style=\"text-align: right;\">            -99.1648</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           533.104</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:28 (running for 00:00:41.41)<br>Memory usage on this node: 6.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         32.0483</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-114.134</td><td style=\"text-align: right;\">            -99.1648</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           575.528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 559.5081967213115\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -93.3107473169906\n",
      "  episode_reward_mean: -112.97977898940303\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 61\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 6.340518951416016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01272121723741293\n",
      "          model: {}\n",
      "          policy_loss: -0.03178858757019043\n",
      "          total_loss: 276.122314453125\n",
      "          vf_explained_var: -0.20616589486598969\n",
      "          vf_loss: 276.1502990722656\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.9\n",
      "    ram_util_percent: 20.900000000000002\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0900462336512511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32602352169803495\n",
      "    mean_inference_ms: 0.5289747059336626\n",
      "    mean_raw_obs_processing_ms: 0.07437019078630032\n",
      "  time_since_restore: 35.98486828804016\n",
      "  time_this_iter_s: 3.9365463256835938\n",
      "  time_total_s: 35.98486828804016\n",
      "  timers:\n",
      "    learn_throughput: 2057.903\n",
      "    learn_time_ms: 1943.726\n",
      "    load_throughput: 30935247.695\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1030.921\n",
      "    sample_time_ms: 3880.024\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671817588\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:33 (running for 00:00:46.71)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         40.3081</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-112.976</td><td style=\"text-align: right;\">            -93.3107</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           583.328</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 590.1805555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -93.3107473169906\n",
      "  episode_reward_mean: -112.58386121206621\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.826778411865234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010720446705818176\n",
      "          model: {}\n",
      "          policy_loss: -0.03029380924999714\n",
      "          total_loss: 118.45962524414062\n",
      "          vf_explained_var: -0.14337755739688873\n",
      "          vf_loss: 118.48670959472656\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.93333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09001495503187394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259977529623879\n",
      "    mean_inference_ms: 0.5279252355991926\n",
      "    mean_raw_obs_processing_ms: 0.07420907827664361\n",
      "  time_since_restore: 44.264172077178955\n",
      "  time_this_iter_s: 3.9560396671295166\n",
      "  time_total_s: 44.264172077178955\n",
      "  timers:\n",
      "    learn_throughput: 2058.69\n",
      "    learn_time_ms: 1942.984\n",
      "    load_throughput: 28841698.47\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 990.724\n",
      "    sample_time_ms: 4037.45\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671817597\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:39 (running for 00:00:52.70)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         44.2642</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-112.584</td><td style=\"text-align: right;\">            -93.3107</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           590.181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:45 (running for 00:00:58.67)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         48.2402</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -112.28</td><td style=\"text-align: right;\">            -92.8687</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           617.473</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 648.0512820512821\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -90.51237783390522\n",
      "  episode_reward_mean: -111.7828716902311\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 78\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.84559965133667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010079648345708847\n",
      "          model: {}\n",
      "          policy_loss: -0.026890454813838005\n",
      "          total_loss: 72.36813354492188\n",
      "          vf_explained_var: -0.16960875689983368\n",
      "          vf_loss: 72.39200592041016\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.46\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08998623904449796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32595240769321676\n",
      "    mean_inference_ms: 0.527495865632787\n",
      "    mean_raw_obs_processing_ms: 0.07407096810865428\n",
      "  time_since_restore: 52.27972197532654\n",
      "  time_this_iter_s: 4.039555311203003\n",
      "  time_total_s: 52.27972197532654\n",
      "  timers:\n",
      "    learn_throughput: 2054.079\n",
      "    learn_time_ms: 1947.344\n",
      "    load_throughput: 28916263.357\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 995.233\n",
      "    sample_time_ms: 4019.158\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671817605\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:50 (running for 00:01:03.80)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         56.3326</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-111.289</td><td style=\"text-align: right;\">            -90.5124</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           650.337</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 683.4651162790698\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -82.5397765114735\n",
      "  episode_reward_mean: -110.52060518893026\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 86\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.670735836029053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009962263517081738\n",
      "          model: {}\n",
      "          policy_loss: -0.026227496564388275\n",
      "          total_loss: 7.788515090942383\n",
      "          vf_explained_var: -0.025038449093699455\n",
      "          vf_loss: 7.811753273010254\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08997941050632202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3260383715740126\n",
      "    mean_inference_ms: 0.5270875035551403\n",
      "    mean_raw_obs_processing_ms: 0.07392809940167157\n",
      "  time_since_restore: 60.36208891868591\n",
      "  time_this_iter_s: 4.02950382232666\n",
      "  time_total_s: 60.36208891868591\n",
      "  timers:\n",
      "    learn_throughput: 2049.261\n",
      "    learn_time_ms: 1951.923\n",
      "    load_throughput: 28455251.018\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 992.732\n",
      "    sample_time_ms: 4029.286\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671817613\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:46:55 (running for 00:01:08.84)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         60.3621</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-110.521</td><td style=\"text-align: right;\">            -82.5398</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           683.465</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:00 (running for 00:01:13.85)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         64.3173</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-109.985</td><td style=\"text-align: right;\">            -77.1683</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           697.258</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-01\n",
      "  done: false\n",
      "  episode_len_mean: 712.7446808510638\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -76.71557696290687\n",
      "  episode_reward_mean: -109.27937672076816\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 94\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.699250221252441\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010482219979166985\n",
      "          model: {}\n",
      "          policy_loss: -0.028056735172867775\n",
      "          total_loss: 144.89393615722656\n",
      "          vf_explained_var: -0.05199733003973961\n",
      "          vf_loss: 144.91885375976562\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.92\n",
      "    ram_util_percent: 21.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08994597713552237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259962971744285\n",
      "    mean_inference_ms: 0.526639405827228\n",
      "    mean_raw_obs_processing_ms: 0.07375674821276192\n",
      "  time_since_restore: 68.20912528038025\n",
      "  time_this_iter_s: 3.8917877674102783\n",
      "  time_total_s: 68.20912528038025\n",
      "  timers:\n",
      "    learn_throughput: 2049.472\n",
      "    learn_time_ms: 1951.722\n",
      "    load_throughput: 27003405.762\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 992.254\n",
      "    sample_time_ms: 4031.227\n",
      "    update_time_ms: 1.35\n",
      "  timestamp: 1671817621\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:06 (running for 00:01:19.63)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         72.0997</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-108.575</td><td style=\"text-align: right;\">            -73.4689</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           731.229</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-09\n",
      "  done: false\n",
      "  episode_len_mean: 757.5555555555555\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -73.46885569225505\n",
      "  episode_reward_mean: -107.87515568687103\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6524786949157715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01055772602558136\n",
      "          model: {}\n",
      "          policy_loss: -0.020433904603123665\n",
      "          total_loss: 2.827256441116333\n",
      "          vf_explained_var: -0.11335103213787079\n",
      "          vf_loss: 2.8445234298706055\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.4\n",
      "    ram_util_percent: 21.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08991644429045768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3259298430147222\n",
      "    mean_inference_ms: 0.526272945707899\n",
      "    mean_raw_obs_processing_ms: 0.07364049724696722\n",
      "  time_since_restore: 76.01431083679199\n",
      "  time_this_iter_s: 3.9145617485046387\n",
      "  time_total_s: 76.01431083679199\n",
      "  timers:\n",
      "    learn_throughput: 2051.69\n",
      "    learn_time_ms: 1949.612\n",
      "    load_throughput: 27064391.031\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 994.731\n",
      "    sample_time_ms: 4021.186\n",
      "    update_time_ms: 1.326\n",
      "  timestamp: 1671817629\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:12 (running for 00:01:25.59)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         76.0143</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-107.875</td><td style=\"text-align: right;\">            -73.4689</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">           757.556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-16\n",
      "  done: false\n",
      "  episode_len_mean: 796.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -66.33643454601743\n",
      "  episode_reward_mean: -106.2590584493743\n",
      "  episode_reward_min: -155.53001021336704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 104\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.63258171081543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007472531404346228\n",
      "          model: {}\n",
      "          policy_loss: -0.02253890037536621\n",
      "          total_loss: 66.96393585205078\n",
      "          vf_explained_var: 0.011790256947278976\n",
      "          vf_loss: 66.9842300415039\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.333333333333336\n",
      "    ram_util_percent: 21.483333333333334\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08982756920547182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3256879623858853\n",
      "    mean_inference_ms: 0.5248371639839405\n",
      "    mean_raw_obs_processing_ms: 0.07328906827995751\n",
      "  time_since_restore: 83.82912540435791\n",
      "  time_this_iter_s: 3.9347105026245117\n",
      "  time_total_s: 83.82912540435791\n",
      "  timers:\n",
      "    learn_throughput: 2076.352\n",
      "    learn_time_ms: 1926.456\n",
      "    load_throughput: 29335925.861\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1006.658\n",
      "    sample_time_ms: 3973.543\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671817636\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:18 (running for 00:01:31.42)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         83.8291</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-106.259</td><td style=\"text-align: right;\">            -66.3364</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">            796.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:23 (running for 00:01:36.43)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         87.7982</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-104.719</td><td style=\"text-align: right;\">            -63.9533</td><td style=\"text-align: right;\">             -155.53</td><td style=\"text-align: right;\">            809.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-24\n",
      "  done: false\n",
      "  episode_len_mean: 853.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -54.738792526859264\n",
      "  episode_reward_mean: -103.31683409865802\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 114\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.570556163787842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011751503683626652\n",
      "          model: {}\n",
      "          policy_loss: -0.01598619855940342\n",
      "          total_loss: 131.1350860595703\n",
      "          vf_explained_var: -0.06928105652332306\n",
      "          vf_loss: 131.1475372314453\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08957001720598605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.324797932973258\n",
      "    mean_inference_ms: 0.5222932823635901\n",
      "    mean_raw_obs_processing_ms: 0.07284877008509741\n",
      "  time_since_restore: 91.70651292800903\n",
      "  time_this_iter_s: 3.9082701206207275\n",
      "  time_total_s: 91.70651292800903\n",
      "  timers:\n",
      "    learn_throughput: 2079.373\n",
      "    learn_time_ms: 1923.656\n",
      "    load_throughput: 30229218.018\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1009.691\n",
      "    sample_time_ms: 3961.607\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671817644\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:28 (running for 00:01:42.24)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         95.6095</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-102.142</td><td style=\"text-align: right;\">            -54.7388</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            868.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 898.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -54.738792526859264\n",
      "  episode_reward_mean: -101.13744371609971\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 118\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.51967191696167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010870070196688175\n",
      "          model: {}\n",
      "          policy_loss: -0.018528630957007408\n",
      "          total_loss: 0.8644792437553406\n",
      "          vf_explained_var: -0.0007182357367128134\n",
      "          vf_loss: 0.8797469735145569\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.88333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08952775586723945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32463043660276375\n",
      "    mean_inference_ms: 0.5216518547757183\n",
      "    mean_raw_obs_processing_ms: 0.07266434132603063\n",
      "  time_since_restore: 99.49541115760803\n",
      "  time_this_iter_s: 3.885935068130493\n",
      "  time_total_s: 99.49541115760803\n",
      "  timers:\n",
      "    learn_throughput: 2088.43\n",
      "    learn_time_ms: 1915.314\n",
      "    load_throughput: 33281523.507\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1015.565\n",
      "    sample_time_ms: 3938.693\n",
      "    update_time_ms: 1.371\n",
      "  timestamp: 1671817652\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:34 (running for 00:01:48.15)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         99.4954</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-101.137</td><td style=\"text-align: right;\">            -54.7388</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            898.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 913.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -97.66475634889761\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 127\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.6290669441223145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009882863610982895\n",
      "          model: {}\n",
      "          policy_loss: -0.024132315069437027\n",
      "          total_loss: 137.83746337890625\n",
      "          vf_explained_var: -0.16362427175045013\n",
      "          vf_loss: 137.858642578125\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.44\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08942282676685015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.324236563520408\n",
      "    mean_inference_ms: 0.5206684116487595\n",
      "    mean_raw_obs_processing_ms: 0.07233281319634166\n",
      "  time_since_restore: 107.3671875\n",
      "  time_this_iter_s: 3.9565773010253906\n",
      "  time_total_s: 107.3671875\n",
      "  timers:\n",
      "    learn_throughput: 2087.898\n",
      "    learn_time_ms: 1915.802\n",
      "    load_throughput: 35454809.806\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1016.84\n",
      "    sample_time_ms: 3933.754\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671817660\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:40 (running for 00:01:54.06)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         107.367</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-97.6648</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            913.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:46 (running for 00:01:59.97)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         111.265</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-97.4661</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">            944.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 959.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -96.38850418733541\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 132\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.67781925201416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009757702238857746\n",
      "          model: {}\n",
      "          policy_loss: -0.026699598878622055\n",
      "          total_loss: 71.39501953125\n",
      "          vf_explained_var: -0.3221835196018219\n",
      "          vf_loss: 71.4188003540039\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.82\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08937594270584129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32405772704503827\n",
      "    mean_inference_ms: 0.5201632334391408\n",
      "    mean_raw_obs_processing_ms: 0.07210594386749487\n",
      "  time_since_restore: 115.14185738563538\n",
      "  time_this_iter_s: 3.877049684524536\n",
      "  time_total_s: 115.14185738563538\n",
      "  timers:\n",
      "    learn_throughput: 2086.093\n",
      "    learn_time_ms: 1917.46\n",
      "    load_throughput: 35454809.806\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1018.064\n",
      "    sample_time_ms: 3929.027\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671817668\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:52 (running for 00:02:05.82)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         119.076</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-95.6204</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">             990.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 1050.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -93.48034450222569\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 138\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.588151931762695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007226522546261549\n",
      "          model: {}\n",
      "          policy_loss: -0.023820700123906136\n",
      "          total_loss: 3.5000650882720947\n",
      "          vf_explained_var: -0.04026513174176216\n",
      "          vf_loss: 3.5217180252075195\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.50000000000001\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08928981283802045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3236918162188872\n",
      "    mean_inference_ms: 0.5197541715239529\n",
      "    mean_raw_obs_processing_ms: 0.07186835872635619\n",
      "  time_since_restore: 123.01969838142395\n",
      "  time_this_iter_s: 3.943251132965088\n",
      "  time_total_s: 123.01969838142395\n",
      "  timers:\n",
      "    learn_throughput: 2085.252\n",
      "    learn_time_ms: 1918.233\n",
      "    load_throughput: 33156553.36\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 1016.128\n",
      "    sample_time_ms: 3936.512\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1671817676\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:47:58 (running for 00:02:11.78)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">          123.02</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-93.4803</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1050.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 1066.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -91.50706391165427\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 143\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.71390438079834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01629435084760189\n",
      "          model: {}\n",
      "          policy_loss: -0.03438657894730568\n",
      "          total_loss: 3.7106854915618896\n",
      "          vf_explained_var: -0.3511037528514862\n",
      "          vf_loss: 3.7401835918426514\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08923912781463027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32346552319924354\n",
      "    mean_inference_ms: 0.5194010221102583\n",
      "    mean_raw_obs_processing_ms: 0.07169286151383675\n",
      "  time_since_restore: 130.8518283367157\n",
      "  time_this_iter_s: 3.8995859622955322\n",
      "  time_total_s: 130.8518283367157\n",
      "  timers:\n",
      "    learn_throughput: 2084.7\n",
      "    learn_time_ms: 1918.741\n",
      "    load_throughput: 33195916.106\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1016.879\n",
      "    sample_time_ms: 3933.606\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1671817684\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:04 (running for 00:02:17.68)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         130.852</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-91.5071</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1066.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:10 (running for 00:02:23.59)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         134.775</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-90.5504</td><td style=\"text-align: right;\">            -49.9368</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1097.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 1121.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -49.93683837701443\n",
      "  episode_reward_mean: -89.6620458623347\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 148\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.524649143218994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007157592568546534\n",
      "          model: {}\n",
      "          policy_loss: -0.024165576323866844\n",
      "          total_loss: 36.132713317871094\n",
      "          vf_explained_var: 0.06165395677089691\n",
      "          vf_loss: 36.154727935791016\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.419999999999995\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08919230764789511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3232424652685692\n",
      "    mean_inference_ms: 0.5190583501604972\n",
      "    mean_raw_obs_processing_ms: 0.07152345890939914\n",
      "  time_since_restore: 138.71032857894897\n",
      "  time_this_iter_s: 3.935013771057129\n",
      "  time_total_s: 138.71032857894897\n",
      "  timers:\n",
      "    learn_throughput: 2084.489\n",
      "    learn_time_ms: 1918.936\n",
      "    load_throughput: 30778235.186\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1015.466\n",
      "    sample_time_ms: 3939.078\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1671817692\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:16 (running for 00:02:29.52)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         142.671</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-87.5761</td><td style=\"text-align: right;\">            -37.5283</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1122.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 1106.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.52829230689759\n",
      "  episode_reward_mean: -86.90324109318887\n",
      "  episode_reward_min: -141.77555981150704\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 162\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.785221576690674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014871382154524326\n",
      "          model: {}\n",
      "          policy_loss: -0.03858889639377594\n",
      "          total_loss: 173.766357421875\n",
      "          vf_explained_var: -0.44147369265556335\n",
      "          vf_loss: 173.8004913330078\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.43333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08908871398688774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32263050751480454\n",
      "    mean_inference_ms: 0.5182105222694524\n",
      "    mean_raw_obs_processing_ms: 0.07113127237885306\n",
      "  time_since_restore: 146.62599563598633\n",
      "  time_this_iter_s: 3.954572916030884\n",
      "  time_total_s: 146.62599563598633\n",
      "  timers:\n",
      "    learn_throughput: 2083.495\n",
      "    learn_time_ms: 1919.851\n",
      "    load_throughput: 30834802.426\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1014.697\n",
      "    sample_time_ms: 3942.065\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1671817700\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:22 (running for 00:02:35.49)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         146.626</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-86.9032</td><td style=\"text-align: right;\">            -37.5283</td><td style=\"text-align: right;\">            -141.776</td><td style=\"text-align: right;\">           1106.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 1136.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.75034602557369\n",
      "  episode_reward_mean: -85.76247162809035\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.453378677368164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009026079438626766\n",
      "          model: {}\n",
      "          policy_loss: -0.02339906617999077\n",
      "          total_loss: 38.5586051940918\n",
      "          vf_explained_var: -0.2259003221988678\n",
      "          vf_loss: 38.57929611206055\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08900630550338098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32219427990818816\n",
      "    mean_inference_ms: 0.5177766772928224\n",
      "    mean_raw_obs_processing_ms: 0.07092894870937845\n",
      "  time_since_restore: 154.49257493019104\n",
      "  time_this_iter_s: 3.918593645095825\n",
      "  time_total_s: 154.49257493019104\n",
      "  timers:\n",
      "    learn_throughput: 2084.948\n",
      "    learn_time_ms: 1918.513\n",
      "    load_throughput: 29157483.49\n",
      "    load_time_ms: 0.137\n",
      "    sample_throughput: 1011.88\n",
      "    sample_time_ms: 3953.037\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1671817707\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:27 (running for 00:02:41.39)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         154.493</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-85.7625</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">              1136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:33 (running for 00:02:47.31)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         158.398</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-84.5074</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1151.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 1167.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.75034602557369\n",
      "  episode_reward_mean: -82.82083281104674\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 174\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.46420955657959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007359570357948542\n",
      "          model: {}\n",
      "          policy_loss: -0.02395291067659855\n",
      "          total_loss: 35.11384963989258\n",
      "          vf_explained_var: -0.09342267364263535\n",
      "          vf_loss: 35.13559341430664\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.51666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08892590505601758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32175675528393244\n",
      "    mean_inference_ms: 0.5173427051614724\n",
      "    mean_raw_obs_processing_ms: 0.07072740040323032\n",
      "  time_since_restore: 162.32040929794312\n",
      "  time_this_iter_s: 3.9223368167877197\n",
      "  time_total_s: 162.32040929794312\n",
      "  timers:\n",
      "    learn_throughput: 2081.768\n",
      "    learn_time_ms: 1921.444\n",
      "    load_throughput: 30891577.978\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1014.081\n",
      "    sample_time_ms: 3944.459\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1671817715\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:39 (running for 00:02:53.18)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         166.227</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-82.2369</td><td style=\"text-align: right;\">            -36.7503</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1145.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-43\n",
      "  done: false\n",
      "  episode_len_mean: 1176.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.765560157990087\n",
      "  episode_reward_mean: -80.81851151701557\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 181\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.274076461791992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014437778852880001\n",
      "          model: {}\n",
      "          policy_loss: -0.03310661390423775\n",
      "          total_loss: 5.485393047332764\n",
      "          vf_explained_var: -0.14664390683174133\n",
      "          vf_loss: 5.514168739318848\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.42\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08883766908698117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32121919434773416\n",
      "    mean_inference_ms: 0.5167200889090546\n",
      "    mean_raw_obs_processing_ms: 0.0705433666350596\n",
      "  time_since_restore: 170.19679284095764\n",
      "  time_this_iter_s: 3.9697389602661133\n",
      "  time_total_s: 170.19679284095764\n",
      "  timers:\n",
      "    learn_throughput: 2084.631\n",
      "    learn_time_ms: 1918.805\n",
      "    load_throughput: 30800837.158\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1012.047\n",
      "    sample_time_ms: 3952.387\n",
      "    update_time_ms: 1.421\n",
      "  timestamp: 1671817723\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:45 (running for 00:02:59.17)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         170.197</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-80.8185</td><td style=\"text-align: right;\">            -27.7656</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1176.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 1191.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23.091330872762892\n",
      "  episode_reward_mean: -77.73011896537314\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 186\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.159714221954346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012011077255010605\n",
      "          model: {}\n",
      "          policy_loss: -0.02221444621682167\n",
      "          total_loss: 1.5564600229263306\n",
      "          vf_explained_var: -0.041677411645650864\n",
      "          vf_loss: 1.575071096420288\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08876312931674915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3207692774913312\n",
      "    mean_inference_ms: 0.5163245799791509\n",
      "    mean_raw_obs_processing_ms: 0.07040139326959698\n",
      "  time_since_restore: 178.09393501281738\n",
      "  time_this_iter_s: 3.946103096008301\n",
      "  time_total_s: 178.09393501281738\n",
      "  timers:\n",
      "    learn_throughput: 2082.499\n",
      "    learn_time_ms: 1920.769\n",
      "    load_throughput: 33354306.163\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.508\n",
      "    sample_time_ms: 3954.492\n",
      "    update_time_ms: 1.415\n",
      "  timestamp: 1671817731\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:51 (running for 00:03:05.10)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         178.094</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-77.7301</td><td style=\"text-align: right;\">            -23.0913</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1191.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:48:57 (running for 00:03:11.00)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         181.985</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-75.8421</td><td style=\"text-align: right;\">            -23.0913</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1206.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 1221.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -19.62076479933533\n",
      "  episode_reward_mean: -74.33538317307747\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 191\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.02078914642334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015727244317531586\n",
      "          model: {}\n",
      "          policy_loss: -0.023728204891085625\n",
      "          total_loss: 1.9430346488952637\n",
      "          vf_explained_var: 0.02868264727294445\n",
      "          vf_loss: 1.9620448350906372\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08870278322258848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203544245783131\n",
      "    mean_inference_ms: 0.5159491320132854\n",
      "    mean_raw_obs_processing_ms: 0.07027817685917336\n",
      "  time_since_restore: 185.90224814414978\n",
      "  time_this_iter_s: 3.917337417602539\n",
      "  time_total_s: 185.90224814414978\n",
      "  timers:\n",
      "    learn_throughput: 2083.988\n",
      "    learn_time_ms: 1919.397\n",
      "    load_throughput: 31312459.873\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1013.576\n",
      "    sample_time_ms: 3946.425\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1671817739\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:03 (running for 00:03:16.90)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         189.849</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-71.8137</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1236.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 1210.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6.688280790192389\n",
      "  episode_reward_mean: -70.97316439791015\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 198\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.046761512756348\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01362482737749815\n",
      "          model: {}\n",
      "          policy_loss: -0.03094792552292347\n",
      "          total_loss: 76.05774688720703\n",
      "          vf_explained_var: -0.08447451144456863\n",
      "          vf_loss: 76.08460998535156\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.68\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08864116376106114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31985836957520053\n",
      "    mean_inference_ms: 0.5155530757627463\n",
      "    mean_raw_obs_processing_ms: 0.07013618998221462\n",
      "  time_since_restore: 193.86620354652405\n",
      "  time_this_iter_s: 4.017568349838257\n",
      "  time_total_s: 193.86620354652405\n",
      "  timers:\n",
      "    learn_throughput: 2082.823\n",
      "    learn_time_ms: 1920.47\n",
      "    load_throughput: 33255135.778\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.872\n",
      "    sample_time_ms: 3953.069\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671817747\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:08 (running for 00:03:21.94)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         193.866</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-70.9732</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1210.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:14 (running for 00:03:27.86)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         197.775</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-69.7793</td><td style=\"text-align: right;\">            -6.68828</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1210.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 1180.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.427794498312586\n",
      "  episode_reward_mean: -69.06477168415154\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 205\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.995788097381592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014605144038796425\n",
      "          model: {}\n",
      "          policy_loss: -0.0337463915348053\n",
      "          total_loss: 212.52139282226562\n",
      "          vf_explained_var: -0.47533535957336426\n",
      "          vf_loss: 212.55075073242188\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.839999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0885989064624983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31943607316315875\n",
      "    mean_inference_ms: 0.515275465843376\n",
      "    mean_raw_obs_processing_ms: 0.07002861092930286\n",
      "  time_since_restore: 201.66234588623047\n",
      "  time_this_iter_s: 3.887784481048584\n",
      "  time_total_s: 201.66234588623047\n",
      "  timers:\n",
      "    learn_throughput: 2083.689\n",
      "    learn_time_ms: 1919.672\n",
      "    load_throughput: 33255135.778\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1011.587\n",
      "    sample_time_ms: 3954.184\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671817755\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:20 (running for 00:03:33.75)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         205.594</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-64.7666</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1225.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 1239.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.34062835792951\n",
      "  episode_reward_mean: -63.02381362011634\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 211\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.003353595733643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012453736737370491\n",
      "          model: {}\n",
      "          policy_loss: -0.025424635037779808\n",
      "          total_loss: 2.5029385089874268\n",
      "          vf_explained_var: -0.07964774966239929\n",
      "          vf_loss: 2.524627208709717\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.449999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08857242088906679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31912124674758896\n",
      "    mean_inference_ms: 0.5149969372410405\n",
      "    mean_raw_obs_processing_ms: 0.06994003873600287\n",
      "  time_since_restore: 209.48947954177856\n",
      "  time_this_iter_s: 3.894988536834717\n",
      "  time_total_s: 209.48947954177856\n",
      "  timers:\n",
      "    learn_throughput: 2081.47\n",
      "    learn_time_ms: 1921.719\n",
      "    load_throughput: 33261728.787\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1013.643\n",
      "    sample_time_ms: 3946.161\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671817763\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:26 (running for 00:03:39.64)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         209.489</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-63.0238</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 1239.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 15.34062835792951\n",
      "  episode_reward_mean: -60.58491780457339\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 215\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.787299633026123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01410067267715931\n",
      "          model: {}\n",
      "          policy_loss: -0.030919622629880905\n",
      "          total_loss: 1.76252019405365\n",
      "          vf_explained_var: 0.20454537868499756\n",
      "          vf_loss: 1.7892097234725952\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08855204311970657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3188956855439976\n",
      "    mean_inference_ms: 0.5148546949569232\n",
      "    mean_raw_obs_processing_ms: 0.06988234613034132\n",
      "  time_since_restore: 217.2959213256836\n",
      "  time_this_iter_s: 3.9238719940185547\n",
      "  time_total_s: 217.2959213256836\n",
      "  timers:\n",
      "    learn_throughput: 2081.736\n",
      "    learn_time_ms: 1921.473\n",
      "    load_throughput: 31920121.766\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1015.232\n",
      "    sample_time_ms: 3939.988\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671817771\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:32 (running for 00:03:45.52)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         217.296</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-60.5849</td><td style=\"text-align: right;\">             15.3406</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:38 (running for 00:03:51.49)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          221.28</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-57.4403</td><td style=\"text-align: right;\">             26.1389</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1239.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 1239.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 26.138924529629477\n",
      "  episode_reward_mean: -55.71052811489122\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 222\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.605030059814453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007656828034669161\n",
      "          model: {}\n",
      "          policy_loss: -0.017413582652807236\n",
      "          total_loss: 12.985599517822266\n",
      "          vf_explained_var: 0.1860601156949997\n",
      "          vf_loss: 13.000716209411621\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.16\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08852847219623758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31858362987500405\n",
      "    mean_inference_ms: 0.5146780290042691\n",
      "    mean_raw_obs_processing_ms: 0.0698057813163448\n",
      "  time_since_restore: 225.16500544548035\n",
      "  time_this_iter_s: 3.8851726055145264\n",
      "  time_total_s: 225.16500544548035\n",
      "  timers:\n",
      "    learn_throughput: 2082.267\n",
      "    learn_time_ms: 1920.983\n",
      "    load_throughput: 31871610.942\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 1013.897\n",
      "    sample_time_ms: 3945.173\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671817778\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:43 (running for 00:03:57.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         229.064</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-53.2495</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1254.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 1254.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -51.18141115083986\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 227\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.40944242477417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00880446471273899\n",
      "          model: {}\n",
      "          policy_loss: -0.024036606773734093\n",
      "          total_loss: 47.472225189208984\n",
      "          vf_explained_var: 0.09751544147729874\n",
      "          vf_loss: 47.49362564086914\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.5\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08851333151689315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3183620317009693\n",
      "    mean_inference_ms: 0.5145564072718624\n",
      "    mean_raw_obs_processing_ms: 0.06975176211422006\n",
      "  time_since_restore: 233.02151489257812\n",
      "  time_this_iter_s: 3.957714796066284\n",
      "  time_total_s: 233.02151489257812\n",
      "  timers:\n",
      "    learn_throughput: 2082.039\n",
      "    learn_time_ms: 1921.194\n",
      "    load_throughput: 31213425.116\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1016.384\n",
      "    sample_time_ms: 3935.522\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1671817786\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:49 (running for 00:04:03.29)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         233.022</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">-51.1814</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1254.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-49-54\n",
      "  done: false\n",
      "  episode_len_mean: 1253.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -44.46430689765319\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 234\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.540538787841797\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015991752967238426\n",
      "          model: {}\n",
      "          policy_loss: -0.02988414466381073\n",
      "          total_loss: 4.635056018829346\n",
      "          vf_explained_var: 0.07719099521636963\n",
      "          vf_loss: 4.660142421722412\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.016666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884995270456065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31807091755187616\n",
      "    mean_inference_ms: 0.5144658583548604\n",
      "    mean_raw_obs_processing_ms: 0.06969026710827342\n",
      "  time_since_restore: 240.8767638206482\n",
      "  time_this_iter_s: 3.9263012409210205\n",
      "  time_total_s: 240.8767638206482\n",
      "  timers:\n",
      "    learn_throughput: 2081.789\n",
      "    learn_time_ms: 1921.424\n",
      "    load_throughput: 31236671.011\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1014.999\n",
      "    sample_time_ms: 3940.891\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1671817794\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:49:55 (running for 00:04:09.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         240.877</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-44.4643</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1253.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:01 (running for 00:04:15.13)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         244.802</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">-42.6506</td><td style=\"text-align: right;\">             45.8675</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1253.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 1253.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 45.86751336097218\n",
      "  episode_reward_mean: -40.60776168155492\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 238\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.174491882324219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015352150425314903\n",
      "          model: {}\n",
      "          policy_loss: -0.029460906982421875\n",
      "          total_loss: 2.613389492034912\n",
      "          vf_explained_var: 0.3502749800682068\n",
      "          vf_loss: 2.638245105743408\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.56666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849483272913952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31792097871504493\n",
      "    mean_inference_ms: 0.5144068310671248\n",
      "    mean_raw_obs_processing_ms: 0.06965775927938185\n",
      "  time_since_restore: 248.67386150360107\n",
      "  time_this_iter_s: 3.8721771240234375\n",
      "  time_total_s: 248.67386150360107\n",
      "  timers:\n",
      "    learn_throughput: 2082.564\n",
      "    learn_time_ms: 1920.709\n",
      "    load_throughput: 31283266.828\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1015.623\n",
      "    sample_time_ms: 3938.471\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1671817802\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:07 (running for 00:04:20.94)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         252.571</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">-36.0504</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1268.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-10\n",
      "  done: false\n",
      "  episode_len_mean: 1259.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 59.349831344597646\n",
      "  episode_reward_mean: -35.56800696829644\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 244\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.148664474487305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014093538746237755\n",
      "          model: {}\n",
      "          policy_loss: -0.02851599082350731\n",
      "          total_loss: 103.91250610351562\n",
      "          vf_explained_var: -0.198480486869812\n",
      "          vf_loss: 103.93679809570312\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.81666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848786639045167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31769911668767564\n",
      "    mean_inference_ms: 0.5143226511169634\n",
      "    mean_raw_obs_processing_ms: 0.06961498084197597\n",
      "  time_since_restore: 256.45938372612\n",
      "  time_this_iter_s: 3.888091802597046\n",
      "  time_total_s: 256.45938372612\n",
      "  timers:\n",
      "    learn_throughput: 2084.231\n",
      "    learn_time_ms: 1919.173\n",
      "    load_throughput: 30620945.428\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1016.217\n",
      "    sample_time_ms: 3936.166\n",
      "    update_time_ms: 1.383\n",
      "  timestamp: 1671817810\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:13 (running for 00:04:26.85)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         256.459</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -35.568</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1259.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 1266.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 59.349831344597646\n",
      "  episode_reward_mean: -29.967131842990476\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 249\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.075194835662842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012313319370150566\n",
      "          model: {}\n",
      "          policy_loss: -0.02818615362048149\n",
      "          total_loss: 3.5164294242858887\n",
      "          vf_explained_var: 0.21115973591804504\n",
      "          vf_loss: 3.540921688079834\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.96\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848347994318456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31751964413151584\n",
      "    mean_inference_ms: 0.5142606672260608\n",
      "    mean_raw_obs_processing_ms: 0.06958266686539079\n",
      "  time_since_restore: 264.33310055732727\n",
      "  time_this_iter_s: 3.9442903995513916\n",
      "  time_total_s: 264.33310055732727\n",
      "  timers:\n",
      "    learn_throughput: 2085.22\n",
      "    learn_time_ms: 1918.263\n",
      "    load_throughput: 30676935.454\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1015.771\n",
      "    sample_time_ms: 3937.896\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671817818\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:19 (running for 00:04:32.77)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         264.333</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-29.9671</td><td style=\"text-align: right;\">             59.3498</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1266.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:25 (running for 00:04:38.73)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         268.245</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-26.3631</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1281.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 1296.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 63.43249696450801\n",
      "  episode_reward_mean: -23.34879778948653\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 255\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9810478687286377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011981776915490627\n",
      "          model: {}\n",
      "          policy_loss: -0.03165898099541664\n",
      "          total_loss: 69.1121826171875\n",
      "          vf_explained_var: -0.1586046814918518\n",
      "          vf_loss: 69.14025115966797\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71666666666667\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847466528183996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172810237653776\n",
      "    mean_inference_ms: 0.514202408463161\n",
      "    mean_raw_obs_processing_ms: 0.06953440993458407\n",
      "  time_since_restore: 272.14142894744873\n",
      "  time_this_iter_s: 3.896162748336792\n",
      "  time_total_s: 272.14142894744873\n",
      "  timers:\n",
      "    learn_throughput: 2084.582\n",
      "    learn_time_ms: 1918.85\n",
      "    load_throughput: 31242487.896\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1017.065\n",
      "    sample_time_ms: 3932.883\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1671817826\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:31 (running for 00:04:44.56)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         276.072</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-18.4308</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1342.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 1357.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 63.43249696450801\n",
      "  episode_reward_mean: -15.801512978616811\n",
      "  episode_reward_min: -244.4461122413696\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 261\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9935495853424072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017227336764335632\n",
      "          model: {}\n",
      "          policy_loss: -0.03877100348472595\n",
      "          total_loss: 5.365960121154785\n",
      "          vf_explained_var: 0.013474843464791775\n",
      "          vf_loss: 5.399562835693359\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.199999999999996\n",
      "    ram_util_percent: 21.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845637825367775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3169992776024903\n",
      "    mean_inference_ms: 0.5141770519209721\n",
      "    mean_raw_obs_processing_ms: 0.06946730010273176\n",
      "  time_since_restore: 280.0361702442169\n",
      "  time_this_iter_s: 3.9646129608154297\n",
      "  time_total_s: 280.0361702442169\n",
      "  timers:\n",
      "    learn_throughput: 2087.077\n",
      "    learn_time_ms: 1916.556\n",
      "    load_throughput: 31225043.737\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1016.139\n",
      "    sample_time_ms: 3936.471\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1671817834\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:36 (running for 00:04:49.58)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         280.036</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">-15.8015</td><td style=\"text-align: right;\">             63.4325</td><td style=\"text-align: right;\">            -244.446</td><td style=\"text-align: right;\">           1357.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 1358.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 64.78455726691469\n",
      "  episode_reward_mean: -9.301243388379138\n",
      "  episode_reward_min: -130.54265340423893\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 268\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9229013919830322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01375757809728384\n",
      "          model: {}\n",
      "          policy_loss: -0.0276393610984087\n",
      "          total_loss: 6.5091071128845215\n",
      "          vf_explained_var: 0.14577075839042664\n",
      "          vf_loss: 6.532619476318359\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.833333333333336\n",
      "    ram_util_percent: 21.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845602328910795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3167876133884678\n",
      "    mean_inference_ms: 0.5140535596516571\n",
      "    mean_raw_obs_processing_ms: 0.06941835891992677\n",
      "  time_since_restore: 287.8343183994293\n",
      "  time_this_iter_s: 3.873603105545044\n",
      "  time_total_s: 287.8343183994293\n",
      "  timers:\n",
      "    learn_throughput: 2086.091\n",
      "    learn_time_ms: 1917.462\n",
      "    load_throughput: 30914346.785\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1015.929\n",
      "    sample_time_ms: 3937.281\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671817841\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:41 (running for 00:04:55.38)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         287.834</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-9.30124</td><td style=\"text-align: right;\">             64.7846</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1358.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:46 (running for 00:05:00.40)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         291.816</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">-7.54279</td><td style=\"text-align: right;\">             65.0833</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1328.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-49\n",
      "  done: false\n",
      "  episode_len_mean: 1343.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 65.08328556086369\n",
      "  episode_reward_mean: -5.204492587578983\n",
      "  episode_reward_min: -130.54265340423893\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 275\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.96517276763916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014771141111850739\n",
      "          model: {}\n",
      "          policy_loss: -0.025464273989200592\n",
      "          total_loss: 4.068432807922363\n",
      "          vf_explained_var: 0.14694848656654358\n",
      "          vf_loss: 4.089465618133545\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.160000000000004\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884485542489842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31653920934179924\n",
      "    mean_inference_ms: 0.514003758603512\n",
      "    mean_raw_obs_processing_ms: 0.06936604577335867\n",
      "  time_since_restore: 295.7117919921875\n",
      "  time_this_iter_s: 3.895420789718628\n",
      "  time_total_s: 295.7117919921875\n",
      "  timers:\n",
      "    learn_throughput: 2085.958\n",
      "    learn_time_ms: 1917.584\n",
      "    load_throughput: 32787211.257\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 1014.037\n",
      "    sample_time_ms: 3944.631\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671817849\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:52 (running for 00:05:06.30)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         295.712</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-5.20449</td><td style=\"text-align: right;\">             65.0833</td><td style=\"text-align: right;\">            -130.543</td><td style=\"text-align: right;\">           1343.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 1350.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 0.6298960534320335\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 282\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9469947814941406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015158119611442089\n",
      "          model: {}\n",
      "          policy_loss: -0.04597778245806694\n",
      "          total_loss: 4.7865309715271\n",
      "          vf_explained_var: -0.027983490377664566\n",
      "          vf_loss: 4.827960968017578\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.38\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08844406642948469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163132207600618\n",
      "    mean_inference_ms: 0.5139360663849644\n",
      "    mean_raw_obs_processing_ms: 0.06932037032196385\n",
      "  time_since_restore: 303.5166642665863\n",
      "  time_this_iter_s: 3.8740456104278564\n",
      "  time_total_s: 303.5166642665863\n",
      "  timers:\n",
      "    learn_throughput: 2085.055\n",
      "    learn_time_ms: 1918.415\n",
      "    load_throughput: 34945253.072\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1015.854\n",
      "    sample_time_ms: 3937.575\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671817857\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:50:58 (running for 00:05:12.18)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         303.517</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">0.629896</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1350.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:04 (running for 00:05:18.09)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         307.438</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> 1.32542</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1335.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 1320.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 3.11810677126801\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 289\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.762481927871704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011617136187851429\n",
      "          model: {}\n",
      "          policy_loss: -0.02532050758600235\n",
      "          total_loss: 37.34484100341797\n",
      "          vf_explained_var: -0.009955924935638905\n",
      "          vf_loss: 37.366676330566406\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.7\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08843234273027731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31605917471572353\n",
      "    mean_inference_ms: 0.5138717321196407\n",
      "    mean_raw_obs_processing_ms: 0.06927459917992734\n",
      "  time_since_restore: 311.3688507080078\n",
      "  time_this_iter_s: 3.9305403232574463\n",
      "  time_total_s: 311.3688507080078\n",
      "  timers:\n",
      "    learn_throughput: 2085.981\n",
      "    learn_time_ms: 1917.563\n",
      "    load_throughput: 35010884.808\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1014.747\n",
      "    sample_time_ms: 3941.871\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671817865\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:10 (running for 00:05:23.91)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         315.223</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 4.54826</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1320.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 1320.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 7.049778781008302\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.5318403244018555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015694156289100647\n",
      "          model: {}\n",
      "          policy_loss: -0.030311185866594315\n",
      "          total_loss: 3.240161657333374\n",
      "          vf_explained_var: 0.10720180720090866\n",
      "          vf_loss: 3.2657647132873535\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.349999999999994\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08842581872094493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31589680492527566\n",
      "    mean_inference_ms: 0.5138149539048361\n",
      "    mean_raw_obs_processing_ms: 0.06924785043020959\n",
      "  time_since_restore: 319.15712213516235\n",
      "  time_this_iter_s: 3.9341299533843994\n",
      "  time_total_s: 319.15712213516235\n",
      "  timers:\n",
      "    learn_throughput: 2085.828\n",
      "    learn_time_ms: 1917.703\n",
      "    load_throughput: 35157619.447\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1017.308\n",
      "    sample_time_ms: 3931.944\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1671817873\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:16 (running for 00:05:29.86)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         319.157</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> 7.04978</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1320.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 1332.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 12.043038435633903\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 300\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.590075969696045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008626681752502918\n",
      "          model: {}\n",
      "          policy_loss: -0.023494351655244827\n",
      "          total_loss: 66.61802673339844\n",
      "          vf_explained_var: 0.047501690685749054\n",
      "          vf_loss: 66.6389389038086\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.3\n",
      "    ram_util_percent: 21.54\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841714262782928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31570849494346487\n",
      "    mean_inference_ms: 0.5137098393252156\n",
      "    mean_raw_obs_processing_ms: 0.06921905794884363\n",
      "  time_since_restore: 326.9877076148987\n",
      "  time_this_iter_s: 3.9105162620544434\n",
      "  time_total_s: 326.9877076148987\n",
      "  timers:\n",
      "    learn_throughput: 2086.604\n",
      "    learn_time_ms: 1916.991\n",
      "    load_throughput: 35627980.463\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1016.854\n",
      "    sample_time_ms: 3933.701\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671817881\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:22 (running for 00:05:35.73)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         326.988</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  12.043</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1332.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:28 (running for 00:05:41.71)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         330.945</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 14.6468</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1348.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 1363.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 82.73673732669712\n",
      "  episode_reward_mean: 17.279553096828607\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 304\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.371147394180298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01655833050608635\n",
      "          model: {}\n",
      "          policy_loss: -0.037599917501211166\n",
      "          total_loss: 2.0469319820404053\n",
      "          vf_explained_var: 0.3261086642742157\n",
      "          vf_loss: 2.079564332962036\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.28\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841533842160096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31560493901416325\n",
      "    mean_inference_ms: 0.5136512616529868\n",
      "    mean_raw_obs_processing_ms: 0.06920264124797325\n",
      "  time_since_restore: 334.849826335907\n",
      "  time_this_iter_s: 3.905325412750244\n",
      "  time_total_s: 334.849826335907\n",
      "  timers:\n",
      "    learn_throughput: 2086.024\n",
      "    learn_time_ms: 1917.523\n",
      "    load_throughput: 35757067.349\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1016.844\n",
      "    sample_time_ms: 3933.739\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1671817889\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:34 (running for 00:05:47.58)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         338.774</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 20.3365</td><td style=\"text-align: right;\">             82.7367</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1378.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 1378.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 84.43436195472016\n",
      "  episode_reward_mean: 22.969029955103295\n",
      "  episode_reward_min: -121.79545442837663\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 310\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.3551456928253174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015161244198679924\n",
      "          model: {}\n",
      "          policy_loss: -0.031618840992450714\n",
      "          total_loss: 4.154765605926514\n",
      "          vf_explained_var: 0.26732322573661804\n",
      "          vf_loss: 4.181836128234863\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.76666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840910478398245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31541734426445805\n",
      "    mean_inference_ms: 0.5135946601501473\n",
      "    mean_raw_obs_processing_ms: 0.0691714187171764\n",
      "  time_since_restore: 342.72857213020325\n",
      "  time_this_iter_s: 3.95479416847229\n",
      "  time_total_s: 342.72857213020325\n",
      "  timers:\n",
      "    learn_throughput: 2086.158\n",
      "    learn_time_ms: 1917.401\n",
      "    load_throughput: 35711400.596\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1015.223\n",
      "    sample_time_ms: 3940.021\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671817897\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:40 (running for 00:05:53.55)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         342.729</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  22.969</td><td style=\"text-align: right;\">             84.4344</td><td style=\"text-align: right;\">            -121.795</td><td style=\"text-align: right;\">           1378.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-45\n",
      "  done: false\n",
      "  episode_len_mean: 1337.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.55517015708877\n",
      "  episode_reward_mean: 22.741863724270537\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 317\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.1229748725891113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01905512809753418\n",
      "          model: {}\n",
      "          policy_loss: -0.03943061828613281\n",
      "          total_loss: 197.3453369140625\n",
      "          vf_explained_var: 0.09852447360754013\n",
      "          vf_loss: 197.37904357910156\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.0\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841134010196303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31524261421153005\n",
      "    mean_inference_ms: 0.5135497992250143\n",
      "    mean_raw_obs_processing_ms: 0.06914848247613206\n",
      "  time_since_restore: 350.6093156337738\n",
      "  time_this_iter_s: 3.8700342178344727\n",
      "  time_total_s: 350.6093156337738\n",
      "  timers:\n",
      "    learn_throughput: 2084.066\n",
      "    learn_time_ms: 1919.325\n",
      "    load_throughput: 35681020.842\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1013.871\n",
      "    sample_time_ms: 3945.274\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671817905\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:46 (running for 00:05:59.48)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         350.609</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 22.7419</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:52 (running for 00:06:05.47)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         354.555</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 22.6094</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1321.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-51-52\n",
      "  done: false\n",
      "  episode_len_mean: 1336.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.55517015708877\n",
      "  episode_reward_mean: 26.182590172996548\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 324\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.102459669113159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016271186992526054\n",
      "          model: {}\n",
      "          policy_loss: -0.030102042481303215\n",
      "          total_loss: 3.039541244506836\n",
      "          vf_explained_var: 0.2062022089958191\n",
      "          vf_loss: 3.0647618770599365\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.519999999999996\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840423406211702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3150330092649332\n",
      "    mean_inference_ms: 0.5135363178025186\n",
      "    mean_raw_obs_processing_ms: 0.06911885155435174\n",
      "  time_since_restore: 358.4737141132355\n",
      "  time_this_iter_s: 3.9191460609436035\n",
      "  time_total_s: 358.4737141132355\n",
      "  timers:\n",
      "    learn_throughput: 2085.176\n",
      "    learn_time_ms: 1918.303\n",
      "    load_throughput: 35575097.54\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1012.213\n",
      "    sample_time_ms: 3951.737\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671817912\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:51:57 (running for 00:06:11.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         362.392</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> 27.1643</td><td style=\"text-align: right;\">             95.5552</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-00\n",
      "  done: false\n",
      "  episode_len_mean: 1337.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.87192519142675\n",
      "  episode_reward_mean: 27.973810778232405\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 330\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.965674877166748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025065293535590172\n",
      "          model: {}\n",
      "          policy_loss: -0.03103558160364628\n",
      "          total_loss: 34.40791320800781\n",
      "          vf_explained_var: -0.06598925590515137\n",
      "          vf_loss: 34.43143081665039\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.050000000000004\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840250334760698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31488535179460514\n",
      "    mean_inference_ms: 0.5135181904767774\n",
      "    mean_raw_obs_processing_ms: 0.06909831005593096\n",
      "  time_since_restore: 366.34616017341614\n",
      "  time_this_iter_s: 3.954435348510742\n",
      "  time_total_s: 366.34616017341614\n",
      "  timers:\n",
      "    learn_throughput: 2084.627\n",
      "    learn_time_ms: 1918.808\n",
      "    load_throughput: 34749826.015\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1010.945\n",
      "    sample_time_ms: 3956.693\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1671817920\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:02 (running for 00:06:16.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         366.346</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 27.9738</td><td style=\"text-align: right;\">             95.8719</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1337.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-08\n",
      "  done: false\n",
      "  episode_len_mean: 1293.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 98.22397382335117\n",
      "  episode_reward_mean: 26.26615291229657\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 338\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7701303958892822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012452061288058758\n",
      "          model: {}\n",
      "          policy_loss: -0.03107476234436035\n",
      "          total_loss: 92.08829498291016\n",
      "          vf_explained_var: 0.04693411663174629\n",
      "          vf_loss: 92.11376953125\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.016666666666666\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839209924567124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31464595871681944\n",
      "    mean_inference_ms: 0.5135337305081521\n",
      "    mean_raw_obs_processing_ms: 0.06906641572182744\n",
      "  time_since_restore: 374.2050111293793\n",
      "  time_this_iter_s: 3.914933919906616\n",
      "  time_total_s: 374.2050111293793\n",
      "  timers:\n",
      "    learn_throughput: 2083.838\n",
      "    learn_time_ms: 1919.535\n",
      "    load_throughput: 34044675.325\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1011.326\n",
      "    sample_time_ms: 3955.203\n",
      "    update_time_ms: 1.347\n",
      "  timestamp: 1671817928\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:08 (running for 00:06:22.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         374.205</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 26.2662</td><td style=\"text-align: right;\">              98.224</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1293.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:14 (running for 00:06:28.19)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          378.16</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 26.5099</td><td style=\"text-align: right;\">             100.146</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1289.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 1298.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.14576247609915\n",
      "  episode_reward_mean: 28.713717585349382\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 343\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.965895175933838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01864263042807579\n",
      "          model: {}\n",
      "          policy_loss: -0.020977802574634552\n",
      "          total_loss: 5.930751323699951\n",
      "          vf_explained_var: 0.009173193946480751\n",
      "          vf_loss: 5.9433393478393555\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.36\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883943941562514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3145466538529968\n",
      "    mean_inference_ms: 0.5135458459970331\n",
      "    mean_raw_obs_processing_ms: 0.06905561437819846\n",
      "  time_since_restore: 382.090119600296\n",
      "  time_this_iter_s: 3.929793119430542\n",
      "  time_total_s: 382.090119600296\n",
      "  timers:\n",
      "    learn_throughput: 2083.829\n",
      "    learn_time_ms: 1919.544\n",
      "    load_throughput: 34183406.683\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1010.887\n",
      "    sample_time_ms: 3956.92\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671817936\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:20 (running for 00:06:34.12)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          382.09</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 28.7137</td><td style=\"text-align: right;\">             100.146</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">            1298.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 1269.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 26.95440202121816\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 347\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.859990119934082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009421590715646744\n",
      "          model: {}\n",
      "          policy_loss: -0.03246340900659561\n",
      "          total_loss: 120.37698364257812\n",
      "          vf_explained_var: -0.0648389607667923\n",
      "          vf_loss: 120.40519714355469\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.66666666666667\n",
      "    ram_util_percent: 22.522222222222222\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841430288993317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.314506742940488\n",
      "    mean_inference_ms: 0.5137131853452284\n",
      "    mean_raw_obs_processing_ms: 0.06906644618795137\n",
      "  time_since_restore: 388.18377685546875\n",
      "  time_this_iter_s: 6.0936572551727295\n",
      "  time_total_s: 388.18377685546875\n",
      "  timers:\n",
      "    learn_throughput: 1992.88\n",
      "    learn_time_ms: 2007.146\n",
      "    load_throughput: 33608205.128\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 981.068\n",
      "    sample_time_ms: 4077.188\n",
      "    update_time_ms: 1.345\n",
      "  timestamp: 1671817942\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:25 (running for 00:06:39.27)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         388.184</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> 26.9544</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1269.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 1239.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 24.636133543856154\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 351\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.856428623199463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011604920960962772\n",
      "          model: {}\n",
      "          policy_loss: -0.03069290705025196\n",
      "          total_loss: 32.55900955200195\n",
      "          vf_explained_var: -0.23071302473545074\n",
      "          vf_loss: 32.58448028564453\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.3125\n",
      "    ram_util_percent: 23.175\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845102498724286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.314541459659209\n",
      "    mean_inference_ms: 0.5139031788887232\n",
      "    mean_raw_obs_processing_ms: 0.0690889812750087\n",
      "  time_since_restore: 393.62455773353577\n",
      "  time_this_iter_s: 5.440780878067017\n",
      "  time_total_s: 393.62455773353577\n",
      "  timers:\n",
      "    learn_throughput: 1891.266\n",
      "    learn_time_ms: 2114.986\n",
      "    load_throughput: 31254128.167\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 949.337\n",
      "    sample_time_ms: 4213.467\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1671817948\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:31 (running for 00:06:44.75)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:36 (running for 00:06:49.77)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:41 (running for 00:06:54.78)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:46 (running for 00:06:59.78)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:51 (running for 00:07:04.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:52:56 (running for 00:07:09.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:01 (running for 00:07:14.79)<br>Memory usage on this node: 7.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:06 (running for 00:07:19.80)<br>Memory usage on this node: 7.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:11 (running for 00:07:24.80)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:16 (running for 00:07:29.81)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:21 (running for 00:07:34.81)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:26 (running for 00:07:39.82)<br>Memory usage on this node: 7.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:31 (running for 00:07:44.82)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:36 (running for 00:07:49.82)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:41 (running for 00:07:54.83)<br>Memory usage on this node: 7.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:46 (running for 00:07:59.83)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:51 (running for 00:08:04.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:53:56 (running for 00:08:09.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:01 (running for 00:08:14.84)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:06 (running for 00:08:19.85)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:11 (running for 00:08:24.85)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:16 (running for 00:08:29.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:21 (running for 00:08:34.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:26 (running for 00:08:39.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:31 (running for 00:08:44.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:36 (running for 00:08:49.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:41 (running for 00:08:54.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:46 (running for 00:08:59.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:51 (running for 00:09:04.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         393.625</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 24.6361</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1239.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-54-53\n",
      "  done: false\n",
      "  episode_len_mean: 1254.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 100.52728852124393\n",
      "  episode_reward_mean: 27.447117710517574\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 355\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1452.11\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 118.75265453299521\n",
      "    episode_reward_mean: 74.1810856568662\n",
      "    episode_reward_min: -127.47615840311785\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 153\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 130\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 131\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 143\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 154\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 52\n",
      "      - 112\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 153\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 125\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 58\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      episode_reward:\n",
      "      - 78.42400498591114\n",
      "      - 107.23675819045734\n",
      "      - 90.93516519455292\n",
      "      - 96.29003044224909\n",
      "      - 83.25675050320302\n",
      "      - 98.42055566307404\n",
      "      - -125.67074016297298\n",
      "      - 82.66246388052717\n",
      "      - 73.6163366743246\n",
      "      - 93.14487152264573\n",
      "      - 90.99009892890544\n",
      "      - 101.14873689156472\n",
      "      - 100.37171714548445\n",
      "      - 87.65793576411654\n",
      "      - 102.73661764596413\n",
      "      - 97.48697272011198\n",
      "      - 92.37115546542333\n",
      "      - 71.79903481600685\n",
      "      - 102.69915795307898\n",
      "      - 89.70385368558587\n",
      "      - 108.21168169774998\n",
      "      - 104.58860545074182\n",
      "      - 108.85922607881567\n",
      "      - 93.04850372730013\n",
      "      - 94.46214578143874\n",
      "      - 105.5815265290149\n",
      "      - 103.93626015781228\n",
      "      - 94.46124004799886\n",
      "      - 101.73515394977915\n",
      "      - 91.42912422639503\n",
      "      - 98.32846556951621\n",
      "      - 106.82301206907009\n",
      "      - 104.17844113640936\n",
      "      - -119.38517284958189\n",
      "      - 106.49715388046461\n",
      "      - 90.70044804339733\n",
      "      - 89.89525145823502\n",
      "      - 89.3188674078093\n",
      "      - 104.64142995106285\n",
      "      - 118.75265453299521\n",
      "      - -122.31440798999245\n",
      "      - 89.63611964106177\n",
      "      - 96.73155319788248\n",
      "      - 99.73832102659532\n",
      "      - 102.7526802937179\n",
      "      - -127.47615840311785\n",
      "      - 114.32462048831073\n",
      "      - 103.33425487037945\n",
      "      - 80.08380470716585\n",
      "      - 106.66913633388711\n",
      "      - -99.51674852522524\n",
      "      - 98.91251943954467\n",
      "      - 108.02227857067986\n",
      "      - 101.4076395165392\n",
      "      - 87.68512411140026\n",
      "      - 91.10292231800081\n",
      "      - 93.99477381535439\n",
      "      - 93.76894666821578\n",
      "      - 89.01957975356382\n",
      "      - 91.17493271486687\n",
      "      - 99.326762892497\n",
      "      - 81.75766606957464\n",
      "      - 101.93966538078844\n",
      "      - 110.02411686724314\n",
      "      - 93.79365284988405\n",
      "      - 85.72561475869279\n",
      "      - 84.24794539385721\n",
      "      - 101.73572315276219\n",
      "      - 80.56923031228351\n",
      "      - 107.52205106860306\n",
      "      - 79.44881992745773\n",
      "      - 80.50545993340666\n",
      "      - 94.30111625185607\n",
      "      - 83.57395171995758\n",
      "      - 92.02572160524632\n",
      "      - 93.11395268752918\n",
      "      - 96.04979773938652\n",
      "      - -111.41583665737075\n",
      "      - -121.77145806111768\n",
      "      - 98.76851986086159\n",
      "      - 93.78064463444262\n",
      "      - 91.8520579333385\n",
      "      - 94.70078801878975\n",
      "      - 70.26692126700037\n",
      "      - -126.26975751312017\n",
      "      - 98.54965751751871\n",
      "      - 97.7366344049263\n",
      "      - 106.6287382263062\n",
      "      - 97.37112949296268\n",
      "      - -122.89920833309927\n",
      "      - 98.81055969419086\n",
      "      - 107.42172554441053\n",
      "      - 89.44662875839563\n",
      "      - 108.55835810110347\n",
      "      - 95.82789523768237\n",
      "      - 113.01186725281181\n",
      "      - -112.43619089618822\n",
      "      - 83.7898624140036\n",
      "      - 96.89196559201505\n",
      "      - 93.42842528026061\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08747001897828019\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.31139213375600866\n",
      "      mean_inference_ms: 0.5059533663250226\n",
      "      mean_raw_obs_processing_ms: 0.060765717905211264\n",
      "    timesteps_this_iter: 145211\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7035062313079834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013569644652307034\n",
      "          model: {}\n",
      "          policy_loss: -0.03527693450450897\n",
      "          total_loss: 6.165103435516357\n",
      "          vf_explained_var: -0.015572791919112206\n",
      "          vf_loss: 6.194273948669434\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.889320388349514\n",
      "    ram_util_percent: 26.581067961165047\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848439734840817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31454508925820396\n",
      "    mean_inference_ms: 0.514181550427439\n",
      "    mean_raw_obs_processing_ms: 0.06911087113896594\n",
      "  time_since_restore: 538.7042179107666\n",
      "  time_this_iter_s: 145.07966017723083\n",
      "  time_total_s: 538.7042179107666\n",
      "  timers:\n",
      "    learn_throughput: 1888.195\n",
      "    learn_time_ms: 2118.425\n",
      "    load_throughput: 31277434.75\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 916.341\n",
      "    sample_time_ms: 4365.189\n",
      "    update_time_ms: 1.99\n",
      "  timestamp: 1671818093\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=164149)\u001b[0m 2022-12-23 17:54:53,423\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:54:56 (running for 00:09:09.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         538.704</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 27.4471</td><td style=\"text-align: right;\">             100.527</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1254.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-01\n",
      "  done: false\n",
      "  episode_len_mean: 1255.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.30952588411367\n",
      "  episode_reward_mean: 28.91912529569661\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 360\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.608483076095581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013637454248964787\n",
      "          model: {}\n",
      "          policy_loss: -0.03710879758000374\n",
      "          total_loss: 2.845229387283325\n",
      "          vf_explained_var: 0.2855461537837982\n",
      "          vf_loss: 2.8762013912200928\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.1\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08854210489814651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31463197908933654\n",
      "    mean_inference_ms: 0.5144904492704786\n",
      "    mean_raw_obs_processing_ms: 0.06915064511540976\n",
      "  time_since_restore: 546.6923115253448\n",
      "  time_this_iter_s: 3.964552879333496\n",
      "  time_total_s: 546.6923115253448\n",
      "  timers:\n",
      "    learn_throughput: 1884.059\n",
      "    learn_time_ms: 2123.075\n",
      "    load_throughput: 29579012.694\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 216.732\n",
      "    sample_time_ms: 18455.966\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818101\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:02 (running for 00:09:15.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         546.692</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> 28.9191</td><td style=\"text-align: right;\">              102.31</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1255.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:07 (running for 00:09:20.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         550.685</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> 30.0872</td><td style=\"text-align: right;\">             110.141</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1255.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 1271.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 34.109509613645265\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 366\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.449204206466675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01432492770254612\n",
      "          model: {}\n",
      "          policy_loss: -0.029611118137836456\n",
      "          total_loss: 4.188887119293213\n",
      "          vf_explained_var: 0.3925740122795105\n",
      "          vf_loss: 4.212051868438721\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.0\n",
      "    ram_util_percent: 29.1\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08859707403800515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3146608489513967\n",
      "    mean_inference_ms: 0.5149097180035009\n",
      "    mean_raw_obs_processing_ms: 0.06918873553670771\n",
      "  time_since_restore: 554.6946558952332\n",
      "  time_this_iter_s: 4.010099649429321\n",
      "  time_total_s: 554.6946558952332\n",
      "  timers:\n",
      "    learn_throughput: 1880.836\n",
      "    learn_time_ms: 2126.714\n",
      "    load_throughput: 29526955.297\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 216.619\n",
      "    sample_time_ms: 18465.585\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1671818109\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:12 (running for 00:09:25.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         554.695</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> 34.1095</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1271.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 1288.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 37.64746195318608\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 371\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.6842358112335205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012124155648052692\n",
      "          model: {}\n",
      "          policy_loss: -0.03358088433742523\n",
      "          total_loss: 105.49585723876953\n",
      "          vf_explained_var: -0.054968222975730896\n",
      "          vf_loss: 105.52398681640625\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.85\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08864888389176645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3147146782838911\n",
      "    mean_inference_ms: 0.5152573638326803\n",
      "    mean_raw_obs_processing_ms: 0.06922438712828831\n",
      "  time_since_restore: 562.7095308303833\n",
      "  time_this_iter_s: 3.955777168273926\n",
      "  time_total_s: 562.7095308303833\n",
      "  timers:\n",
      "    learn_throughput: 1877.663\n",
      "    learn_time_ms: 2130.307\n",
      "    load_throughput: 29066555.787\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 216.446\n",
      "    sample_time_ms: 18480.32\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818117\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:17 (running for 00:09:30.98)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">          562.71</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 37.6475</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">            1288.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:22 (running for 00:09:36.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         566.757</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> 40.4139</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1303.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 1303.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 42.662411900685385\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 376\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3780109882354736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0168222114443779\n",
      "          model: {}\n",
      "          policy_loss: -0.03533028066158295\n",
      "          total_loss: 3.2900192737579346\n",
      "          vf_explained_var: 0.43231603503227234\n",
      "          vf_loss: 3.317779541015625\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.02\n",
      "    ram_util_percent: 29.140000000000004\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08870234566630175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31478270995944857\n",
      "    mean_inference_ms: 0.5156129435019607\n",
      "    mean_raw_obs_processing_ms: 0.06926023836408315\n",
      "  time_since_restore: 570.7154223918915\n",
      "  time_this_iter_s: 3.9586386680603027\n",
      "  time_total_s: 570.7154223918915\n",
      "  timers:\n",
      "    learn_throughput: 1956.578\n",
      "    learn_time_ms: 2044.386\n",
      "    load_throughput: 29433712.281\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 217.795\n",
      "    sample_time_ms: 18365.882\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1671818125\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:27 (running for 00:09:41.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         570.715</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> 42.6624</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1303.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:33 (running for 00:09:47.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         574.721</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> 47.1577</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -125.023</td><td style=\"text-align: right;\">           1333.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 1317.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 46.35326511237408\n",
      "  episode_reward_min: -125.0230157165937\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 382\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3456103801727295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012088745832443237\n",
      "          model: {}\n",
      "          policy_loss: -0.027218114584684372\n",
      "          total_loss: 15.973329544067383\n",
      "          vf_explained_var: 0.5255213975906372\n",
      "          vf_loss: 15.99510669708252\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.25\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08876904685309162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3148755878973006\n",
      "    mean_inference_ms: 0.5160532088311155\n",
      "    mean_raw_obs_processing_ms: 0.0693070653179218\n",
      "  time_since_restore: 578.8327028751373\n",
      "  time_this_iter_s: 4.1118996143341064\n",
      "  time_total_s: 578.8327028751373\n",
      "  timers:\n",
      "    learn_throughput: 2057.197\n",
      "    learn_time_ms: 1944.393\n",
      "    load_throughput: 30126083.678\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 221.015\n",
      "    sample_time_ms: 18098.314\n",
      "    update_time_ms: 1.381\n",
      "  timestamp: 1671818133\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:38 (running for 00:09:52.28)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         582.919</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  43.367</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1287.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 1288.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 119.97948814402007\n",
      "  episode_reward_mean: 44.204749594958514\n",
      "  episode_reward_min: -126.94262364739367\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 390\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0511093139648438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01803765259683132\n",
      "          model: {}\n",
      "          policy_loss: -0.02746838890016079\n",
      "          total_loss: 25.28322410583496\n",
      "          vf_explained_var: 0.09650327265262604\n",
      "          vf_loss: 25.302576065063477\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.76666666666666\n",
      "    ram_util_percent: 29.266666666666666\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08886839731248371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31505139462702675\n",
      "    mean_inference_ms: 0.516656544803917\n",
      "    mean_raw_obs_processing_ms: 0.06937737613741662\n",
      "  time_since_restore: 586.9022233486176\n",
      "  time_this_iter_s: 3.9833316802978516\n",
      "  time_total_s: 586.9022233486176\n",
      "  timers:\n",
      "    learn_throughput: 2057.613\n",
      "    learn_time_ms: 1944.0\n",
      "    load_throughput: 32183418.377\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 990.281\n",
      "    sample_time_ms: 4039.258\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1671818141\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:43 (running for 00:09:57.29)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         586.902</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> 44.2047</td><td style=\"text-align: right;\">             119.979</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1288.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:49 (running for 00:10:02.46)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         591.024</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  44.088</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -126.943</td><td style=\"text-align: right;\">           1274.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 1259.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 122.6038127516478\n",
      "  episode_reward_mean: 42.91967497064192\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 398\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.22045636177063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013660403899848461\n",
      "          model: {}\n",
      "          policy_loss: -0.033493686467409134\n",
      "          total_loss: 12.973958969116211\n",
      "          vf_explained_var: 0.16314588487148285\n",
      "          vf_loss: 13.001306533813477\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08897132740821215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3152554224220419\n",
      "    mean_inference_ms: 0.5173227308992535\n",
      "    mean_raw_obs_processing_ms: 0.0694548289151114\n",
      "  time_since_restore: 595.0009732246399\n",
      "  time_this_iter_s: 3.976592540740967\n",
      "  time_total_s: 595.0009732246399\n",
      "  timers:\n",
      "    learn_throughput: 2056.918\n",
      "    learn_time_ms: 1944.657\n",
      "    load_throughput: 33097683.961\n",
      "    load_time_ms: 0.121\n",
      "    sample_throughput: 987.892\n",
      "    sample_time_ms: 4049.026\n",
      "    update_time_ms: 1.366\n",
      "  timestamp: 1671818149\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:54 (running for 00:10:07.49)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">          599.05</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> 43.7369</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 122.6038127516478\n",
      "  episode_reward_mean: 44.44832267207842\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 403\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.321965217590332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023172365501523018\n",
      "          model: {}\n",
      "          policy_loss: -0.024016866460442543\n",
      "          total_loss: 2.4006521701812744\n",
      "          vf_explained_var: 0.4318290650844574\n",
      "          vf_loss: 2.4142415523529053\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.650000000000006\n",
      "    ram_util_percent: 29.483333333333334\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08903426221603805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31537496318243635\n",
      "    mean_inference_ms: 0.5177527855697415\n",
      "    mean_raw_obs_processing_ms: 0.06950315083477063\n",
      "  time_since_restore: 603.1596901416779\n",
      "  time_this_iter_s: 4.109773635864258\n",
      "  time_total_s: 603.1596901416779\n",
      "  timers:\n",
      "    learn_throughput: 2049.79\n",
      "    learn_time_ms: 1951.419\n",
      "    load_throughput: 33635156.375\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 985.031\n",
      "    sample_time_ms: 4060.785\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1671818158\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:55:59 (running for 00:10:12.63)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">          603.16</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> 44.4483</td><td style=\"text-align: right;\">             122.604</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:04 (running for 00:10:17.68)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         607.197</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> 45.6969</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 126.33096261964698\n",
      "  episode_reward_mean: 46.262064415339076\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 409\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.384523630142212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00953748356550932\n",
      "          model: {}\n",
      "          policy_loss: -0.030215207487344742\n",
      "          total_loss: 2.475980520248413\n",
      "          vf_explained_var: 0.5267147421836853\n",
      "          vf_loss: 2.499758005142212\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.94\n",
      "    ram_util_percent: 29.5\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08911128745059836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3155261763532507\n",
      "    mean_inference_ms: 0.5182765675322294\n",
      "    mean_raw_obs_processing_ms: 0.0695636082761876\n",
      "  time_since_restore: 611.1544244289398\n",
      "  time_this_iter_s: 3.957780122756958\n",
      "  time_total_s: 611.1544244289398\n",
      "  timers:\n",
      "    learn_throughput: 2049.188\n",
      "    learn_time_ms: 1951.993\n",
      "    load_throughput: 32097218.29\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 984.111\n",
      "    sample_time_ms: 4064.583\n",
      "    update_time_ms: 1.382\n",
      "  timestamp: 1671818166\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:09 (running for 00:10:22.69)<br>Memory usage on this node: 9.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         611.154</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> 46.2621</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:14 (running for 00:10:27.82)<br>Memory usage on this node: 9.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         615.299</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">   46.71</td><td style=\"text-align: right;\">             126.331</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1259.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 1259.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 47.62177510591871\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 413\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0139198303222656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012564915232360363\n",
      "          model: {}\n",
      "          policy_loss: -0.02764781191945076\n",
      "          total_loss: 1.7638471126556396\n",
      "          vf_explained_var: 0.6364338994026184\n",
      "          vf_loss: 1.7830135822296143\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.62857142857143\n",
      "    ram_util_percent: 30.37142857142857\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08916326587789809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31563540490398123\n",
      "    mean_inference_ms: 0.5186277868653006\n",
      "    mean_raw_obs_processing_ms: 0.06960445057727725\n",
      "  time_since_restore: 619.9019114971161\n",
      "  time_this_iter_s: 4.6027398109436035\n",
      "  time_total_s: 619.9019114971161\n",
      "  timers:\n",
      "    learn_throughput: 1992.444\n",
      "    learn_time_ms: 2007.584\n",
      "    load_throughput: 33614938.89\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 980.603\n",
      "    sample_time_ms: 4079.122\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1671818175\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:19 (running for 00:10:33.34)<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         624.767</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> 52.1212</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1285.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 1301.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 55.581310278733774\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 420\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8927011489868164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023308034986257553\n",
      "          model: {}\n",
      "          policy_loss: -0.02061375416815281\n",
      "          total_loss: 53.472206115722656\n",
      "          vf_explained_var: 0.5795298218727112\n",
      "          vf_loss: 53.47708511352539\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.57142857142858\n",
      "    ram_util_percent: 35.357142857142854\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08927617790716287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31590125253596346\n",
      "    mean_inference_ms: 0.5193653848555152\n",
      "    mean_raw_obs_processing_ms: 0.06969585349552912\n",
      "  time_since_restore: 629.6155297756195\n",
      "  time_this_iter_s: 4.848086595535278\n",
      "  time_total_s: 629.6155297756195\n",
      "  timers:\n",
      "    learn_throughput: 1933.136\n",
      "    learn_time_ms: 2069.177\n",
      "    load_throughput: 31143894.561\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 937.049\n",
      "    sample_time_ms: 4268.718\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1671818184\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:25 (running for 00:10:39.21)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         629.616</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\"> 55.5813</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1301.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:30 (running for 00:10:44.31)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         633.701</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> 54.0808</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1286.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 1286.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 133.203362715587\n",
      "  episode_reward_mean: 54.77455877721565\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 426\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0714166164398193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008197540417313576\n",
      "          model: {}\n",
      "          policy_loss: -0.034332480281591415\n",
      "          total_loss: 22.94219207763672\n",
      "          vf_explained_var: 0.418192982673645\n",
      "          vf_loss: 22.968225479125977\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.666666666666664\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08938626601966489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31618140360249386\n",
      "    mean_inference_ms: 0.5200287931939909\n",
      "    mean_raw_obs_processing_ms: 0.06978515740545982\n",
      "  time_since_restore: 637.7871990203857\n",
      "  time_this_iter_s: 4.0865397453308105\n",
      "  time_total_s: 637.7871990203857\n",
      "  timers:\n",
      "    learn_throughput: 1931.544\n",
      "    learn_time_ms: 2070.882\n",
      "    load_throughput: 30937149.179\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 930.474\n",
      "    sample_time_ms: 4298.885\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1671818192\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:36 (running for 00:10:49.42)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         637.787</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> 54.7746</td><td style=\"text-align: right;\">             133.203</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1286.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 1301.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 58.99895573487629\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 431\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0431289672851562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007684572134166956\n",
      "          model: {}\n",
      "          policy_loss: -0.019729336723685265\n",
      "          total_loss: 17.551807403564453\n",
      "          vf_explained_var: 0.5051132440567017\n",
      "          vf_loss: 17.56375503540039\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.483333333333334\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08947606078744702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163998649183084\n",
      "    mean_inference_ms: 0.5205962877244271\n",
      "    mean_raw_obs_processing_ms: 0.0698571013509692\n",
      "  time_since_restore: 645.8360183238983\n",
      "  time_this_iter_s: 3.9911794662475586\n",
      "  time_total_s: 645.8360183238983\n",
      "  timers:\n",
      "    learn_throughput: 1936.012\n",
      "    learn_time_ms: 2066.103\n",
      "    load_throughput: 28986205.943\n",
      "    load_time_ms: 0.138\n",
      "    sample_throughput: 932.262\n",
      "    sample_time_ms: 4290.639\n",
      "    update_time_ms: 1.449\n",
      "  timestamp: 1671818201\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:41 (running for 00:10:54.51)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         645.836</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">  58.999</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1301.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:46 (running for 00:10:59.57)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         649.878</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\"> 61.6011</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">              1316</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-49\n",
      "  done: false\n",
      "  episode_len_mean: 1346.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 66.37294805165301\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 436\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.177299737930298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01001816801726818\n",
      "          model: {}\n",
      "          policy_loss: -0.02881176397204399\n",
      "          total_loss: 2.413037061691284\n",
      "          vf_explained_var: 0.6113097667694092\n",
      "          vf_loss: 2.4317054748535156\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.43333333333333\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08957580559198954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31667754349457516\n",
      "    mean_inference_ms: 0.5211458338421836\n",
      "    mean_raw_obs_processing_ms: 0.0699359119041447\n",
      "  time_since_restore: 653.9037961959839\n",
      "  time_this_iter_s: 4.026112079620361\n",
      "  time_total_s: 653.9037961959839\n",
      "  timers:\n",
      "    learn_throughput: 1933.809\n",
      "    learn_time_ms: 2068.456\n",
      "    load_throughput: 29641724.382\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 931.427\n",
      "    sample_time_ms: 4294.484\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1671818209\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:51 (running for 00:11:04.62)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         653.904</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> 66.3729</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1346.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:56:56 (running for 00:11:09.72)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         657.987</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> 66.9769</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1346.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 1350.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 69.00297647506746\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 441\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2565758228302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008600729517638683\n",
      "          model: {}\n",
      "          policy_loss: -0.029129138216376305\n",
      "          total_loss: 3.206780195236206\n",
      "          vf_explained_var: 0.5763169527053833\n",
      "          vf_loss: 3.227201223373413\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.92\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0896641123457716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316880790383373\n",
      "    mean_inference_ms: 0.5217383248021558\n",
      "    mean_raw_obs_processing_ms: 0.07000630745383107\n",
      "  time_since_restore: 662.031911611557\n",
      "  time_this_iter_s: 4.044794082641602\n",
      "  time_total_s: 662.031911611557\n",
      "  timers:\n",
      "    learn_throughput: 1989.723\n",
      "    learn_time_ms: 2010.33\n",
      "    load_throughput: 29610335.334\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 933.359\n",
      "    sample_time_ms: 4285.598\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1671818217\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:01 (running for 00:11:14.82)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         666.046</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\"> 70.2796</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1350.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 1379.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 138.2160273659629\n",
      "  episode_reward_mean: 74.54114709397021\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 446\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.204482316970825\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008961320854723454\n",
      "          model: {}\n",
      "          policy_loss: -0.02460744045674801\n",
      "          total_loss: 2.5113046169281006\n",
      "          vf_explained_var: 0.5614314675331116\n",
      "          vf_loss: 2.5268383026123047\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.65\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08975002145732258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3171253615104138\n",
      "    mean_inference_ms: 0.5221938162768166\n",
      "    mean_raw_obs_processing_ms: 0.07007074204522987\n",
      "  time_since_restore: 670.062967300415\n",
      "  time_this_iter_s: 4.016982078552246\n",
      "  time_total_s: 670.062967300415\n",
      "  timers:\n",
      "    learn_throughput: 2053.543\n",
      "    learn_time_ms: 1947.853\n",
      "    load_throughput: 29863325.027\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 978.468\n",
      "    sample_time_ms: 4088.024\n",
      "    update_time_ms: 1.437\n",
      "  timestamp: 1671818225\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:06 (running for 00:11:19.86)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         670.063</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> 74.5411</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1379.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:11 (running for 00:11:24.91)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         674.093</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">  75.123</td><td style=\"text-align: right;\">             138.216</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1379.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 1394.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 78.109069994111\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 452\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2254371643066406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008994014002382755\n",
      "          model: {}\n",
      "          policy_loss: -0.031866997480392456\n",
      "          total_loss: 2.239166498184204\n",
      "          vf_explained_var: 0.5651485323905945\n",
      "          vf_loss: 2.2619271278381348\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.17999999999999\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08981458340251397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172652171623913\n",
      "    mean_inference_ms: 0.5226222166779463\n",
      "    mean_raw_obs_processing_ms: 0.07011941778558727\n",
      "  time_since_restore: 678.1003065109253\n",
      "  time_this_iter_s: 4.007109880447388\n",
      "  time_total_s: 678.1003065109253\n",
      "  timers:\n",
      "    learn_throughput: 2053.903\n",
      "    learn_time_ms: 1947.512\n",
      "    load_throughput: 30012908.766\n",
      "    load_time_ms: 0.133\n",
      "    sample_throughput: 987.277\n",
      "    sample_time_ms: 4051.547\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1671818233\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:16 (running for 00:11:29.94)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">           678.1</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> 78.1091</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1394.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:21 (running for 00:11:35.06)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         682.172</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> 79.0305</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1394.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 1393.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 79.99243735256908\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2322611808776855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0071028308011591434\n",
      "          model: {}\n",
      "          policy_loss: -0.030082108452916145\n",
      "          total_loss: 9.26272201538086\n",
      "          vf_explained_var: 0.5659865736961365\n",
      "          vf_loss: 9.285612106323242\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.1\n",
      "    ram_util_percent: 36.35\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08987236609889492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31738774869238096\n",
      "    mean_inference_ms: 0.5229688329560273\n",
      "    mean_raw_obs_processing_ms: 0.07016066030780642\n",
      "  time_since_restore: 686.2089760303497\n",
      "  time_this_iter_s: 4.037159442901611\n",
      "  time_total_s: 686.2089760303497\n",
      "  timers:\n",
      "    learn_throughput: 2051.849\n",
      "    learn_time_ms: 1949.461\n",
      "    load_throughput: 32084941.671\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 986.064\n",
      "    sample_time_ms: 4056.533\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1671818241\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:26 (running for 00:11:40.19)<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          690.29</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> 80.3943</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-30\n",
      "  done: false\n",
      "  episode_len_mean: 1393.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 81.18645039663168\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 463\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2718379497528076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009212736040353775\n",
      "          model: {}\n",
      "          policy_loss: -0.029092730954289436\n",
      "          total_loss: 2.3715531826019287\n",
      "          vf_explained_var: 0.6008093953132629\n",
      "          vf_loss: 2.3913180828094482\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 34.46666666666667\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08992700932394586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31752120785546484\n",
      "    mean_inference_ms: 0.5232621091556249\n",
      "    mean_raw_obs_processing_ms: 0.07019887886983084\n",
      "  time_since_restore: 694.6848835945129\n",
      "  time_this_iter_s: 4.395348072052002\n",
      "  time_total_s: 694.6848835945129\n",
      "  timers:\n",
      "    learn_throughput: 2030.454\n",
      "    learn_time_ms: 1970.003\n",
      "    load_throughput: 32710501.072\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 980.193\n",
      "    sample_time_ms: 4080.83\n",
      "    update_time_ms: 1.456\n",
      "  timestamp: 1671818250\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:32 (running for 00:11:45.61)<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         694.685</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> 81.1865</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:37 (running for 00:11:50.73)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         698.785</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\"> 81.7178</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1393.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 1380.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 80.53942816985963\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 470\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.139829158782959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00637059798464179\n",
      "          model: {}\n",
      "          policy_loss: -0.033145446330308914\n",
      "          total_loss: 117.55618286132812\n",
      "          vf_explained_var: 0.2730858325958252\n",
      "          vf_loss: 117.58287048339844\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.38333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09000760141093485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3177395588276255\n",
      "    mean_inference_ms: 0.5236669120017611\n",
      "    mean_raw_obs_processing_ms: 0.07025764058603018\n",
      "  time_since_restore: 702.8028135299683\n",
      "  time_this_iter_s: 4.017354726791382\n",
      "  time_total_s: 702.8028135299683\n",
      "  timers:\n",
      "    learn_throughput: 2025.692\n",
      "    learn_time_ms: 1974.634\n",
      "    load_throughput: 32678644.332\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 976.123\n",
      "    sample_time_ms: 4097.843\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1671818258\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:42 (running for 00:11:55.76)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         706.781</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\"> 81.1051</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1380.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 1366.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 141.7852220239368\n",
      "  episode_reward_mean: 79.3544606917004\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 476\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0782907009124756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011249763891100883\n",
      "          model: {}\n",
      "          policy_loss: -0.0308041013777256\n",
      "          total_loss: 12.833398818969727\n",
      "          vf_explained_var: 0.5108848214149475\n",
      "          vf_loss: 12.852811813354492\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.38333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09006364526431637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3178404503990074\n",
      "    mean_inference_ms: 0.5240359090767374\n",
      "    mean_raw_obs_processing_ms: 0.07029820909590243\n",
      "  time_since_restore: 710.8771681785583\n",
      "  time_this_iter_s: 4.095668315887451\n",
      "  time_total_s: 710.8771681785583\n",
      "  timers:\n",
      "    learn_throughput: 2024.169\n",
      "    learn_time_ms: 1976.12\n",
      "    load_throughput: 33900214.185\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 975.37\n",
      "    sample_time_ms: 4101.009\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1671818266\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:47 (running for 00:12:00.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         710.877</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 79.3545</td><td style=\"text-align: right;\">             141.785</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:52 (running for 00:12:05.92)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         714.879</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> 80.1382</td><td style=\"text-align: right;\">             147.251</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 1366.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.25126510115922\n",
      "  episode_reward_mean: 80.40847320797435\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 481\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.20212459564209\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009055829606950283\n",
      "          model: {}\n",
      "          policy_loss: -0.033792540431022644\n",
      "          total_loss: 4.765120029449463\n",
      "          vf_explained_var: 0.5354225039482117\n",
      "          vf_loss: 4.789742946624756\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.96666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09011979590673563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31799487457924824\n",
      "    mean_inference_ms: 0.524307434764039\n",
      "    mean_raw_obs_processing_ms: 0.07034018683605428\n",
      "  time_since_restore: 718.8134818077087\n",
      "  time_this_iter_s: 3.9347705841064453\n",
      "  time_total_s: 718.8134818077087\n",
      "  timers:\n",
      "    learn_throughput: 2025.842\n",
      "    learn_time_ms: 1974.487\n",
      "    load_throughput: 32768000.0\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 976.609\n",
      "    sample_time_ms: 4095.807\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1671818274\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:57:58 (running for 00:12:11.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         718.813</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> 80.4085</td><td style=\"text-align: right;\">             147.251</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1366.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 1400.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 151.26318671870854\n",
      "  episode_reward_mean: 87.09442703855493\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 486\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8513638973236084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008961099199950695\n",
      "          model: {}\n",
      "          policy_loss: -0.028916602954268456\n",
      "          total_loss: 2.6553854942321777\n",
      "          vf_explained_var: 0.5985805988311768\n",
      "          vf_loss: 2.6752288341522217\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.2\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09017036937851768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31811351076414424\n",
      "    mean_inference_ms: 0.524575675215681\n",
      "    mean_raw_obs_processing_ms: 0.07037534665446524\n",
      "  time_since_restore: 726.7648782730103\n",
      "  time_this_iter_s: 3.935490131378174\n",
      "  time_total_s: 726.7648782730103\n",
      "  timers:\n",
      "    learn_throughput: 2031.658\n",
      "    learn_time_ms: 1968.835\n",
      "    load_throughput: 32736031.22\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 980.225\n",
      "    sample_time_ms: 4080.696\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1671818282\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:04 (running for 00:12:17.87)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         726.765</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> 87.0944</td><td style=\"text-align: right;\">             151.263</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1400.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:10 (running for 00:12:23.86)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         730.733</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> 90.4803</td><td style=\"text-align: right;\">             151.263</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">           1414.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 1429.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 151.26318671870854\n",
      "  episode_reward_mean: 93.5864754174947\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 492\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8561146259307861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00982006173580885\n",
      "          model: {}\n",
      "          policy_loss: -0.027513107284903526\n",
      "          total_loss: 2.826796770095825\n",
      "          vf_explained_var: 0.6242074370384216\n",
      "          vf_loss: 2.844367027282715\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.916666666666664\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09022913547509027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31824960634206767\n",
      "    mean_inference_ms: 0.5248798195830648\n",
      "    mean_raw_obs_processing_ms: 0.07041690575770904\n",
      "  time_since_restore: 734.8074824810028\n",
      "  time_this_iter_s: 4.074692487716675\n",
      "  time_total_s: 734.8074824810028\n",
      "  timers:\n",
      "    learn_throughput: 2055.712\n",
      "    learn_time_ms: 1945.798\n",
      "    load_throughput: 32793620.016\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 986.37\n",
      "    sample_time_ms: 4055.273\n",
      "    update_time_ms: 1.419\n",
      "  timestamp: 1671818290\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:15 (running for 00:12:28.99)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         738.827</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> 96.2941</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1443.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-18\n",
      "  done: false\n",
      "  episode_len_mean: 1443.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 96.92724446063967\n",
      "  episode_reward_min: -128.45734317091478\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 496\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9297425746917725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009765749797224998\n",
      "          model: {}\n",
      "          policy_loss: -0.031220557168126106\n",
      "          total_loss: 2.3137872219085693\n",
      "          vf_explained_var: 0.6765412092208862\n",
      "          vf_loss: 2.3351197242736816\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.68333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09026447923978669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31831583233013794\n",
      "    mean_inference_ms: 0.5250834841790782\n",
      "    mean_raw_obs_processing_ms: 0.0704406825602085\n",
      "  time_since_restore: 742.8542339801788\n",
      "  time_this_iter_s: 4.027336597442627\n",
      "  time_total_s: 742.8542339801788\n",
      "  timers:\n",
      "    learn_throughput: 2065.469\n",
      "    learn_time_ms: 1936.606\n",
      "    load_throughput: 32844980.423\n",
      "    load_time_ms: 0.122\n",
      "    sample_throughput: 992.421\n",
      "    sample_time_ms: 4030.548\n",
      "    update_time_ms: 1.443\n",
      "  timestamp: 1671818298\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:20 (running for 00:12:34.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         742.854</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> 96.9272</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -128.457</td><td style=\"text-align: right;\">            1443.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:25 (running for 00:12:39.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         746.891</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> 100.063</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1458.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-26\n",
      "  done: false\n",
      "  episode_len_mean: 1472.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 103.08487672342753\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 502\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9912160634994507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009914674796164036\n",
      "          model: {}\n",
      "          policy_loss: -0.027051905170083046\n",
      "          total_loss: 3.8043265342712402\n",
      "          vf_explained_var: 0.5716634392738342\n",
      "          vf_loss: 3.8213398456573486\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.583333333333336\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09031921564811883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184313046292195\n",
      "    mean_inference_ms: 0.5253832087791459\n",
      "    mean_raw_obs_processing_ms: 0.07047755122723778\n",
      "  time_since_restore: 750.8689427375793\n",
      "  time_this_iter_s: 3.9783523082733154\n",
      "  time_total_s: 750.8689427375793\n",
      "  timers:\n",
      "    learn_throughput: 2066.801\n",
      "    learn_time_ms: 1935.358\n",
      "    load_throughput: 33825032.258\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 994.464\n",
      "    sample_time_ms: 4022.267\n",
      "    update_time_ms: 1.427\n",
      "  timestamp: 1671818306\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:31 (running for 00:12:45.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         754.823</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\"> 101.703</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1467.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-34\n",
      "  done: false\n",
      "  episode_len_mean: 1443.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 98.00017965300813\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 508\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9284090995788574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009837925434112549\n",
      "          model: {}\n",
      "          policy_loss: -0.0339592844247818\n",
      "          total_loss: 117.75109100341797\n",
      "          vf_explained_var: -0.11257561296224594\n",
      "          vf_loss: 117.77509307861328\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.43333333333334\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09037100269325254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31853237660904404\n",
      "    mean_inference_ms: 0.5256610569854435\n",
      "    mean_raw_obs_processing_ms: 0.07051130708633016\n",
      "  time_since_restore: 758.7778739929199\n",
      "  time_this_iter_s: 3.954930067062378\n",
      "  time_total_s: 758.7778739929199\n",
      "  timers:\n",
      "    learn_throughput: 2069.653\n",
      "    learn_time_ms: 1932.691\n",
      "    load_throughput: 35128174.204\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 995.867\n",
      "    sample_time_ms: 4016.602\n",
      "    update_time_ms: 1.422\n",
      "  timestamp: 1671818314\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:37 (running for 00:12:51.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         758.778</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> 98.0002</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:42 (running for 00:12:56.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         762.758</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> 98.8024</td><td style=\"text-align: right;\">             153.854</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 1443.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.8535013854828\n",
      "  episode_reward_mean: 99.2611229595821\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 513\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8944987058639526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009089785628020763\n",
      "          model: {}\n",
      "          policy_loss: -0.03356286883354187\n",
      "          total_loss: 2.626680612564087\n",
      "          vf_explained_var: 0.5540494322776794\n",
      "          vf_loss: 2.6510400772094727\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.6\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09041398069266997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3186345120178261\n",
      "    mean_inference_ms: 0.5258697612000426\n",
      "    mean_raw_obs_processing_ms: 0.07054137089245695\n",
      "  time_since_restore: 766.795618057251\n",
      "  time_this_iter_s: 4.037226676940918\n",
      "  time_total_s: 766.795618057251\n",
      "  timers:\n",
      "    learn_throughput: 2066.691\n",
      "    learn_time_ms: 1935.461\n",
      "    load_throughput: 35142890.658\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 994.604\n",
      "    sample_time_ms: 4021.701\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1671818322\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:47 (running for 00:13:01.14)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         770.817</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">  100.24</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1443.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 1451.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 101.43433429837191\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 519\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9832863807678223\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004306313116103411\n",
      "          model: {}\n",
      "          policy_loss: -0.021216535940766335\n",
      "          total_loss: 49.465518951416016\n",
      "          vf_explained_var: 0.5097894072532654\n",
      "          vf_loss: 49.48237609863281\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.71666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904491522654834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187041977237091\n",
      "    mean_inference_ms: 0.5260234892885487\n",
      "    mean_raw_obs_processing_ms: 0.07056162717787956\n",
      "  time_since_restore: 774.8638536930084\n",
      "  time_this_iter_s: 4.046861171722412\n",
      "  time_total_s: 774.8638536930084\n",
      "  timers:\n",
      "    learn_throughput: 2062.418\n",
      "    learn_time_ms: 1939.471\n",
      "    load_throughput: 35231448.971\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 994.997\n",
      "    sample_time_ms: 4020.113\n",
      "    update_time_ms: 1.424\n",
      "  timestamp: 1671818330\n",
      "  timesteps_since_restore: 632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:52 (running for 00:13:06.25)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         774.864</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> 101.434</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1451.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-58-58\n",
      "  done: false\n",
      "  episode_len_mean: 1466.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 104.73193088521928\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 523\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.263923406600952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015270866453647614\n",
      "          model: {}\n",
      "          policy_loss: -0.036166224628686905\n",
      "          total_loss: 4.279318332672119\n",
      "          vf_explained_var: 0.573413074016571\n",
      "          vf_loss: 4.307753562927246\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.18333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09046305248059394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31870525125412635\n",
      "    mean_inference_ms: 0.5261070717626054\n",
      "    mean_raw_obs_processing_ms: 0.07056570025268895\n",
      "  time_since_restore: 782.8214087486267\n",
      "  time_this_iter_s: 3.960432529449463\n",
      "  time_total_s: 782.8214087486267\n",
      "  timers:\n",
      "    learn_throughput: 2062.162\n",
      "    learn_time_ms: 1939.712\n",
      "    load_throughput: 35069431.438\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 995.878\n",
      "    sample_time_ms: 4016.557\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1671818338\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:58:58 (running for 00:13:12.21)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         782.821</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> 104.732</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:03 (running for 00:13:17.26)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         786.817</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\"> 107.611</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-06\n",
      "  done: false\n",
      "  episode_len_mean: 1481.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.10053406310233\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 529\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0093767642974854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01552146952599287\n",
      "          model: {}\n",
      "          policy_loss: -0.02709922567009926\n",
      "          total_loss: 4.140784740447998\n",
      "          vf_explained_var: 0.49240657687187195\n",
      "          vf_loss: 4.1600260734558105\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.6\n",
      "    ram_util_percent: 29.116666666666664\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0904913419695422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31876409963172736\n",
      "    mean_inference_ms: 0.5261917979684595\n",
      "    mean_raw_obs_processing_ms: 0.07057983689547226\n",
      "  time_since_restore: 790.7967224121094\n",
      "  time_this_iter_s: 3.979743719100952\n",
      "  time_total_s: 790.7967224121094\n",
      "  timers:\n",
      "    learn_throughput: 2064.864\n",
      "    learn_time_ms: 1937.173\n",
      "    load_throughput: 35062102.403\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 996.352\n",
      "    sample_time_ms: 4014.646\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1671818346\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:09 (running for 00:13:23.22)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         790.797</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\"> 108.101</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-14\n",
      "  done: false\n",
      "  episode_len_mean: 1481.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.69614895957615\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 533\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.972563624382019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01606319472193718\n",
      "          model: {}\n",
      "          policy_loss: -0.03846196085214615\n",
      "          total_loss: 2.9623382091522217\n",
      "          vf_explained_var: 0.6525730490684509\n",
      "          vf_loss: 2.992668390274048\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.8\n",
      "    ram_util_percent: 29.1\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09050595957707232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31877946676959834\n",
      "    mean_inference_ms: 0.5262478018596556\n",
      "    mean_raw_obs_processing_ms: 0.0705855764898176\n",
      "  time_since_restore: 798.6506869792938\n",
      "  time_this_iter_s: 3.896947145462036\n",
      "  time_total_s: 798.6506869792938\n",
      "  timers:\n",
      "    learn_throughput: 2063.967\n",
      "    learn_time_ms: 1938.016\n",
      "    load_throughput: 35164988.472\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 997.966\n",
      "    sample_time_ms: 4008.151\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1671818354\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:15 (running for 00:13:29.15)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         798.651</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> 108.696</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">            1481.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:21 (running for 00:13:35.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         802.644</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> 108.425</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 1480.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.48519569951101\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 539\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.140388250350952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017859743908047676\n",
      "          model: {}\n",
      "          policy_loss: -0.03953013941645622\n",
      "          total_loss: 3.592892646789551\n",
      "          vf_explained_var: 0.5088661313056946\n",
      "          vf_loss: 3.6233813762664795\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.083333333333336\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0905256077739236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31879683787417584\n",
      "    mean_inference_ms: 0.5263186908934042\n",
      "    mean_raw_obs_processing_ms: 0.0705929866882034\n",
      "  time_since_restore: 806.6391081809998\n",
      "  time_this_iter_s: 3.9950547218322754\n",
      "  time_total_s: 806.6391081809998\n",
      "  timers:\n",
      "    learn_throughput: 2066.962\n",
      "    learn_time_ms: 1935.207\n",
      "    load_throughput: 35172360.587\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 998.606\n",
      "    sample_time_ms: 4005.586\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671818362\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:26 (running for 00:13:40.17)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         810.647</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> 108.626</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 1466.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 106.66917669276721\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 544\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7537689208984375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018452072516083717\n",
      "          model: {}\n",
      "          policy_loss: -0.024161245673894882\n",
      "          total_loss: 18.26736831665039\n",
      "          vf_explained_var: 0.5194090604782104\n",
      "          vf_loss: 18.28218650817871\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.550000000000004\n",
      "    ram_util_percent: 29.099999999999998\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09053735406430505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187798085477536\n",
      "    mean_inference_ms: 0.5263811552643191\n",
      "    mean_raw_obs_processing_ms: 0.07059400759641116\n",
      "  time_since_restore: 814.6282787322998\n",
      "  time_this_iter_s: 3.9817054271698\n",
      "  time_total_s: 814.6282787322998\n",
      "  timers:\n",
      "    learn_throughput: 2074.156\n",
      "    learn_time_ms: 1928.495\n",
      "    load_throughput: 35216658.27\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 999.4\n",
      "    sample_time_ms: 4002.4\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1671818370\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:31 (running for 00:13:45.18)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         814.628</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> 106.669</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:36 (running for 00:13:50.22)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         818.605</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> 107.758</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1466.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 1480.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 110.35076007801894\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 550\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9181017875671387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014733814634382725\n",
      "          model: {}\n",
      "          policy_loss: -0.0371541827917099\n",
      "          total_loss: 3.881101369857788\n",
      "          vf_explained_var: 0.5383106470108032\n",
      "          vf_loss: 3.910796642303467\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09055357336088968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31878708061463173\n",
      "    mean_inference_ms: 0.5264362797737904\n",
      "    mean_raw_obs_processing_ms: 0.07059913173512816\n",
      "  time_since_restore: 822.5185160636902\n",
      "  time_this_iter_s: 3.9136862754821777\n",
      "  time_total_s: 822.5185160636902\n",
      "  timers:\n",
      "    learn_throughput: 2074.542\n",
      "    learn_time_ms: 1928.137\n",
      "    load_throughput: 35424864.865\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1002.563\n",
      "    sample_time_ms: 3989.773\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1671818378\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:42 (running for 00:13:56.11)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         826.508</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> 110.068</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 1465.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 108.30934537967141\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 555\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.776754379272461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012994810938835144\n",
      "          model: {}\n",
      "          policy_loss: -0.025578750297427177\n",
      "          total_loss: 33.37646484375\n",
      "          vf_explained_var: 0.6455994844436646\n",
      "          vf_loss: 33.39546203613281\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09056516119974745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187866348534209\n",
      "    mean_inference_ms: 0.5264742432615271\n",
      "    mean_raw_obs_processing_ms: 0.07060209768340403\n",
      "  time_since_restore: 830.4624907970428\n",
      "  time_this_iter_s: 3.954704999923706\n",
      "  time_total_s: 830.4624907970428\n",
      "  timers:\n",
      "    learn_throughput: 2074.11\n",
      "    learn_time_ms: 1928.538\n",
      "    load_throughput: 35552481.458\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1003.352\n",
      "    sample_time_ms: 3986.636\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671818386\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:47 (running for 00:14:01.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         830.462</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> 108.309</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1465.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:52 (running for 00:14:06.13)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         834.484</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\"> 111.254</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 1480.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 111.988369270871\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 561\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.834228277206421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01627211645245552\n",
      "          model: {}\n",
      "          policy_loss: -0.03025376982986927\n",
      "          total_loss: 2.6952555179595947\n",
      "          vf_explained_var: 0.7106389999389648\n",
      "          vf_loss: 2.717271566390991\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.0\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058059641339135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3188046131661466\n",
      "    mean_inference_ms: 0.5264979168704504\n",
      "    mean_raw_obs_processing_ms: 0.0706075590408524\n",
      "  time_since_restore: 838.4072070121765\n",
      "  time_this_iter_s: 3.9231433868408203\n",
      "  time_total_s: 838.4072070121765\n",
      "  timers:\n",
      "    learn_throughput: 2072.803\n",
      "    learn_time_ms: 1929.754\n",
      "    load_throughput: 31512426.747\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1001.751\n",
      "    sample_time_ms: 3993.009\n",
      "    update_time_ms: 1.367\n",
      "  timestamp: 1671818394\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:59:58 (running for 00:14:12.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         842.335</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\"> 112.288</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 1480.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 112.5882183347492\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 565\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8180304765701294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015471341088414192\n",
      "          model: {}\n",
      "          policy_loss: -0.03155018761754036\n",
      "          total_loss: 1.9245480298995972\n",
      "          vf_explained_var: 0.7282019257545471\n",
      "          vf_loss: 1.9482659101486206\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.949999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058446192012916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187810892418695\n",
      "    mean_inference_ms: 0.5264982096348566\n",
      "    mean_raw_obs_processing_ms: 0.07060448835664604\n",
      "  time_since_restore: 846.2946693897247\n",
      "  time_this_iter_s: 3.9592814445495605\n",
      "  time_total_s: 846.2946693897247\n",
      "  timers:\n",
      "    learn_throughput: 2072.149\n",
      "    learn_time_ms: 1930.363\n",
      "    load_throughput: 29449211.866\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1003.616\n",
      "    sample_time_ms: 3985.588\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818402\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:04 (running for 00:14:18.00)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         846.295</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\"> 112.588</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1480.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:09 (running for 00:14:23.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         850.267</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> 116.241</td><td style=\"text-align: right;\">             158.306</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1503.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 1503.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.3059190532892\n",
      "  episode_reward_mean: 116.55379401048896\n",
      "  episode_reward_min: -123.87341639392201\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 571\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9036041498184204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016182834282517433\n",
      "          model: {}\n",
      "          policy_loss: -0.03180156648159027\n",
      "          total_loss: 2.4185869693756104\n",
      "          vf_explained_var: 0.6380742788314819\n",
      "          vf_loss: 2.4421958923339844\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.15\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09058909008943206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31874202263951706\n",
      "    mean_inference_ms: 0.5264926271200592\n",
      "    mean_raw_obs_processing_ms: 0.07059898169109878\n",
      "  time_since_restore: 854.2399389743805\n",
      "  time_this_iter_s: 3.972935199737549\n",
      "  time_total_s: 854.2399389743805\n",
      "  timers:\n",
      "    learn_throughput: 2068.367\n",
      "    learn_time_ms: 1933.893\n",
      "    load_throughput: 27786048.36\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 1005.257\n",
      "    sample_time_ms: 3979.084\n",
      "    update_time_ms: 1.376\n",
      "  timestamp: 1671818410\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:15 (running for 00:14:28.96)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         858.191</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\"> 117.298</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -123.873</td><td style=\"text-align: right;\">           1503.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 1504.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 117.94291416281503\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 576\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8034014701843262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012455887161195278\n",
      "          model: {}\n",
      "          policy_loss: -0.030662240460515022\n",
      "          total_loss: 94.1190414428711\n",
      "          vf_explained_var: 0.2867136597633362\n",
      "          vf_loss: 94.14339447021484\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.86\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059190324643213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3187038555524555\n",
      "    mean_inference_ms: 0.526480747905874\n",
      "    mean_raw_obs_processing_ms: 0.07059180751620692\n",
      "  time_since_restore: 862.1637780666351\n",
      "  time_this_iter_s: 3.9727847576141357\n",
      "  time_total_s: 862.1637780666351\n",
      "  timers:\n",
      "    learn_throughput: 2069.059\n",
      "    learn_time_ms: 1933.246\n",
      "    load_throughput: 26677080.617\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 1003.909\n",
      "    sample_time_ms: 3984.426\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671818418\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:20 (running for 00:14:33.98)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         862.164</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\"> 117.943</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1504.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 1504.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 119.3531006627639\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 582\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8471916913986206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016527485102415085\n",
      "          model: {}\n",
      "          policy_loss: -0.031330883502960205\n",
      "          total_loss: 4.180943489074707\n",
      "          vf_explained_var: 0.5633950233459473\n",
      "          vf_loss: 4.203907489776611\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.550000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059446984571924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31865831650570337\n",
      "    mean_inference_ms: 0.5264641335649449\n",
      "    mean_raw_obs_processing_ms: 0.07058383853585332\n",
      "  time_since_restore: 870.082897901535\n",
      "  time_this_iter_s: 3.9546597003936768\n",
      "  time_total_s: 870.082897901535\n",
      "  timers:\n",
      "    learn_throughput: 2069.023\n",
      "    learn_time_ms: 1933.28\n",
      "    load_throughput: 26622050.143\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 1004.564\n",
      "    sample_time_ms: 3981.829\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671818426\n",
      "  timesteps_since_restore: 728000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:26 (running for 00:14:39.91)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         870.083</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\"> 119.353</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1504.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:31 (running for 00:14:44.93)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         874.051</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">  121.98</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">            1516.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-34\n",
      "  done: false\n",
      "  episode_len_mean: 1516.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 162.82701392494062\n",
      "  episode_reward_mean: 122.18547219602313\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 586\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.926087737083435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012157624587416649\n",
      "          model: {}\n",
      "          policy_loss: -0.03196243941783905\n",
      "          total_loss: 3.8844289779663086\n",
      "          vf_explained_var: 0.6053617000579834\n",
      "          vf_loss: 3.9071593284606934\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.21666666666667\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059593612165955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31862850855476105\n",
      "    mean_inference_ms: 0.5264518862128275\n",
      "    mean_raw_obs_processing_ms: 0.07057819592741867\n",
      "  time_since_restore: 878.0074260234833\n",
      "  time_this_iter_s: 3.9566478729248047\n",
      "  time_total_s: 878.0074260234833\n",
      "  timers:\n",
      "    learn_throughput: 2072.032\n",
      "    learn_time_ms: 1930.473\n",
      "    load_throughput: 29475080.815\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 1004.301\n",
      "    sample_time_ms: 3982.87\n",
      "    update_time_ms: 1.375\n",
      "  timestamp: 1671818434\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:37 (running for 00:14:50.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         878.007</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\"> 122.185</td><td style=\"text-align: right;\">             162.827</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">            1516.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 1486.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 118.3570604541922\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 594\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.752101182937622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013806791976094246\n",
      "          model: {}\n",
      "          policy_loss: -0.03381708264350891\n",
      "          total_loss: 43.44443130493164\n",
      "          vf_explained_var: 0.027147194370627403\n",
      "          vf_loss: 43.46776580810547\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.519999999999996\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059776396599885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.318569735323229\n",
      "    mean_inference_ms: 0.5264228995532331\n",
      "    mean_raw_obs_processing_ms: 0.07056633370940191\n",
      "  time_since_restore: 885.9561908245087\n",
      "  time_this_iter_s: 3.961832284927368\n",
      "  time_total_s: 885.9561908245087\n",
      "  timers:\n",
      "    learn_throughput: 2071.082\n",
      "    learn_time_ms: 1931.358\n",
      "    load_throughput: 31553913.861\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1003.637\n",
      "    sample_time_ms: 3985.503\n",
      "    update_time_ms: 1.408\n",
      "  timestamp: 1671818442\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:42 (running for 00:14:55.89)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         885.956</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\"> 118.357</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1486.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:48 (running for 00:15:01.88)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          889.96</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\"> 116.262</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 1471.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 116.57366809175647\n",
      "  episode_reward_min: -122.36796940326505\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 599\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9909077882766724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010573149658739567\n",
      "          model: {}\n",
      "          policy_loss: -0.03185923770070076\n",
      "          total_loss: 2.889963150024414\n",
      "          vf_explained_var: 0.5293506979942322\n",
      "          vf_loss: 2.9137935638427734\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.25000000000001\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09060049494254377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3185562663642656\n",
      "    mean_inference_ms: 0.5263837595468669\n",
      "    mean_raw_obs_processing_ms: 0.07056200636428528\n",
      "  time_since_restore: 893.9568929672241\n",
      "  time_this_iter_s: 3.996941566467285\n",
      "  time_total_s: 893.9568929672241\n",
      "  timers:\n",
      "    learn_throughput: 2071.904\n",
      "    learn_time_ms: 1930.591\n",
      "    load_throughput: 33743395.012\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 1002.248\n",
      "    sample_time_ms: 3991.027\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1671818450\n",
      "  timesteps_since_restore: 752000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:53 (running for 00:15:06.93)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         893.957</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\"> 116.574</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:00:58 (running for 00:15:11.95)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          897.98</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\"> 116.692</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -122.368</td><td style=\"text-align: right;\">           1471.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 1475.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 166.74202091520726\n",
      "  episode_reward_mean: 119.03418156623087\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 606\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.803322672843933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016566909849643707\n",
      "          model: {}\n",
      "          policy_loss: -0.02549314685165882\n",
      "          total_loss: 10.045744895935059\n",
      "          vf_explained_var: 0.5733997225761414\n",
      "          vf_loss: 10.058658599853516\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.22\n",
      "    ram_util_percent: 29.24\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059969853185117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184989378979168\n",
      "    mean_inference_ms: 0.5263445664821316\n",
      "    mean_raw_obs_processing_ms: 0.07055080584504098\n",
      "  time_since_restore: 902.0137250423431\n",
      "  time_this_iter_s: 4.033287525177002\n",
      "  time_total_s: 902.0137250423431\n",
      "  timers:\n",
      "    learn_throughput: 2058.743\n",
      "    learn_time_ms: 1942.933\n",
      "    load_throughput: 35507335.45\n",
      "    load_time_ms: 0.113\n",
      "    sample_throughput: 1000.209\n",
      "    sample_time_ms: 3999.163\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1671818458\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:03 (running for 00:15:17.06)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         906.054</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\"> 121.525</td><td style=\"text-align: right;\">             166.742</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1485.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 1485.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 121.82626207060939\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 610\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.841288685798645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010425618849694729\n",
      "          model: {}\n",
      "          policy_loss: -0.0314665213227272\n",
      "          total_loss: 2.797149181365967\n",
      "          vf_explained_var: 0.5720115900039673\n",
      "          vf_loss: 2.8206984996795654\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.233333333333334\n",
      "    ram_util_percent: 29.25\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09060004873950744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31846940712546506\n",
      "    mean_inference_ms: 0.5263259442314299\n",
      "    mean_raw_obs_processing_ms: 0.07054488348841313\n",
      "  time_since_restore: 910.0619647502899\n",
      "  time_this_iter_s: 4.007719039916992\n",
      "  time_total_s: 910.0619647502899\n",
      "  timers:\n",
      "    learn_throughput: 2056.266\n",
      "    learn_time_ms: 1945.274\n",
      "    load_throughput: 34879866.944\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 995.412\n",
      "    sample_time_ms: 4018.435\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1671818466\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:08 (running for 00:15:22.09)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         910.062</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\"> 121.826</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1485.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:13 (running for 00:15:27.14)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         914.063</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\"> 114.578</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1448.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 1443.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 114.35580726549148\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 618\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8702466487884521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00830814614892006\n",
      "          model: {}\n",
      "          policy_loss: -0.032873667776584625\n",
      "          total_loss: 62.43569564819336\n",
      "          vf_explained_var: -0.05174282565712929\n",
      "          vf_loss: 62.46225357055664\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.980000000000004\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059933871446325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3184087246800651\n",
      "    mean_inference_ms: 0.5262827400018135\n",
      "    mean_raw_obs_processing_ms: 0.07053257983933549\n",
      "  time_since_restore: 918.0408627986908\n",
      "  time_this_iter_s: 3.9775795936584473\n",
      "  time_total_s: 918.0408627986908\n",
      "  timers:\n",
      "    learn_throughput: 2050.138\n",
      "    learn_time_ms: 1951.088\n",
      "    load_throughput: 33777362.593\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 995.368\n",
      "    sample_time_ms: 4018.613\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1671818474\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:19 (running for 00:15:33.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">          922.02</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\"> 111.768</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 1435.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 112.40180741662654\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 624\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7632206678390503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012623617425560951\n",
      "          model: {}\n",
      "          policy_loss: -0.0342831090092659\n",
      "          total_loss: 10.354460716247559\n",
      "          vf_explained_var: 0.32655295729637146\n",
      "          vf_loss: 10.379156112670898\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.78333333333333\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059784362501871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31836265956490506\n",
      "    mean_inference_ms: 0.5262454871493252\n",
      "    mean_raw_obs_processing_ms: 0.07052388873282874\n",
      "  time_since_restore: 925.9963090419769\n",
      "  time_this_iter_s: 3.9758732318878174\n",
      "  time_total_s: 925.9963090419769\n",
      "  timers:\n",
      "    learn_throughput: 2052.089\n",
      "    learn_time_ms: 1949.233\n",
      "    load_throughput: 33804585.936\n",
      "    load_time_ms: 0.118\n",
      "    sample_throughput: 993.742\n",
      "    sample_time_ms: 4025.189\n",
      "    update_time_ms: 1.307\n",
      "  timestamp: 1671818482\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:24 (running for 00:15:38.12)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         925.996</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\"> 112.402</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_a08d1_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_18-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 1435.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.16670273425854\n",
      "  episode_reward_mean: 113.71228094785818\n",
      "  episode_reward_min: -121.24888283536583\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 629\n",
      "  experiment_id: 0357c2d5cbf641ff999e683b4bb30628\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6843246221542358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012338604778051376\n",
      "          model: {}\n",
      "          policy_loss: -0.03559298440814018\n",
      "          total_loss: 3.518728494644165\n",
      "          vf_explained_var: 0.5752816796302795\n",
      "          vf_loss: 3.544951915740967\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.5\n",
      "    ram_util_percent: 29.2\n",
      "  pid: 164149\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09059297990454476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31829483332164527\n",
      "    mean_inference_ms: 0.52622218188453\n",
      "    mean_raw_obs_processing_ms: 0.07051190506946271\n",
      "  time_since_restore: 933.9019014835358\n",
      "  time_this_iter_s: 3.95416522026062\n",
      "  time_total_s: 933.9019014835358\n",
      "  timers:\n",
      "    learn_throughput: 2054.719\n",
      "    learn_time_ms: 1946.738\n",
      "    load_throughput: 29428549.377\n",
      "    load_time_ms: 0.136\n",
      "    sample_throughput: 995.88\n",
      "    sample_time_ms: 4016.549\n",
      "    update_time_ms: 1.301\n",
      "  timestamp: 1671818490\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: a08d1_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:30 (running for 00:15:44.04)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         933.902</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\"> 113.712</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:36 (running for 00:15:50.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:41 (running for 00:15:55.03)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:46 (running for 00:16:00.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:51 (running for 00:16:05.04)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:01:56 (running for 00:16:10.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:01 (running for 00:16:15.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:06 (running for 00:16:20.05)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:11 (running for 00:16:25.06)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:16 (running for 00:16:30.06)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:21 (running for 00:16:35.07)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:26 (running for 00:16:40.07)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:31 (running for 00:16:45.08)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:36 (running for 00:16:50.08)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:41 (running for 00:16:55.08)<br>Memory usage on this node: 9.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:46 (running for 00:17:00.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:51 (running for 00:17:05.09)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:02:56 (running for 00:17:10.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 18:03:01 (running for 00:17:15.10)<br>Memory usage on this node: 9.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.79 GiB heap, 0.0/7.9 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/exercises/bipedal_walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_a08d1_00000</td><td>RUNNING </td><td>192.168.0.98:164149</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         937.837</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 114.324</td><td style=\"text-align: right;\">             169.167</td><td style=\"text-align: right;\">            -121.249</td><td style=\"text-align: right;\">           1435.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Fill in the blanks with the correct settings and run the cell\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"BipedalWalker-v3\",\n",
    "                 \"evaluation_interval\": 100,\n",
    "                 \"evaluation_num_episodes\": 100\n",
    "                 },\n",
    "         local_dir=\"bipedal_walker_v3\",    # Save results to the relative path bipedal_walker_v3\n",
    "         checkpoint_freq=100,    # Save the agent at every evaluation\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fb4b1-2985-47b4-a15c-bc9baaa316cc",
   "metadata": {},
   "source": [
    "Open `tensorboard` in a terminal to visualize the results. Stop the experiment once the evaluation performance has reached 250 cumulative rewards per episode.\n",
    "\n",
    "A performance of > 250 typically means that the robot has learned how to walk! \n",
    "\n",
    "I am sure you want to see the robot walking. In the next lesson, we will learn how to see the walking robot in action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdeeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
