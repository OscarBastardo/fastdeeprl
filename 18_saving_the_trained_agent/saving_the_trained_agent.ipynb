{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4857903-f15a-4d84-a936-fb7567ff530f",
   "metadata": {},
   "source": [
    "# Saving the trained agent\n",
    "\n",
    "<img src=\"images/flow/flow.png\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8b7d6f-73e9-44f5-bb63-50764c21eb3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_17-36-43_946352_160766/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_17-36-43_946352_160766/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_17-36-43_946352_160766',\n",
       " 'metrics_export_port': 64073,\n",
       " 'gcs_address': '192.168.0.98:62327',\n",
       " 'address': '192.168.0.98:62327',\n",
       " 'node_id': 'a532665bc3eed705be698f182802b8497558a0eeecd0e9416195057d'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdbfe1-23a7-42a8-ae54-d41b2e6f531c",
   "metadata": {},
   "source": [
    "## Step 1: Create checkpoints during the experiment run\n",
    "\n",
    "- Checkpoint save all information to **restore the current policy**.\n",
    "- Checkpoints can be created at regular intervals via `tune.run()`'s `checkpoint_freq` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e8f94a-5cfd-4f8d-9333-ec38f3986096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:53,273\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:53,274\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:53,274\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:53,274\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:57,881\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:58 (running for 00:00:08.25)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:36:58,830\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:59 (running for 00:00:09.25)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:37:00,712\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-03\n",
      "  done: false\n",
      "  episode_len_mean: 21.288770053475936\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 78.0\n",
      "  episode_reward_mean: 21.288770053475936\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 187\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6660336256027222\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027907222509384155\n",
      "          model: {}\n",
      "          policy_loss: -0.036938656121492386\n",
      "          total_loss: 191.57762145996094\n",
      "          vf_explained_var: 0.01673300750553608\n",
      "          vf_loss: 191.60899353027344\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.185714285714276\n",
      "    ram_util_percent: 24.800000000000004\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06262034062055195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06491877620967904\n",
      "    mean_inference_ms: 0.6775331708348615\n",
      "    mean_raw_obs_processing_ms: 0.10805825507891213\n",
      "  time_since_restore: 4.406257152557373\n",
      "  time_this_iter_s: 4.406257152557373\n",
      "  time_total_s: 4.406257152557373\n",
      "  timers:\n",
      "    learn_throughput: 1592.6\n",
      "    learn_time_ms: 2511.616\n",
      "    load_throughput: 19925434.679\n",
      "    load_time_ms: 0.201\n",
      "    sample_throughput: 1412.839\n",
      "    sample_time_ms: 2831.178\n",
      "    update_time_ms: 1.961\n",
      "  timestamp: 1671817023\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:05 (running for 00:00:14.68)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.40626</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.2888</td><td style=\"text-align: right;\">                  78</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           21.2888</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-08\n",
      "  done: false\n",
      "  episode_len_mean: 41.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 142.0\n",
      "  episode_reward_mean: 41.66\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 272\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 96.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 200.0\n",
      "    episode_reward_mean: 96.55\n",
      "    episode_reward_min: 24.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 110\n",
      "      - 194\n",
      "      - 24\n",
      "      - 68\n",
      "      - 59\n",
      "      - 83\n",
      "      - 35\n",
      "      - 181\n",
      "      - 89\n",
      "      - 51\n",
      "      - 57\n",
      "      - 99\n",
      "      - 138\n",
      "      - 130\n",
      "      - 63\n",
      "      - 108\n",
      "      - 42\n",
      "      - 70\n",
      "      - 130\n",
      "      - 200\n",
      "      episode_reward:\n",
      "      - 110.0\n",
      "      - 194.0\n",
      "      - 24.0\n",
      "      - 68.0\n",
      "      - 59.0\n",
      "      - 83.0\n",
      "      - 35.0\n",
      "      - 181.0\n",
      "      - 89.0\n",
      "      - 51.0\n",
      "      - 57.0\n",
      "      - 99.0\n",
      "      - 138.0\n",
      "      - 130.0\n",
      "      - 63.0\n",
      "      - 108.0\n",
      "      - 42.0\n",
      "      - 70.0\n",
      "      - 130.0\n",
      "      - 200.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.058555701751393074\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.058122673390074556\n",
      "      mean_inference_ms: 0.6224385946680546\n",
      "      mean_raw_obs_processing_ms: 0.08186111785857082\n",
      "    timesteps_this_iter: 1931\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6129814386367798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017226822674274445\n",
      "          model: {}\n",
      "          policy_loss: -0.029068341478705406\n",
      "          total_loss: 452.4669494628906\n",
      "          vf_explained_var: 0.10296483337879181\n",
      "          vf_loss: 452.49078369140625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.962500000000006\n",
      "    ram_util_percent: 24.8875\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061947291306672356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06435597928415081\n",
      "    mean_inference_ms: 0.6660674001278604\n",
      "    mean_raw_obs_processing_ms: 0.10316580077934379\n",
      "  time_since_restore: 10.079674243927002\n",
      "  time_this_iter_s: 5.673417091369629\n",
      "  time_total_s: 10.079674243927002\n",
      "  timers:\n",
      "    learn_throughput: 1681.35\n",
      "    learn_time_ms: 2379.04\n",
      "    load_throughput: 20348351.728\n",
      "    load_time_ms: 0.197\n",
      "    sample_throughput: 1113.355\n",
      "    sample_time_ms: 3592.743\n",
      "    update_time_ms: 1.748\n",
      "  timestamp: 1671817028\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=161905)\u001b[0m 2022-12-23 17:37:08,976\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:11 (running for 00:00:20.45)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.0797</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   41.66</td><td style=\"text-align: right;\">                 142</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             41.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:16 (running for 00:00:25.48)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.1231</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.54</td><td style=\"text-align: right;\">                 291</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             68.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:21 (running for 00:00:30.52)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.1231</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.54</td><td style=\"text-align: right;\">                 291</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             68.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 98.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 435.0\n",
      "  episode_reward_mean: 98.74\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 332\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 338.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 338.0\n",
      "    episode_reward_min: 140.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 202\n",
      "      - 399\n",
      "      - 500\n",
      "      - 426\n",
      "      - 239\n",
      "      - 500\n",
      "      - 500\n",
      "      - 294\n",
      "      - 488\n",
      "      - 477\n",
      "      - 304\n",
      "      - 432\n",
      "      - 186\n",
      "      - 246\n",
      "      - 500\n",
      "      - 140\n",
      "      - 314\n",
      "      - 197\n",
      "      - 197\n",
      "      - 219\n",
      "      episode_reward:\n",
      "      - 202.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 426.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 294.0\n",
      "      - 488.0\n",
      "      - 477.0\n",
      "      - 304.0\n",
      "      - 432.0\n",
      "      - 186.0\n",
      "      - 246.0\n",
      "      - 500.0\n",
      "      - 140.0\n",
      "      - 314.0\n",
      "      - 197.0\n",
      "      - 197.0\n",
      "      - 219.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05751914052002123\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.057344715330823484\n",
      "      mean_inference_ms: 0.6004282531790947\n",
      "      mean_raw_obs_processing_ms: 0.07799705271889852\n",
      "    timesteps_this_iter: 6760\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5624475479125977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037387702614068985\n",
      "          model: {}\n",
      "          policy_loss: -0.01738204061985016\n",
      "          total_loss: 766.6632690429688\n",
      "          vf_explained_var: 0.08671965450048447\n",
      "          vf_loss: 766.679443359375\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.98461538461538\n",
      "    ram_util_percent: 24.899999999999995\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06183534976909589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06432986141084003\n",
      "    mean_inference_ms: 0.6575892788045222\n",
      "    mean_raw_obs_processing_ms: 0.09902885988208755\n",
      "  time_since_restore: 23.4952290058136\n",
      "  time_this_iter_s: 9.372080087661743\n",
      "  time_total_s: 23.4952290058136\n",
      "  timers:\n",
      "    learn_throughput: 1719.726\n",
      "    learn_time_ms: 2325.952\n",
      "    load_throughput: 20170984.07\n",
      "    load_time_ms: 0.198\n",
      "    sample_throughput: 943.618\n",
      "    sample_time_ms: 4239.004\n",
      "    update_time_ms: 1.621\n",
      "  timestamp: 1671817042\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:26 (running for 00:00:35.90)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         23.4952</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   98.74</td><td style=\"text-align: right;\">                 435</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             98.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:31 (running for 00:00:40.98)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">           27.52</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            131.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:36 (running for 00:00:45.98)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">           27.52</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            131.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 168.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 168.92\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 354\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 431.35\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 431.35\n",
      "    episode_reward_min: 76.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 283\n",
      "      - 500\n",
      "      - 241\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 228\n",
      "      - 414\n",
      "      - 76\n",
      "      - 500\n",
      "      - 500\n",
      "      - 467\n",
      "      - 418\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 283.0\n",
      "      - 500.0\n",
      "      - 241.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 228.0\n",
      "      - 414.0\n",
      "      - 76.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 467.0\n",
      "      - 418.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.057458189248391675\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05731891557411657\n",
      "      mean_inference_ms: 0.5960452913845635\n",
      "      mean_raw_obs_processing_ms: 0.07759199743217461\n",
      "    timesteps_this_iter: 8627\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5686151385307312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034151896834373474\n",
      "          model: {}\n",
      "          policy_loss: -0.008929600939154625\n",
      "          total_loss: 514.5067138671875\n",
      "          vf_explained_var: 0.2504657506942749\n",
      "          vf_loss: 514.5153198242188\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95333333333333\n",
      "    ram_util_percent: 24.973333333333336\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061676786854929715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0641143935004951\n",
      "    mean_inference_ms: 0.6550715784401808\n",
      "    mean_raw_obs_processing_ms: 0.09744419416152769\n",
      "  time_since_restore: 38.314300775527954\n",
      "  time_this_iter_s: 10.794342756271362\n",
      "  time_total_s: 38.314300775527954\n",
      "  timers:\n",
      "    learn_throughput: 1734.946\n",
      "    learn_time_ms: 2305.547\n",
      "    load_throughput: 21750928.263\n",
      "    load_time_ms: 0.184\n",
      "    sample_throughput: 789.649\n",
      "    sample_time_ms: 5065.543\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1671817057\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:42 (running for 00:00:51.87)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         42.3528</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  201.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            201.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:47 (running for 00:00:56.88)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         42.3528</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  201.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            201.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-52\n",
      "  done: false\n",
      "  episode_len_mean: 236.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 236.87\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 370\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 444.15\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 444.15\n",
      "    episode_reward_min: 266.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 457\n",
      "      - 500\n",
      "      - 500\n",
      "      - 447\n",
      "      - 500\n",
      "      - 305\n",
      "      - 500\n",
      "      - 500\n",
      "      - 318\n",
      "      - 266\n",
      "      - 339\n",
      "      - 274\n",
      "      - 500\n",
      "      - 500\n",
      "      - 477\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 457.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 447.0\n",
      "      - 500.0\n",
      "      - 305.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 318.0\n",
      "      - 266.0\n",
      "      - 339.0\n",
      "      - 274.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 477.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.057363744892516076\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0571477013720364\n",
      "      mean_inference_ms: 0.5937423262739171\n",
      "      mean_raw_obs_processing_ms: 0.07723766504812855\n",
      "    timesteps_this_iter: 8883\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5495507121086121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003040600335225463\n",
      "          model: {}\n",
      "          policy_loss: -0.004149890970438719\n",
      "          total_loss: 457.1791076660156\n",
      "          vf_explained_var: 0.022055456414818764\n",
      "          vf_loss: 457.18310546875\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 24.93125\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061509724889047615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06390620963126802\n",
      "    mean_inference_ms: 0.6531900742502738\n",
      "    mean_raw_obs_processing_ms: 0.09613300048565801\n",
      "  time_since_restore: 53.38248014450073\n",
      "  time_this_iter_s: 11.029680252075195\n",
      "  time_total_s: 53.38248014450073\n",
      "  timers:\n",
      "    learn_throughput: 1737.184\n",
      "    learn_time_ms: 2302.577\n",
      "    load_throughput: 21389279.363\n",
      "    load_time_ms: 0.187\n",
      "    sample_throughput: 706.281\n",
      "    sample_time_ms: 5663.469\n",
      "    update_time_ms: 1.558\n",
      "  timestamp: 1671817072\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:52 (running for 00:01:01.94)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         53.3825</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  236.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            236.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:57 (running for 00:01:06.98)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         57.3988</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  270.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            270.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:02 (running for 00:01:12.03)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         57.3988</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  270.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            270.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:07 (running for 00:01:17.03)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         57.3988</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  270.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            270.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 298.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 298.69\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 387\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 474.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 474.6\n",
      "    episode_reward_min: 246.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 246\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 437\n",
      "      - 309\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 246.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05737802872385519\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05706721757008274\n",
      "      mean_inference_ms: 0.5931405384694054\n",
      "      mean_raw_obs_processing_ms: 0.07705203009471977\n",
      "    timesteps_this_iter: 9492\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5540298819541931\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006434320472180843\n",
      "          model: {}\n",
      "          policy_loss: -0.004580328706651926\n",
      "          total_loss: 442.2370300292969\n",
      "          vf_explained_var: -0.06211351230740547\n",
      "          vf_loss: 442.2415771484375\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.2625\n",
      "    ram_util_percent: 24.95\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06146299767919278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0638938893794745\n",
      "    mean_inference_ms: 0.6507449266865418\n",
      "    mean_raw_obs_processing_ms: 0.09468006361006863\n",
      "  time_since_restore: 68.97207903862\n",
      "  time_this_iter_s: 11.573267459869385\n",
      "  time_total_s: 68.97207903862\n",
      "  timers:\n",
      "    learn_throughput: 1736.538\n",
      "    learn_time_ms: 2303.433\n",
      "    load_throughput: 23014013.717\n",
      "    load_time_ms: 0.174\n",
      "    sample_throughput: 661.494\n",
      "    sample_time_ms: 6046.918\n",
      "    update_time_ms: 1.54\n",
      "  timestamp: 1671817088\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:13 (running for 00:01:22.66)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.9774</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  329.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            329.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:18 (running for 00:01:27.67)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.9774</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  329.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            329.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:23 (running for 00:01:32.67)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         72.9774</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  329.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            329.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 363.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 363.79\n",
      "  episode_reward_min: 16.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 403\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05734699917302934\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0570616471864997\n",
      "      mean_inference_ms: 0.5928936967621403\n",
      "      mean_raw_obs_processing_ms: 0.07696059275368464\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46938416361808777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036560124717652798\n",
      "          model: {}\n",
      "          policy_loss: -0.001965724863111973\n",
      "          total_loss: 422.8761291503906\n",
      "          vf_explained_var: 0.03210340812802315\n",
      "          vf_loss: 422.8780822753906\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.470588235294116\n",
      "    ram_util_percent: 25.0\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0613589867180001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06378794495880602\n",
      "    mean_inference_ms: 0.6495210328156807\n",
      "    mean_raw_obs_processing_ms: 0.09353570948852866\n",
      "  time_since_restore: 84.94672012329102\n",
      "  time_this_iter_s: 11.969284534454346\n",
      "  time_total_s: 84.94672012329102\n",
      "  timers:\n",
      "    learn_throughput: 1753.532\n",
      "    learn_time_ms: 2281.111\n",
      "    load_throughput: 23311401.973\n",
      "    load_time_ms: 0.172\n",
      "    sample_throughput: 580.019\n",
      "    sample_time_ms: 6896.321\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1671817104\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:28 (running for 00:01:37.89)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.1831</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  394.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            394.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:33 (running for 00:01:42.94)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.1831</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  394.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            394.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:38 (running for 00:01:47.95)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.1831</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  394.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            394.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 419.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 419.2\n",
      "  episode_reward_min: 16.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 421\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05738489562854582\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05712919519136558\n",
      "      mean_inference_ms: 0.593001831282909\n",
      "      mean_raw_obs_processing_ms: 0.07696009094926509\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5103356838226318\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008770144544541836\n",
      "          model: {}\n",
      "          policy_loss: -0.006191136781126261\n",
      "          total_loss: 719.5060424804688\n",
      "          vf_explained_var: -0.05256789177656174\n",
      "          vf_loss: 719.51220703125\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.3111111111111\n",
      "    ram_util_percent: 25.555555555555557\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06127908849297959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06375606487500882\n",
      "    mean_inference_ms: 0.6488774162416773\n",
      "    mean_raw_obs_processing_ms: 0.09235716811290459\n",
      "  time_since_restore: 101.38210034370422\n",
      "  time_this_iter_s: 12.199014663696289\n",
      "  time_total_s: 101.38210034370422\n",
      "  timers:\n",
      "    learn_throughput: 1733.088\n",
      "    learn_time_ms: 2308.019\n",
      "    load_throughput: 23797469.504\n",
      "    load_time_ms: 0.168\n",
      "    sample_throughput: 529.121\n",
      "    sample_time_ms: 7559.713\n",
      "    update_time_ms: 1.549\n",
      "  timestamp: 1671817120\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:43 (running for 00:01:53.19)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         101.382</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">   419.2</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             419.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:49 (running for 00:01:59.18)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         105.415</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  445.66</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  55</td><td style=\"text-align: right;\">            445.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:54 (running for 00:02:04.24)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         105.415</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  445.66</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  55</td><td style=\"text-align: right;\">            445.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 458.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 458.07\n",
      "  episode_reward_min: 55.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 437\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 489.5\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 489.5\n",
      "    episode_reward_min: 290.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 290\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 290.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05736019548590084\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05710711513265534\n",
      "      mean_inference_ms: 0.5926560863345511\n",
      "      mean_raw_obs_processing_ms: 0.07693436665552739\n",
      "    timesteps_this_iter: 9790\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5119616985321045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005402083043009043\n",
      "          model: {}\n",
      "          policy_loss: -0.0027030755300074816\n",
      "          total_loss: 499.7520751953125\n",
      "          vf_explained_var: -0.025662068277597427\n",
      "          vf_loss: 499.7547607421875\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.93125\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06125484082503576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06375705849670077\n",
      "    mean_inference_ms: 0.6489234305668397\n",
      "    mean_raw_obs_processing_ms: 0.09162660769291497\n",
      "  time_since_restore: 117.12063884735107\n",
      "  time_this_iter_s: 11.705204486846924\n",
      "  time_total_s: 117.12063884735107\n",
      "  timers:\n",
      "    learn_throughput: 1730.331\n",
      "    learn_time_ms: 2311.697\n",
      "    load_throughput: 22629101.699\n",
      "    load_time_ms: 0.177\n",
      "    sample_throughput: 511.249\n",
      "    sample_time_ms: 7823.976\n",
      "    update_time_ms: 1.525\n",
      "  timestamp: 1671817136\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:00 (running for 00:02:09.93)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         117.121</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  458.07</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  55</td><td style=\"text-align: right;\">            458.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:05 (running for 00:02:15.02)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.136</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  470.69</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            470.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:10 (running for 00:02:20.02)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         121.136</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  470.69</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            470.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-39-11\n",
      "  done: false\n",
      "  episode_len_mean: 480.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 480.84\n",
      "  episode_reward_min: 104.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 453\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 459.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 459.85\n",
      "    episode_reward_min: 291.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 426\n",
      "      - 343\n",
      "      - 379\n",
      "      - 473\n",
      "      - 500\n",
      "      - 500\n",
      "      - 418\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 291\n",
      "      - 500\n",
      "      - 500\n",
      "      - 367\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 426.0\n",
      "      - 343.0\n",
      "      - 379.0\n",
      "      - 473.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 418.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 291.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 367.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05710977844020885\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05685663927530584\n",
      "      mean_inference_ms: 0.590161894377904\n",
      "      mean_raw_obs_processing_ms: 0.07668421482711348\n",
      "    timesteps_this_iter: 9197\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.49454039335250854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0054311975836753845\n",
      "          model: {}\n",
      "          policy_loss: -0.002683911705389619\n",
      "          total_loss: 512.4330444335938\n",
      "          vf_explained_var: -0.029449569061398506\n",
      "          vf_loss: 512.4357299804688\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.89375\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06124501121293945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06377473394081132\n",
      "    mean_inference_ms: 0.6490272037945085\n",
      "    mean_raw_obs_processing_ms: 0.09108612979992675\n",
      "  time_since_restore: 132.1353895664215\n",
      "  time_this_iter_s: 10.99988865852356\n",
      "  time_total_s: 132.1353895664215\n",
      "  timers:\n",
      "    learn_throughput: 1734.437\n",
      "    learn_time_ms: 2306.224\n",
      "    load_throughput: 22900922.741\n",
      "    load_time_ms: 0.175\n",
      "    sample_throughput: 505.581\n",
      "    sample_time_ms: 7911.694\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1671817151\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:16 (running for 00:02:25.98)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         136.031</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  483.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            483.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:21 (running for 00:02:30.98)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         136.031</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  483.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            483.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:26 (running for 00:02:35.99)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         136.031</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  483.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            483.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-39-26\n",
      "  done: false\n",
      "  episode_len_mean: 482.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 482.0\n",
      "  episode_reward_min: 104.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 469\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 490.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 490.45\n",
      "    episode_reward_min: 309.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 309\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05684551620980046\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05655659770299619\n",
      "      mean_inference_ms: 0.5871304281647636\n",
      "      mean_raw_obs_processing_ms: 0.07631786900422236\n",
      "    timesteps_this_iter: 9809\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5471547842025757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004468538332730532\n",
      "          model: {}\n",
      "          policy_loss: -0.005327142309397459\n",
      "          total_loss: 380.1043701171875\n",
      "          vf_explained_var: 0.05688590183854103\n",
      "          vf_loss: 380.10968017578125\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.96875\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06121023429141141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06377042319091336\n",
      "    mean_inference_ms: 0.648523779709048\n",
      "    mean_raw_obs_processing_ms: 0.09060124494262223\n",
      "  time_since_restore: 147.1942081451416\n",
      "  time_this_iter_s: 11.163541555404663\n",
      "  time_total_s: 147.1942081451416\n",
      "  timers:\n",
      "    learn_throughput: 1747.105\n",
      "    learn_time_ms: 2289.502\n",
      "    load_throughput: 21845333.333\n",
      "    load_time_ms: 0.183\n",
      "    sample_throughput: 507.67\n",
      "    sample_time_ms: 7879.127\n",
      "    update_time_ms: 1.553\n",
      "  timestamp: 1671817166\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:32 (running for 00:02:41.94)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          150.99</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  483.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            483.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:37 (running for 00:02:47.00)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          150.99</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  483.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            483.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-39-41\n",
      "  done: false\n",
      "  episode_len_mean: 481.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 481.26\n",
      "  episode_reward_min: 104.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 486\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 498.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 498.6\n",
      "    episode_reward_min: 472.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 472\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 472.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.056639205482907594\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.056340754447326905\n",
      "      mean_inference_ms: 0.5848055854460241\n",
      "      mean_raw_obs_processing_ms: 0.07605864130356013\n",
      "    timesteps_this_iter: 9972\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5324649810791016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004788981284946203\n",
      "          model: {}\n",
      "          policy_loss: -0.002317962236702442\n",
      "          total_loss: 512.7765502929688\n",
      "          vf_explained_var: 0.07569920271635056\n",
      "          vf_loss: 512.7788696289062\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5375\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06112633773403985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06368725781333208\n",
      "    mean_inference_ms: 0.6473846425494685\n",
      "    mean_raw_obs_processing_ms: 0.0901608767118184\n",
      "  time_since_restore: 162.29226160049438\n",
      "  time_this_iter_s: 11.302359819412231\n",
      "  time_total_s: 162.29226160049438\n",
      "  timers:\n",
      "    learn_throughput: 1761.648\n",
      "    learn_time_ms: 2270.602\n",
      "    load_throughput: 23560196.602\n",
      "    load_time_ms: 0.17\n",
      "    sample_throughput: 512.017\n",
      "    sample_time_ms: 7812.246\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1671817181\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:42 (running for 00:02:52.28)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         162.292</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  481.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            481.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:48 (running for 00:02:58.16)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         166.086</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  486.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            486.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:53 (running for 00:03:03.17)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         166.086</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  486.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            486.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 486.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.67\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 502\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 499.95\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 499.95\n",
      "    episode_reward_min: 499.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 499\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.056461530110578154\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05617506537541197\n",
      "      mean_inference_ms: 0.5827726238568914\n",
      "      mean_raw_obs_processing_ms: 0.07583355034271025\n",
      "    timesteps_this_iter: 9999\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4963940382003784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005369347985833883\n",
      "          model: {}\n",
      "          policy_loss: -0.002022728556767106\n",
      "          total_loss: 501.9591369628906\n",
      "          vf_explained_var: -0.0020605733152478933\n",
      "          vf_loss: 501.9611511230469\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.96470588235294\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06098649560755278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06354890546679397\n",
      "    mean_inference_ms: 0.6455895781419914\n",
      "    mean_raw_obs_processing_ms: 0.08974852120731973\n",
      "  time_since_restore: 177.421537399292\n",
      "  time_this_iter_s: 11.335597515106201\n",
      "  time_total_s: 177.421537399292\n",
      "  timers:\n",
      "    learn_throughput: 1796.423\n",
      "    learn_time_ms: 2226.647\n",
      "    load_throughput: 24403223.273\n",
      "    load_time_ms: 0.164\n",
      "    sample_throughput: 520.299\n",
      "    sample_time_ms: 7687.89\n",
      "    update_time_ms: 1.459\n",
      "  timestamp: 1671817197\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:59 (running for 00:03:08.53)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         177.422</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  486.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            486.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:04 (running for 00:03:14.31)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         181.243</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  486.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            486.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:09 (running for 00:03:19.38)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         181.243</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  486.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            486.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-40-12\n",
      "  done: false\n",
      "  episode_len_mean: 488.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.85\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 518\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 489.35\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 489.35\n",
      "    episode_reward_min: 287.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 287\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 287.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05632199231720666\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05602027745350728\n",
      "      mean_inference_ms: 0.5812207554478696\n",
      "      mean_raw_obs_processing_ms: 0.07566172747241043\n",
      "    timesteps_this_iter: 9787\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5136764645576477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006380924489349127\n",
      "          model: {}\n",
      "          policy_loss: -0.0013076552422717214\n",
      "          total_loss: 452.0995178222656\n",
      "          vf_explained_var: 0.08605086803436279\n",
      "          vf_loss: 452.1007995605469\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.98125\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06076704709230839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06333566489000172\n",
      "    mean_inference_ms: 0.6427966764495534\n",
      "    mean_raw_obs_processing_ms: 0.08926155388550316\n",
      "  time_since_restore: 192.41975212097168\n",
      "  time_this_iter_s: 11.176305294036865\n",
      "  time_total_s: 192.41975212097168\n",
      "  timers:\n",
      "    learn_throughput: 1811.623\n",
      "    learn_time_ms: 2207.964\n",
      "    load_throughput: 23763762.04\n",
      "    load_time_ms: 0.168\n",
      "    sample_throughput: 526.185\n",
      "    sample_time_ms: 7601.893\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1671817212\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:15 (running for 00:03:24.52)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">          192.42</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  488.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            488.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:21 (running for 00:03:30.43)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         196.227</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  491.21</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            491.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:26 (running for 00:03:35.43)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         196.227</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  491.21</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            491.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 487.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 487.91\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 535\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 490.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 490.45\n",
      "    episode_reward_min: 309.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 309\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05621819268723406\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05590725128796938\n",
      "      mean_inference_ms: 0.5800561960179548\n",
      "      mean_raw_obs_processing_ms: 0.07552504121488016\n",
      "    timesteps_this_iter: 9809\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.575197696685791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00808875821530819\n",
      "          model: {}\n",
      "          policy_loss: -0.00813768059015274\n",
      "          total_loss: 409.3234558105469\n",
      "          vf_explained_var: 0.14302197098731995\n",
      "          vf_loss: 409.3315734863281\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0875\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06046596554717627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06302538418592846\n",
      "    mean_inference_ms: 0.6392571777927871\n",
      "    mean_raw_obs_processing_ms: 0.08871452349306735\n",
      "  time_since_restore: 207.4966962337494\n",
      "  time_this_iter_s: 11.26927399635315\n",
      "  time_total_s: 207.4966962337494\n",
      "  timers:\n",
      "    learn_throughput: 1824.729\n",
      "    learn_time_ms: 2192.107\n",
      "    load_throughput: 23530457.223\n",
      "    load_time_ms: 0.17\n",
      "    sample_throughput: 531.11\n",
      "    sample_time_ms: 7531.396\n",
      "    update_time_ms: 1.47\n",
      "  timestamp: 1671817227\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:31 (running for 00:03:40.49)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         211.314</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  477.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            477.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:36 (running for 00:03:45.50)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         211.314</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  477.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            477.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:41 (running for 00:03:50.58)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         211.314</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  477.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            477.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 477.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 477.05\n",
      "  episode_reward_min: 52.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 554\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 441.3\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 441.3\n",
      "    episode_reward_min: 329.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 487\n",
      "      - 500\n",
      "      - 401\n",
      "      - 489\n",
      "      - 363\n",
      "      - 419\n",
      "      - 447\n",
      "      - 413\n",
      "      - 467\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 388\n",
      "      - 500\n",
      "      - 329\n",
      "      - 434\n",
      "      - 473\n",
      "      - 422\n",
      "      - 333\n",
      "      - 461\n",
      "      episode_reward:\n",
      "      - 487.0\n",
      "      - 500.0\n",
      "      - 401.0\n",
      "      - 489.0\n",
      "      - 363.0\n",
      "      - 419.0\n",
      "      - 447.0\n",
      "      - 413.0\n",
      "      - 467.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 388.0\n",
      "      - 500.0\n",
      "      - 329.0\n",
      "      - 434.0\n",
      "      - 473.0\n",
      "      - 422.0\n",
      "      - 333.0\n",
      "      - 461.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05615765429271178\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05584322625653097\n",
      "      mean_inference_ms: 0.5792389256339339\n",
      "      mean_raw_obs_processing_ms: 0.07544442246842856\n",
      "    timesteps_this_iter: 8826\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5164773464202881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00867490191012621\n",
      "          model: {}\n",
      "          policy_loss: -0.005522888153791428\n",
      "          total_loss: 345.8601379394531\n",
      "          vf_explained_var: 0.26402366161346436\n",
      "          vf_loss: 345.86566162109375\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30666666666666\n",
      "    ram_util_percent: 25.60000000000001\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06013500116107263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0626827162035528\n",
      "    mean_inference_ms: 0.6353685507797823\n",
      "    mean_raw_obs_processing_ms: 0.08814843007809085\n",
      "  time_since_restore: 221.80242395401\n",
      "  time_this_iter_s: 10.488444805145264\n",
      "  time_total_s: 221.80242395401\n",
      "  timers:\n",
      "    learn_throughput: 1834.298\n",
      "    learn_time_ms: 2180.67\n",
      "    load_throughput: 23679909.668\n",
      "    load_time_ms: 0.169\n",
      "    sample_throughput: 529.144\n",
      "    sample_time_ms: 7559.383\n",
      "    update_time_ms: 1.47\n",
      "  timestamp: 1671817241\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:46 (running for 00:03:55.82)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         225.588</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  474.24</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            474.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:51 (running for 00:04:00.90)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         225.588</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  474.24</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            474.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:56 (running for 00:04:05.90)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         225.588</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  474.24</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            474.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_60b6a_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 474.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 474.76\n",
      "  episode_reward_min: 52.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 570\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 492.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 492.1\n",
      "    episode_reward_min: 342.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 342\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 342.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05607454637696853\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.055770428006547544\n",
      "      mean_inference_ms: 0.5784088067278427\n",
      "      mean_raw_obs_processing_ms: 0.07534395773495574\n",
      "    timesteps_this_iter: 9842\n",
      "  experiment_id: 198bf0d6f92047d4a6c45aaf71ccc937\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5508266091346741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009822187945246696\n",
      "          model: {}\n",
      "          policy_loss: -0.004973192233592272\n",
      "          total_loss: 239.21974182128906\n",
      "          vf_explained_var: 0.30452191829681396\n",
      "          vf_loss: 239.22470092773438\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8875\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 161905\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.059891233318772344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.062430612182364335\n",
      "    mean_inference_ms: 0.6325105151447895\n",
      "    mean_raw_obs_processing_ms: 0.0877396850883876\n",
      "  time_since_restore: 236.78424715995789\n",
      "  time_this_iter_s: 11.196498394012451\n",
      "  time_total_s: 236.78424715995789\n",
      "  timers:\n",
      "    learn_throughput: 1836.961\n",
      "    learn_time_ms: 2177.509\n",
      "    load_throughput: 21936736.402\n",
      "    load_time_ms: 0.182\n",
      "    sample_throughput: 534.204\n",
      "    sample_time_ms: 7487.781\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1671817256\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 60b6a_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:40:58,650\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:58 (running for 00:04:08.14)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/15.06 GiB heap, 0.0/7.53 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_60b6a_00000</td><td>RUNNING </td><td>192.168.0.98:161905</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         236.784</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">  474.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">            474.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:40:58,940\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_60b6a_00000]\n",
      "2022-12-23 17:40:58,941\tINFO tune.py:639 -- Total run time: 250.45 seconds (248.06 seconds for the tuning loop).\n",
      "2022-12-23 17:40:58,941\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f013d7f7b80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2,\n",
    "                 \"evaluation_num_episodes\": 20\n",
    "                 },\n",
    "         local_dir=\"cartpole_v1\",\n",
    "         checkpoint_freq=2,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4642d-b733-4d3a-8fec-cc62208507c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Manually stop training after a certain performance is reached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdeeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
