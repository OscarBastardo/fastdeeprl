{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595528b-6ea8-474d-aae6-02a79c6c99d3",
   "metadata": {},
   "source": [
    "# Visualizing training and evaluation\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ce636-9502-41f9-b6b3-a98460332a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af232e4e-d9dc-4fb3-9654-9e652c80dddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_16-58-35_014239_45359/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_16-58-35_014239_45359/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_16-58-35_014239_45359',\n",
       " 'metrics_export_port': 54883,\n",
       " 'gcs_address': '192.168.0.98:60506',\n",
       " 'address': '192.168.0.98:60506',\n",
       " 'node_id': '67c9ebf3fb43a04f72776494a33a5f2686e8259783f7264aadadedf3'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0033ea09-1697-4f6e-b8de-578e6aa10c64",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:44,651\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:44,651\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:44,651\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:44,651\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:49,309\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:58:50 (running for 00:00:07.95)<br>Memory usage on this node: 5.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:50,059\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:58:51 (running for 00:00:08.96)<br>Memory usage on this node: 5.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=45494)\u001b[0m 2022-12-23 16:58:51,868\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-58-54\n",
      "  done: false\n",
      "  episode_len_mean: 23.497041420118343\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 23.497041420118343\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 169\n",
      "  episodes_total: 169\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6659464240074158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027651933953166008\n",
      "          model: {}\n",
      "          policy_loss: -0.0377194806933403\n",
      "          total_loss: 228.9647216796875\n",
      "          vf_explained_var: 0.022448014467954636\n",
      "          vf_loss: 228.9969024658203\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.71428571428571\n",
      "    ram_util_percent: 18.400000000000002\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06283861690701831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06287422152087471\n",
      "    mean_inference_ms: 0.6507737081236837\n",
      "    mean_raw_obs_processing_ms: 0.10247725022251974\n",
      "  time_since_restore: 4.3122217655181885\n",
      "  time_this_iter_s: 4.3122217655181885\n",
      "  time_total_s: 4.3122217655181885\n",
      "  timers:\n",
      "    learn_throughput: 1605.511\n",
      "    learn_time_ms: 2491.419\n",
      "    load_throughput: 20460019.512\n",
      "    load_time_ms: 0.196\n",
      "    sample_throughput: 1562.734\n",
      "    sample_time_ms: 2559.617\n",
      "    update_time_ms: 1.808\n",
      "  timestamp: 1671814734\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:58:56 (running for 00:00:14.29)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.31222</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  23.497</td><td style=\"text-align: right;\">                  90</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            23.497</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 41.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 137.0\n",
      "  episode_reward_mean: 41.72\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 254\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 88.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 236.0\n",
      "    episode_reward_mean: 88.9\n",
      "    episode_reward_min: 18.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 22\n",
      "      - 120\n",
      "      - 108\n",
      "      - 112\n",
      "      - 111\n",
      "      - 38\n",
      "      - 87\n",
      "      - 236\n",
      "      - 38\n",
      "      - 18\n",
      "      - 87\n",
      "      - 146\n",
      "      - 75\n",
      "      - 20\n",
      "      - 114\n",
      "      - 77\n",
      "      - 20\n",
      "      - 140\n",
      "      - 92\n",
      "      - 117\n",
      "      episode_reward:\n",
      "      - 22.0\n",
      "      - 120.0\n",
      "      - 108.0\n",
      "      - 112.0\n",
      "      - 111.0\n",
      "      - 38.0\n",
      "      - 87.0\n",
      "      - 236.0\n",
      "      - 38.0\n",
      "      - 18.0\n",
      "      - 87.0\n",
      "      - 146.0\n",
      "      - 75.0\n",
      "      - 20.0\n",
      "      - 114.0\n",
      "      - 77.0\n",
      "      - 20.0\n",
      "      - 140.0\n",
      "      - 92.0\n",
      "      - 117.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05540558298763899\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05613625083095643\n",
      "      mean_inference_ms: 0.593282990136664\n",
      "      mean_raw_obs_processing_ms: 0.07707781842613973\n",
      "    timesteps_this_iter: 1778\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6122690439224243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017667081207036972\n",
      "          model: {}\n",
      "          policy_loss: -0.03248727694153786\n",
      "          total_loss: 315.77911376953125\n",
      "          vf_explained_var: 0.0880792960524559\n",
      "          vf_loss: 315.8062744140625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.028571428571425\n",
      "    ram_util_percent: 18.400000000000002\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06195526917668562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06230131343098219\n",
      "    mean_inference_ms: 0.6425178145226187\n",
      "    mean_raw_obs_processing_ms: 0.09997806353035397\n",
      "  time_since_restore: 9.716984748840332\n",
      "  time_this_iter_s: 5.4047629833221436\n",
      "  time_total_s: 9.716984748840332\n",
      "  timers:\n",
      "    learn_throughput: 1690.687\n",
      "    learn_time_ms: 2365.901\n",
      "    load_throughput: 20385438.639\n",
      "    load_time_ms: 0.196\n",
      "    sample_throughput: 1170.633\n",
      "    sample_time_ms: 3416.956\n",
      "    update_time_ms: 1.586\n",
      "  timestamp: 1671814739\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:01 (running for 00:00:19.73)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.71698</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   41.72</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             41.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:06 (running for 00:00:24.77)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         13.7037</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   69.68</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             69.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:11 (running for 00:00:29.77)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         13.7037</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   69.68</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             69.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 98.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 367.0\n",
      "  episode_reward_mean: 98.08\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 312\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 381.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 381.1\n",
      "    episode_reward_min: 29.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 472\n",
      "      - 380\n",
      "      - 298\n",
      "      - 347\n",
      "      - 500\n",
      "      - 500\n",
      "      - 268\n",
      "      - 335\n",
      "      - 378\n",
      "      - 368\n",
      "      - 277\n",
      "      - 479\n",
      "      - 500\n",
      "      - 475\n",
      "      - 252\n",
      "      - 500\n",
      "      - 500\n",
      "      - 29\n",
      "      - 264\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 472.0\n",
      "      - 380.0\n",
      "      - 298.0\n",
      "      - 347.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 268.0\n",
      "      - 335.0\n",
      "      - 378.0\n",
      "      - 368.0\n",
      "      - 277.0\n",
      "      - 479.0\n",
      "      - 500.0\n",
      "      - 475.0\n",
      "      - 252.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 29.0\n",
      "      - 264.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05465692033819437\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05539571106554334\n",
      "      mean_inference_ms: 0.5772415342108771\n",
      "      mean_raw_obs_processing_ms: 0.07486571632614923\n",
      "    timesteps_this_iter: 7622\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5507956743240356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00777528015896678\n",
      "          model: {}\n",
      "          policy_loss: -0.016636909916996956\n",
      "          total_loss: 742.6449584960938\n",
      "          vf_explained_var: 0.10726438462734222\n",
      "          vf_loss: 742.6591796875\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17857142857142\n",
      "    ram_util_percent: 18.5\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061710921007650216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.062277965139081105\n",
      "    mean_inference_ms: 0.6386037044126919\n",
      "    mean_raw_obs_processing_ms: 0.09812839529148025\n",
      "  time_since_restore: 23.55309557914734\n",
      "  time_this_iter_s: 9.849371194839478\n",
      "  time_total_s: 23.55309557914734\n",
      "  timers:\n",
      "    learn_throughput: 1728.227\n",
      "    learn_time_ms: 2314.511\n",
      "    load_throughput: 20964968.447\n",
      "    load_time_ms: 0.191\n",
      "    sample_throughput: 981.616\n",
      "    sample_time_ms: 4074.914\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1671814753\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:17 (running for 00:00:35.60)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          27.519</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            131.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:22 (running for 00:00:40.61)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          27.519</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            131.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:27 (running for 00:00:45.64)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          27.519</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            131.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 164.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 164.11\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 333\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 475.15\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 475.15\n",
      "    episode_reward_min: 216.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 216\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 296\n",
      "      - 500\n",
      "      - 491\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 216.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 296.0\n",
      "      - 500.0\n",
      "      - 491.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05485702004919\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05547975363793941\n",
      "      mean_inference_ms: 0.5762679466896525\n",
      "      mean_raw_obs_processing_ms: 0.07429948510933972\n",
      "    timesteps_this_iter: 9503\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5391895174980164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007992073893547058\n",
      "          model: {}\n",
      "          policy_loss: -0.014010325074195862\n",
      "          total_loss: 548.2415161132812\n",
      "          vf_explained_var: 0.15676188468933105\n",
      "          vf_loss: 548.25439453125\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.675\n",
      "    ram_util_percent: 18.5\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06167189378132686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06220245310615789\n",
      "    mean_inference_ms: 0.6381380792977467\n",
      "    mean_raw_obs_processing_ms: 0.0963885184930104\n",
      "  time_since_restore: 38.7308988571167\n",
      "  time_this_iter_s: 11.211889266967773\n",
      "  time_total_s: 38.7308988571167\n",
      "  timers:\n",
      "    learn_throughput: 1747.488\n",
      "    learn_time_ms: 2289.0\n",
      "    load_throughput: 22221478.146\n",
      "    load_time_ms: 0.18\n",
      "    sample_throughput: 796.602\n",
      "    sample_time_ms: 5021.327\n",
      "    update_time_ms: 1.617\n",
      "  timestamp: 1671814768\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:32 (running for 00:00:50.83)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         42.7028</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  199.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            199.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:37 (running for 00:00:55.86)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         42.7028</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  199.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            199.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:42 (running for 00:01:00.87)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         42.7028</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  199.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            199.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 239.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 239.31\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 351\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05488826434697399\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05557186181875863\n",
      "      mean_inference_ms: 0.5768462541259756\n",
      "      mean_raw_obs_processing_ms: 0.07412048742905106\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5152256488800049\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005703157279640436\n",
      "          model: {}\n",
      "          policy_loss: -0.014610136859118938\n",
      "          total_loss: 364.1573791503906\n",
      "          vf_explained_var: 0.12404384464025497\n",
      "          vf_loss: 364.1716003417969\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.38125\n",
      "    ram_util_percent: 18.51875\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06160977691194445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0621044752019705\n",
      "    mean_inference_ms: 0.637408971409044\n",
      "    mean_raw_obs_processing_ms: 0.0945313748637759\n",
      "  time_since_restore: 54.321697473526\n",
      "  time_this_iter_s: 11.618867635726929\n",
      "  time_total_s: 54.321697473526\n",
      "  timers:\n",
      "    learn_throughput: 1756.311\n",
      "    learn_time_ms: 2277.501\n",
      "    load_throughput: 24465499.089\n",
      "    load_time_ms: 0.163\n",
      "    sample_throughput: 705.536\n",
      "    sample_time_ms: 5669.45\n",
      "    update_time_ms: 1.607\n",
      "  timestamp: 1671814784\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:48 (running for 00:01:06.45)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         58.2758</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  271.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            271.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:53 (running for 00:01:11.46)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         58.2758</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  271.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            271.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:59:58 (running for 00:01:16.49)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         58.2758</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  271.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            271.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-59-59\n",
      "  done: false\n",
      "  episode_len_mean: 301.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 301.29\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 368\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 474.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 474.65\n",
      "    episode_reward_min: 386.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 410\n",
      "      - 475\n",
      "      - 500\n",
      "      - 467\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 432\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 386\n",
      "      - 500\n",
      "      - 405\n",
      "      - 486\n",
      "      - 432\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 410.0\n",
      "      - 475.0\n",
      "      - 500.0\n",
      "      - 467.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 386.0\n",
      "      - 500.0\n",
      "      - 405.0\n",
      "      - 486.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05488939998354593\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05565865181335437\n",
      "      mean_inference_ms: 0.577102630284999\n",
      "      mean_raw_obs_processing_ms: 0.07410255214753651\n",
      "    timesteps_this_iter: 9493\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47548454999923706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0029359767213463783\n",
      "          model: {}\n",
      "          policy_loss: -0.007531945127993822\n",
      "          total_loss: 442.9351501464844\n",
      "          vf_explained_var: 0.13905972242355347\n",
      "          vf_loss: 442.9425354003906\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.043749999999996\n",
      "    ram_util_percent: 18.6\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06153755558722767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06211317897252091\n",
      "    mean_inference_ms: 0.6358954655500622\n",
      "    mean_raw_obs_processing_ms: 0.09350363840405229\n",
      "  time_since_restore: 69.52343130111694\n",
      "  time_this_iter_s: 11.247634649276733\n",
      "  time_total_s: 69.52343130111694\n",
      "  timers:\n",
      "    learn_throughput: 1761.658\n",
      "    learn_time_ms: 2270.588\n",
      "    load_throughput: 24453018.51\n",
      "    load_time_ms: 0.164\n",
      "    sample_throughput: 655.875\n",
      "    sample_time_ms: 6098.718\n",
      "    update_time_ms: 1.592\n",
      "  timestamp: 1671814799\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:03 (running for 00:01:21.72)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         73.5008</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  332.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            332.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:08 (running for 00:01:26.76)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         73.5008</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  332.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            332.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:13 (running for 00:01:31.76)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         73.5008</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  332.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            332.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-00-15\n",
      "  done: false\n",
      "  episode_len_mean: 364.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 364.8\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 384\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.054905399161951494\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.055645646065067764\n",
      "      mean_inference_ms: 0.576727118898941\n",
      "      mean_raw_obs_processing_ms: 0.07409389510116202\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4282943606376648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006296669133007526\n",
      "          model: {}\n",
      "          policy_loss: -0.004055194556713104\n",
      "          total_loss: 285.07720947265625\n",
      "          vf_explained_var: 0.27058303356170654\n",
      "          vf_loss: 285.0812072753906\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.45625\n",
      "    ram_util_percent: 18.6\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061502301842693764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06204391468663749\n",
      "    mean_inference_ms: 0.635279250505989\n",
      "    mean_raw_obs_processing_ms: 0.09200782669068559\n",
      "  time_since_restore: 85.0699532032013\n",
      "  time_this_iter_s: 11.569164514541626\n",
      "  time_total_s: 85.0699532032013\n",
      "  timers:\n",
      "    learn_throughput: 1782.499\n",
      "    learn_time_ms: 2244.04\n",
      "    load_throughput: 26994716.01\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 576.271\n",
      "    sample_time_ms: 6941.175\n",
      "    update_time_ms: 1.584\n",
      "  timestamp: 1671814815\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:19 (running for 00:01:37.29)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.0175</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   397.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">             397.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:24 (running for 00:01:42.29)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.0175</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   397.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">             397.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:29 (running for 00:01:47.33)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         89.0175</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   397.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">             397.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 421.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 421.35\n",
      "  episode_reward_min: 23.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 400\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 483.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 483.4\n",
      "    episode_reward_min: 389.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 469\n",
      "      - 428\n",
      "      - 491\n",
      "      - 500\n",
      "      - 426\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 490\n",
      "      - 500\n",
      "      - 498\n",
      "      - 389\n",
      "      - 497\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 480\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 469.0\n",
      "      - 428.0\n",
      "      - 491.0\n",
      "      - 500.0\n",
      "      - 426.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 490.0\n",
      "      - 500.0\n",
      "      - 498.0\n",
      "      - 389.0\n",
      "      - 497.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 480.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05489143815693863\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05561078353352318\n",
      "      mean_inference_ms: 0.5763136015313648\n",
      "      mean_raw_obs_processing_ms: 0.07399336718716362\n",
      "    timesteps_this_iter: 9668\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46427538990974426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030025006271898746\n",
      "          model: {}\n",
      "          policy_loss: -0.002670615678653121\n",
      "          total_loss: 228.55458068847656\n",
      "          vf_explained_var: 0.2891348898410797\n",
      "          vf_loss: 228.55722045898438\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.35\n",
      "    ram_util_percent: 18.6\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06142779768466573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.062006308296248126\n",
      "    mean_inference_ms: 0.6339593957920746\n",
      "    mean_raw_obs_processing_ms: 0.09094352103478101\n",
      "  time_since_restore: 100.31215357780457\n",
      "  time_this_iter_s: 11.294615268707275\n",
      "  time_total_s: 100.31215357780457\n",
      "  timers:\n",
      "    learn_throughput: 1788.485\n",
      "    learn_time_ms: 2236.53\n",
      "    load_throughput: 27103741.519\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 529.749\n",
      "    sample_time_ms: 7550.748\n",
      "    update_time_ms: 1.549\n",
      "  timestamp: 1671814830\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:34 (running for 00:01:52.60)<br>Memory usage on this node: 5.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         104.281</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  445.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            445.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:39 (running for 00:01:57.63)<br>Memory usage on this node: 4.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         104.281</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  445.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            445.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:44 (running for 00:02:02.63)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         104.281</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  445.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            445.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-00-44\n",
      "  done: false\n",
      "  episode_len_mean: 458.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 458.04\n",
      "  episode_reward_min: 127.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 417\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05368950312154942\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.054170753816954756\n",
      "      mean_inference_ms: 0.5634436764636886\n",
      "      mean_raw_obs_processing_ms: 0.07176246858839334\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4495571553707123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004566378891468048\n",
      "          model: {}\n",
      "          policy_loss: 0.000956619274802506\n",
      "          total_loss: 424.7896423339844\n",
      "          vf_explained_var: 0.06236150488257408\n",
      "          vf_loss: 424.7886657714844\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.33571428571429\n",
      "    ram_util_percent: 14.057142857142862\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.061331200772330875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06190692953043795\n",
      "    mean_inference_ms: 0.6326433760254028\n",
      "    mean_raw_obs_processing_ms: 0.08984260704082367\n",
      "  time_since_restore: 114.51045823097229\n",
      "  time_this_iter_s: 10.22968316078186\n",
      "  time_total_s: 114.51045823097229\n",
      "  timers:\n",
      "    learn_throughput: 1796.475\n",
      "    learn_time_ms: 2226.583\n",
      "    load_throughput: 26144952.47\n",
      "    load_time_ms: 0.153\n",
      "    sample_throughput: 519.815\n",
      "    sample_time_ms: 7695.049\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1671814844\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:50 (running for 00:02:07.99)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         117.601</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  472.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            472.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:00:55 (running for 00:02:12.99)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         117.601</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  472.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            472.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 482.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 482.86\n",
      "  episode_reward_min: 220.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 433\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.052637807427901494\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05290778100295871\n",
      "      mean_inference_ms: 0.5534441368940096\n",
      "      mean_raw_obs_processing_ms: 0.06968154582269963\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4374323785305023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005192064680159092\n",
      "          model: {}\n",
      "          policy_loss: -0.002257291693240404\n",
      "          total_loss: 512.8043212890625\n",
      "          vf_explained_var: 0.0721060112118721\n",
      "          vf_loss: 512.8065795898438\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.12307692307692\n",
      "    ram_util_percent: 10.953846153846152\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06103926367646007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.061562673144296504\n",
      "    mean_inference_ms: 0.6296286671980384\n",
      "    mean_raw_obs_processing_ms: 0.08864012019412684\n",
      "  time_since_restore: 127.05708646774292\n",
      "  time_this_iter_s: 9.456523180007935\n",
      "  time_total_s: 127.05708646774292\n",
      "  timers:\n",
      "    learn_throughput: 1863.69\n",
      "    learn_time_ms: 2146.279\n",
      "    load_throughput: 24899400.416\n",
      "    load_time_ms: 0.161\n",
      "    sample_throughput: 535.669\n",
      "    sample_time_ms: 7467.298\n",
      "    update_time_ms: 1.476\n",
      "  timestamp: 1671814857\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:00 (running for 00:02:18.46)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         127.057</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  482.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 220</td><td style=\"text-align: right;\">            482.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:05 (running for 00:02:23.56)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         130.168</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  482.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 220</td><td style=\"text-align: right;\">            482.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 489.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.18\n",
      "  episode_reward_min: 257.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 449\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 487.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 487.4\n",
      "    episode_reward_min: 323.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 435\n",
      "      - 490\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 323\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 435.0\n",
      "      - 490.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 323.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05178704419851252\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05188488723240366\n",
      "      mean_inference_ms: 0.5443420774844661\n",
      "      mean_raw_obs_processing_ms: 0.06803055985087698\n",
      "    timesteps_this_iter: 9748\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4378652274608612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007686077617108822\n",
      "          model: {}\n",
      "          policy_loss: -0.006527089513838291\n",
      "          total_loss: 495.7074890136719\n",
      "          vf_explained_var: -0.03124498575925827\n",
      "          vf_loss: 495.7140197753906\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.853846153846156\n",
      "    ram_util_percent: 11.069230769230767\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06051223929371023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06099187783173408\n",
      "    mean_inference_ms: 0.6242682954581852\n",
      "    mean_raw_obs_processing_ms: 0.08745366275671079\n",
      "  time_since_restore: 139.32432293891907\n",
      "  time_this_iter_s: 9.15665602684021\n",
      "  time_total_s: 139.32432293891907\n",
      "  timers:\n",
      "    learn_throughput: 1937.092\n",
      "    learn_time_ms: 2064.951\n",
      "    load_throughput: 26596727.964\n",
      "    load_time_ms: 0.15\n",
      "    sample_throughput: 558.24\n",
      "    sample_time_ms: 7165.377\n",
      "    update_time_ms: 1.454\n",
      "  timestamp: 1671814869\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:10 (running for 00:02:28.77)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         139.324</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  489.18</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            489.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:16 (running for 00:02:33.92)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">           142.5</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  487.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            487.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:21 (running for 00:02:38.95)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">           142.5</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  487.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            487.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 488.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.95\n",
      "  episode_reward_min: 257.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 465\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05105035541679466\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05100403084750606\n",
      "      mean_inference_ms: 0.5365439900302538\n",
      "      mean_raw_obs_processing_ms: 0.06663427446212536\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4224943220615387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00446663610637188\n",
      "          model: {}\n",
      "          policy_loss: -0.0035382923670113087\n",
      "          total_loss: 471.02880859375\n",
      "          vf_explained_var: 0.002572912024334073\n",
      "          vf_loss: 471.0323181152344\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.50714285714286\n",
      "    ram_util_percent: 11.085714285714284\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05981879434486653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06023049765174954\n",
      "    mean_inference_ms: 0.6174417685675333\n",
      "    mean_raw_obs_processing_ms: 0.08608033866363768\n",
      "  time_since_restore: 151.78837203979492\n",
      "  time_this_iter_s: 9.288418054580688\n",
      "  time_total_s: 151.78837203979492\n",
      "  timers:\n",
      "    learn_throughput: 2012.682\n",
      "    learn_time_ms: 1987.398\n",
      "    load_throughput: 25489541.173\n",
      "    load_time_ms: 0.157\n",
      "    sample_throughput: 581.544\n",
      "    sample_time_ms: 6878.242\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1671814882\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:26 (running for 00:02:44.36)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         154.906</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  488.95</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            488.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:31 (running for 00:02:49.39)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         154.906</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  488.95</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            488.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 490.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 490.43\n",
      "  episode_reward_min: 257.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 481\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05055235043741237\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05040855802583936\n",
      "      mean_inference_ms: 0.5311830073447711\n",
      "      mean_raw_obs_processing_ms: 0.06564332898117582\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.39691823720932007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005895981099456549\n",
      "          model: {}\n",
      "          policy_loss: -0.002193228807300329\n",
      "          total_loss: 485.4899597167969\n",
      "          vf_explained_var: -0.040944185107946396\n",
      "          vf_loss: 485.49212646484375\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.05714285714286\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05896440654441665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05928459330181564\n",
      "    mean_inference_ms: 0.6091811730563491\n",
      "    mean_raw_obs_processing_ms: 0.08448058024295435\n",
      "  time_since_restore: 164.37604975700378\n",
      "  time_this_iter_s: 9.470193862915039\n",
      "  time_total_s: 164.37604975700378\n",
      "  timers:\n",
      "    learn_throughput: 2092.643\n",
      "    learn_time_ms: 1911.458\n",
      "    load_throughput: 26491735.354\n",
      "    load_time_ms: 0.151\n",
      "    sample_throughput: 608.672\n",
      "    sample_time_ms: 6571.685\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1671814894\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:36 (running for 00:02:54.85)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         164.376</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  490.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            490.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:42 (running for 00:03:00.00)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         167.493</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  490.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            490.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:47 (running for 00:03:05.01)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         167.493</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  490.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            490.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 491.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 491.04\n",
      "  episode_reward_min: 257.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 497\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05002889675028899\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.049766230155987114\n",
      "      mean_inference_ms: 0.5254663280562731\n",
      "      mean_raw_obs_processing_ms: 0.06463352770750838\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38785749673843384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00726685207337141\n",
      "          model: {}\n",
      "          policy_loss: -0.0031882987823337317\n",
      "          total_loss: 488.7899169921875\n",
      "          vf_explained_var: -0.033597223460674286\n",
      "          vf_loss: 488.7930908203125\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.592307692307696\n",
      "    ram_util_percent: 11.2\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05799323753572518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05820367329065206\n",
      "    mean_inference_ms: 0.5998003007149554\n",
      "    mean_raw_obs_processing_ms: 0.08273214680441228\n",
      "  time_since_restore: 176.65130639076233\n",
      "  time_this_iter_s: 9.158474683761597\n",
      "  time_total_s: 176.65130639076233\n",
      "  timers:\n",
      "    learn_throughput: 2178.079\n",
      "    learn_time_ms: 1836.481\n",
      "    load_throughput: 28320756.246\n",
      "    load_time_ms: 0.141\n",
      "    sample_throughput: 635.128\n",
      "    sample_time_ms: 6297.942\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1671814907\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:52 (running for 00:03:10.34)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         179.784</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  494.95</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            494.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:01:57 (running for 00:03:15.34)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         179.784</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  494.95</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">            494.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 494.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.68\n",
      "  episode_reward_min: 125.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 514\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 499.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 499.6\n",
      "    episode_reward_min: 492.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 492\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 492.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04958424609776633\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04922372579136195\n",
      "      mean_inference_ms: 0.5206019799757171\n",
      "      mean_raw_obs_processing_ms: 0.06376408965397058\n",
      "    timesteps_this_iter: 9992\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4075930416584015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033920553978532553\n",
      "          model: {}\n",
      "          policy_loss: -0.0017783410148695111\n",
      "          total_loss: 450.54864501953125\n",
      "          vf_explained_var: -0.06395433843135834\n",
      "          vf_loss: 450.5504150390625\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.269230769230766\n",
      "    ram_util_percent: 11.0\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05685185229544265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05693626769236909\n",
      "    mean_inference_ms: 0.5887616026314636\n",
      "    mean_raw_obs_processing_ms: 0.0807442403354244\n",
      "  time_since_restore: 188.97269701957703\n",
      "  time_this_iter_s: 9.18867802619934\n",
      "  time_total_s: 188.97269701957703\n",
      "  timers:\n",
      "    learn_throughput: 2180.144\n",
      "    learn_time_ms: 1834.741\n",
      "    load_throughput: 29916576.32\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 641.309\n",
      "    sample_time_ms: 6237.239\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1671814919\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:02 (running for 00:03:20.55)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         188.973</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  494.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 125</td><td style=\"text-align: right;\">            494.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:07 (running for 00:03:25.62)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         192.066</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  494.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 125</td><td style=\"text-align: right;\">            494.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_0cb4c_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-02-11\n",
      "  done: false\n",
      "  episode_len_mean: 494.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.68\n",
      "  episode_reward_min: 125.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 530\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04918894752709225\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04875695842525478\n",
      "      mean_inference_ms: 0.516295117298338\n",
      "      mean_raw_obs_processing_ms: 0.06301168874508974\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4d335abe366c4dfaa5c3b9b17fe36c4f\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4293353855609894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008003028109669685\n",
      "          model: {}\n",
      "          policy_loss: -0.0005810376023873687\n",
      "          total_loss: 414.9165344238281\n",
      "          vf_explained_var: 0.03479130566120148\n",
      "          vf_loss: 414.9171142578125\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.261538461538464\n",
      "    ram_util_percent: 11.0\n",
      "  pid: 45494\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.055861613456728104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05582891039112004\n",
      "    mean_inference_ms: 0.579259899107964\n",
      "    mean_raw_obs_processing_ms: 0.07900177420495509\n",
      "  time_since_restore: 201.21778464317322\n",
      "  time_this_iter_s: 9.151511192321777\n",
      "  time_total_s: 201.21778464317322\n",
      "  timers:\n",
      "    learn_throughput: 2184.725\n",
      "    learn_time_ms: 1830.894\n",
      "    load_throughput: 29889926.955\n",
      "    load_time_ms: 0.134\n",
      "    sample_throughput: 644.381\n",
      "    sample_time_ms: 6207.511\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1671814931\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 0cb4c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:12 (running for 00:03:30.82)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         201.218</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  494.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 125</td><td style=\"text-align: right;\">            494.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:02:17,006\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:17 (running for 00:03:34.92)<br>Memory usage on this node: 3.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_0cb4c_00000</td><td>RUNNING </td><td>192.168.0.98:45494</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">           204.3</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  494.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 125</td><td style=\"text-align: right;\">            494.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:02:17,242\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_0cb4c_00000]\n",
      "2022-12-23 17:02:17,242\tINFO tune.py:639 -- Total run time: 217.17 seconds (214.89 seconds for the tuning loop).\n",
      "2022-12-23 17:02:17,243\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f1a9e666df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aac37a-e712-47a9-88e2-d13b029f2edd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing a different results directory\n",
    "\n",
    "- Can be set via the `tune.run()` argument `local_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db158dd2-5f3e-4132-b641-2e11ff95383b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:23 (running for 00:00:00.11)<br>Memory usage on this node: 2.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:25,247\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:25,248\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:25,248\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:25,248\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:28,492\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:29 (running for 00:00:05.79)<br>Memory usage on this node: 3.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:29,101\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=45490)\u001b[0m 2022-12-23 17:02:30,435\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 23.151162790697676\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 78.0\n",
      "  episode_reward_mean: 23.151162790697676\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 172\n",
      "  episodes_total: 172\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6662864685058594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027451276779174805\n",
      "          model: {}\n",
      "          policy_loss: -0.03879998251795769\n",
      "          total_loss: 207.1736602783203\n",
      "          vf_explained_var: 0.02129075862467289\n",
      "          vf_loss: 207.2069549560547\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.739999999999995\n",
      "    ram_util_percent: 10.34\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04467351742829757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04399460235397439\n",
      "    mean_inference_ms: 0.4861982746877294\n",
      "    mean_raw_obs_processing_ms: 0.07242634557355113\n",
      "  time_since_restore: 3.3358592987060547\n",
      "  time_this_iter_s: 3.3358592987060547\n",
      "  time_total_s: 3.3358592987060547\n",
      "  timers:\n",
      "    learn_throughput: 2011.649\n",
      "    learn_time_ms: 1988.418\n",
      "    load_throughput: 29694187.611\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 2057.415\n",
      "    sample_time_ms: 1944.188\n",
      "    update_time_ms: 1.419\n",
      "  timestamp: 1671814952\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:33 (running for 00:00:10.16)<br>Memory usage on this node: 3.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.33586</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 23.1512</td><td style=\"text-align: right;\">                  78</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           23.1512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:38 (running for 00:00:15.62)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.79927</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   42.87</td><td style=\"text-align: right;\">                 125</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             42.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-02-40\n",
      "  done: false\n",
      "  episode_len_mean: 70.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 338.0\n",
      "  episode_reward_mean: 70.57\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 294\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5787163376808167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010323881171643734\n",
      "          model: {}\n",
      "          policy_loss: -0.021338608115911484\n",
      "          total_loss: 665.2346801757812\n",
      "          vf_explained_var: 0.1544189602136612\n",
      "          vf_loss: 665.2528686523438\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.925\n",
      "    ram_util_percent: 10.4\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04397175135148865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043303955273348675\n",
      "    mean_inference_ms: 0.4742517524431266\n",
      "    mean_raw_obs_processing_ms: 0.06755435757730849\n",
      "  time_since_restore: 10.89902925491333\n",
      "  time_this_iter_s: 3.0997631549835205\n",
      "  time_total_s: 10.89902925491333\n",
      "  timers:\n",
      "    learn_throughput: 2116.667\n",
      "    learn_time_ms: 1889.763\n",
      "    load_throughput: 26987478.82\n",
      "    load_time_ms: 0.148\n",
      "    sample_throughput: 1236.488\n",
      "    sample_time_ms: 3234.969\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1671814960\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:44 (running for 00:00:20.76)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          10.899</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   70.57</td><td style=\"text-align: right;\">                 338</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             70.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 98.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 359.0\n",
      "  episode_reward_mean: 98.68\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 313\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 267.2\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 497.0\n",
      "    episode_reward_mean: 267.2\n",
      "    episode_reward_min: 101.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 241\n",
      "      - 265\n",
      "      - 497\n",
      "      - 257\n",
      "      - 307\n",
      "      - 325\n",
      "      - 277\n",
      "      - 279\n",
      "      - 204\n",
      "      - 191\n",
      "      - 227\n",
      "      - 443\n",
      "      - 261\n",
      "      - 202\n",
      "      - 224\n",
      "      - 274\n",
      "      - 160\n",
      "      - 101\n",
      "      - 239\n",
      "      - 370\n",
      "      episode_reward:\n",
      "      - 241.0\n",
      "      - 265.0\n",
      "      - 497.0\n",
      "      - 257.0\n",
      "      - 307.0\n",
      "      - 325.0\n",
      "      - 277.0\n",
      "      - 279.0\n",
      "      - 204.0\n",
      "      - 191.0\n",
      "      - 227.0\n",
      "      - 443.0\n",
      "      - 261.0\n",
      "      - 202.0\n",
      "      - 224.0\n",
      "      - 274.0\n",
      "      - 160.0\n",
      "      - 101.0\n",
      "      - 239.0\n",
      "      - 370.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04353683305379699\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04272554672539122\n",
      "      mean_inference_ms: 0.46076446843357716\n",
      "      mean_raw_obs_processing_ms: 0.054503163095144205\n",
      "    timesteps_this_iter: 5344\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.584316611289978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004786294419318438\n",
      "          model: {}\n",
      "          policy_loss: -0.01264077890664339\n",
      "          total_loss: 763.6520385742188\n",
      "          vf_explained_var: 0.3030892312526703\n",
      "          vf_loss: 763.6631469726562\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.788888888888888\n",
      "    ram_util_percent: 10.4\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04383061459083454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04311611040288403\n",
      "    mean_inference_ms: 0.4726333446643028\n",
      "    mean_raw_obs_processing_ms: 0.06660969696152001\n",
      "  time_since_restore: 17.1707284450531\n",
      "  time_this_iter_s: 6.2716991901397705\n",
      "  time_total_s: 17.1707284450531\n",
      "  timers:\n",
      "    learn_throughput: 2132.797\n",
      "    learn_time_ms: 1875.471\n",
      "    load_throughput: 28826831.615\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 1249.641\n",
      "    sample_time_ms: 3200.919\n",
      "    update_time_ms: 1.44\n",
      "  timestamp: 1671814966\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:49 (running for 00:00:26.03)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         17.1707</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   98.68</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             98.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:54 (running for 00:00:31.15)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.2545</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.45</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            130.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 163.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 163.64\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 350\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 249.25\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 464.0\n",
      "    episode_reward_mean: 249.25\n",
      "    episode_reward_min: 112.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 360\n",
      "      - 290\n",
      "      - 237\n",
      "      - 223\n",
      "      - 112\n",
      "      - 361\n",
      "      - 305\n",
      "      - 169\n",
      "      - 176\n",
      "      - 464\n",
      "      - 204\n",
      "      - 285\n",
      "      - 194\n",
      "      - 231\n",
      "      - 269\n",
      "      - 223\n",
      "      - 177\n",
      "      - 183\n",
      "      - 229\n",
      "      - 293\n",
      "      episode_reward:\n",
      "      - 360.0\n",
      "      - 290.0\n",
      "      - 237.0\n",
      "      - 223.0\n",
      "      - 112.0\n",
      "      - 361.0\n",
      "      - 305.0\n",
      "      - 169.0\n",
      "      - 176.0\n",
      "      - 464.0\n",
      "      - 204.0\n",
      "      - 285.0\n",
      "      - 194.0\n",
      "      - 231.0\n",
      "      - 269.0\n",
      "      - 223.0\n",
      "      - 177.0\n",
      "      - 183.0\n",
      "      - 229.0\n",
      "      - 293.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04345418208771989\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.042655289503120655\n",
      "      mean_inference_ms: 0.45786980573336916\n",
      "      mean_raw_obs_processing_ms: 0.054038815367119485\n",
      "    timesteps_this_iter: 4985\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.55582195520401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031904748175293207\n",
      "          model: {}\n",
      "          policy_loss: -0.007195560727268457\n",
      "          total_loss: 310.9983825683594\n",
      "          vf_explained_var: 0.5029656887054443\n",
      "          vf_loss: 311.0050964355469\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.95555555555556\n",
      "    ram_util_percent: 10.4\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043726720217354506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04309778509064657\n",
      "    mean_inference_ms: 0.46976778136378994\n",
      "    mean_raw_obs_processing_ms: 0.06464641402705044\n",
      "  time_since_restore: 26.319777965545654\n",
      "  time_this_iter_s: 6.0653235912323\n",
      "  time_total_s: 26.319777965545654\n",
      "  timers:\n",
      "    learn_throughput: 2147.222\n",
      "    learn_time_ms: 1862.872\n",
      "    load_throughput: 30624671.737\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1081.372\n",
      "    sample_time_ms: 3699.005\n",
      "    update_time_ms: 1.435\n",
      "  timestamp: 1671814975\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:02:59 (running for 00:00:36.34)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         29.4307</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  189.98</td><td style=\"text-align: right;\">                 452</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            189.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 208.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 208.08\n",
      "  episode_reward_min: 22.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 385\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 230.7\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 281.0\n",
      "    episode_reward_mean: 230.7\n",
      "    episode_reward_min: 188.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 226\n",
      "      - 249\n",
      "      - 259\n",
      "      - 238\n",
      "      - 203\n",
      "      - 257\n",
      "      - 281\n",
      "      - 227\n",
      "      - 202\n",
      "      - 249\n",
      "      - 252\n",
      "      - 249\n",
      "      - 206\n",
      "      - 227\n",
      "      - 240\n",
      "      - 188\n",
      "      - 211\n",
      "      - 216\n",
      "      - 232\n",
      "      - 202\n",
      "      episode_reward:\n",
      "      - 226.0\n",
      "      - 249.0\n",
      "      - 259.0\n",
      "      - 238.0\n",
      "      - 203.0\n",
      "      - 257.0\n",
      "      - 281.0\n",
      "      - 227.0\n",
      "      - 202.0\n",
      "      - 249.0\n",
      "      - 252.0\n",
      "      - 249.0\n",
      "      - 206.0\n",
      "      - 227.0\n",
      "      - 240.0\n",
      "      - 188.0\n",
      "      - 211.0\n",
      "      - 216.0\n",
      "      - 232.0\n",
      "      - 202.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04340621258884821\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0426063648171829\n",
      "      mean_inference_ms: 0.4568389674333749\n",
      "      mean_raw_obs_processing_ms: 0.05387156938245817\n",
      "    timesteps_this_iter: 4614\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5289879441261292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006719144061207771\n",
      "          model: {}\n",
      "          policy_loss: -0.005676810164004564\n",
      "          total_loss: 86.89659118652344\n",
      "          vf_explained_var: 0.7670581936836243\n",
      "          vf_loss: 86.90201568603516\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.0375\n",
      "    ram_util_percent: 10.4\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04358207297699705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04295000348669758\n",
      "    mean_inference_ms: 0.46763063943369476\n",
      "    mean_raw_obs_processing_ms: 0.0630489977294828\n",
      "  time_since_restore: 35.31042408943176\n",
      "  time_this_iter_s: 5.879741668701172\n",
      "  time_total_s: 35.31042408943176\n",
      "  timers:\n",
      "    learn_throughput: 2152.688\n",
      "    learn_time_ms: 1858.142\n",
      "    load_throughput: 31640199.906\n",
      "    load_time_ms: 0.126\n",
      "    sample_throughput: 1017.882\n",
      "    sample_time_ms: 3929.729\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1671814984\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:05 (running for 00:00:42.27)<br>Memory usage on this node: 3.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         35.3104</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  208.08</td><td style=\"text-align: right;\">                 452</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            208.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:10 (running for 00:00:47.47)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          38.522</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">     219</td><td style=\"text-align: right;\">                 452</td><td style=\"text-align: right;\">                  41</td><td style=\"text-align: right;\">               219</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-03-14\n",
      "  done: false\n",
      "  episode_len_mean: 227.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 227.33\n",
      "  episode_reward_min: 119.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 421\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 285.3\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 387.0\n",
      "    episode_reward_mean: 285.3\n",
      "    episode_reward_min: 207.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 387\n",
      "      - 319\n",
      "      - 248\n",
      "      - 378\n",
      "      - 291\n",
      "      - 366\n",
      "      - 285\n",
      "      - 207\n",
      "      - 263\n",
      "      - 272\n",
      "      - 277\n",
      "      - 243\n",
      "      - 264\n",
      "      - 284\n",
      "      - 221\n",
      "      - 269\n",
      "      - 314\n",
      "      - 306\n",
      "      - 267\n",
      "      - 245\n",
      "      episode_reward:\n",
      "      - 387.0\n",
      "      - 319.0\n",
      "      - 248.0\n",
      "      - 378.0\n",
      "      - 291.0\n",
      "      - 366.0\n",
      "      - 285.0\n",
      "      - 207.0\n",
      "      - 263.0\n",
      "      - 272.0\n",
      "      - 277.0\n",
      "      - 243.0\n",
      "      - 264.0\n",
      "      - 284.0\n",
      "      - 221.0\n",
      "      - 269.0\n",
      "      - 314.0\n",
      "      - 306.0\n",
      "      - 267.0\n",
      "      - 245.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.043384699094590085\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04260844428352062\n",
      "      mean_inference_ms: 0.45677957650739553\n",
      "      mean_raw_obs_processing_ms: 0.0536777093821443\n",
      "    timesteps_this_iter: 5706\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5171896815299988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011463165283203125\n",
      "          model: {}\n",
      "          policy_loss: -0.005328778177499771\n",
      "          total_loss: 35.63433837890625\n",
      "          vf_explained_var: 0.8961784243583679\n",
      "          vf_loss: 35.63945007324219\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.933333333333337\n",
      "    ram_util_percent: 10.944444444444445\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043645880348637665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04305868944501068\n",
      "    mean_inference_ms: 0.4676881639354305\n",
      "    mean_raw_obs_processing_ms: 0.062254608925631845\n",
      "  time_since_restore: 45.322410583496094\n",
      "  time_this_iter_s: 6.8004539012908936\n",
      "  time_total_s: 45.322410583496094\n",
      "  timers:\n",
      "    learn_throughput: 2128.893\n",
      "    learn_time_ms: 1878.911\n",
      "    load_throughput: 31889785.212\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 983.032\n",
      "    sample_time_ms: 4069.042\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1671814994\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:16 (running for 00:00:53.31)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         45.3224</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  227.33</td><td style=\"text-align: right;\">                 452</td><td style=\"text-align: right;\">                 119</td><td style=\"text-align: right;\">            227.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:21 (running for 00:00:58.44)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         48.4593</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  237.55</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 119</td><td style=\"text-align: right;\">            237.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 247.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 247.5\n",
      "  episode_reward_min: 164.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 446\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 376.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 376.85\n",
      "    episode_reward_min: 227.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 327\n",
      "      - 383\n",
      "      - 350\n",
      "      - 313\n",
      "      - 403\n",
      "      - 227\n",
      "      - 432\n",
      "      - 285\n",
      "      - 500\n",
      "      - 300\n",
      "      - 404\n",
      "      - 372\n",
      "      - 500\n",
      "      - 343\n",
      "      - 401\n",
      "      - 444\n",
      "      - 363\n",
      "      - 500\n",
      "      - 343\n",
      "      - 347\n",
      "      episode_reward:\n",
      "      - 327.0\n",
      "      - 383.0\n",
      "      - 350.0\n",
      "      - 313.0\n",
      "      - 403.0\n",
      "      - 227.0\n",
      "      - 432.0\n",
      "      - 285.0\n",
      "      - 500.0\n",
      "      - 300.0\n",
      "      - 404.0\n",
      "      - 372.0\n",
      "      - 500.0\n",
      "      - 343.0\n",
      "      - 401.0\n",
      "      - 444.0\n",
      "      - 363.0\n",
      "      - 500.0\n",
      "      - 343.0\n",
      "      - 347.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.043384404484389\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0425682061580366\n",
      "      mean_inference_ms: 0.45584576268309346\n",
      "      mean_raw_obs_processing_ms: 0.053411187471374674\n",
      "    timesteps_this_iter: 7537\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5168083310127258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002325814450159669\n",
      "          model: {}\n",
      "          policy_loss: -0.0044751535169780254\n",
      "          total_loss: 38.45918273925781\n",
      "          vf_explained_var: 0.7764595150947571\n",
      "          vf_loss: 38.46360778808594\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.50909090909091\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04377947425638611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043222027381884104\n",
      "    mean_inference_ms: 0.468691650175217\n",
      "    mean_raw_obs_processing_ms: 0.062070463151499604\n",
      "  time_since_restore: 56.01310729980469\n",
      "  time_this_iter_s: 7.553805589675903\n",
      "  time_total_s: 56.01310729980469\n",
      "  timers:\n",
      "    learn_throughput: 2143.704\n",
      "    learn_time_ms: 1865.93\n",
      "    load_throughput: 34800282.099\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 883.89\n",
      "    sample_time_ms: 4525.451\n",
      "    update_time_ms: 1.373\n",
      "  timestamp: 1671815005\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:27 (running for 00:01:04.01)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         56.0131</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   247.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 164</td><td style=\"text-align: right;\">             247.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:32 (running for 00:01:09.10)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         59.0906</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  263.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 164</td><td style=\"text-align: right;\">            263.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:37 (running for 00:01:14.12)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         59.0906</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  263.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 164</td><td style=\"text-align: right;\">            263.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 282.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 282.76\n",
      "  episode_reward_min: 166.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 464\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04335470970660919\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04259308888798667\n",
      "      mean_inference_ms: 0.45561264786455363\n",
      "      mean_raw_obs_processing_ms: 0.05325784995442344\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5117801427841187\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004781919065862894\n",
      "          model: {}\n",
      "          policy_loss: -0.002107262844219804\n",
      "          total_loss: 290.9099426269531\n",
      "          vf_explained_var: 0.23910996317863464\n",
      "          vf_loss: 290.9119873046875\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.50769230769231\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043835792861245546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04329029350228472\n",
      "    mean_inference_ms: 0.4691867028732648\n",
      "    mean_raw_obs_processing_ms: 0.06192383723271329\n",
      "  time_since_restore: 68.14923000335693\n",
      "  time_this_iter_s: 9.058642864227295\n",
      "  time_total_s: 68.14923000335693\n",
      "  timers:\n",
      "    learn_throughput: 2143.291\n",
      "    learn_time_ms: 1866.289\n",
      "    load_throughput: 34450135.524\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 826.94\n",
      "    sample_time_ms: 4837.111\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1671815017\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:42 (running for 00:01:19.28)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         71.2348</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  304.88</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            304.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:47 (running for 00:01:24.31)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         71.2348</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  304.88</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            304.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 327.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 327.83\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 480\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04334926415702687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04257174572997706\n",
      "      mean_inference_ms: 0.4553999923560501\n",
      "      mean_raw_obs_processing_ms: 0.053156222940817933\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5494793057441711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004902753978967667\n",
      "          model: {}\n",
      "          policy_loss: -0.0023761242628097534\n",
      "          total_loss: 240.71212768554688\n",
      "          vf_explained_var: 0.38506677746772766\n",
      "          vf_loss: 240.7145233154297\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.36923076923077\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043852136446659085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04330676863310229\n",
      "    mean_inference_ms: 0.469332051129016\n",
      "    mean_raw_obs_processing_ms: 0.06174477340417596\n",
      "  time_since_restore: 80.26283383369446\n",
      "  time_this_iter_s: 9.028040170669556\n",
      "  time_total_s: 80.26283383369446\n",
      "  timers:\n",
      "    learn_throughput: 2143.889\n",
      "    learn_time_ms: 1865.768\n",
      "    load_throughput: 30344033.279\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 782.139\n",
      "    sample_time_ms: 5114.18\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1671815029\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:52 (running for 00:01:29.33)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         80.2628</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  327.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            327.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:03:57 (running for 00:01:34.49)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         83.3824</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   349.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">             349.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-04-01\n",
      "  done: false\n",
      "  episode_len_mean: 371.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 371.36\n",
      "  episode_reward_min: 168.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 497\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.043339734842669744\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04256711951617538\n",
      "      mean_inference_ms: 0.45534251776551066\n",
      "      mean_raw_obs_processing_ms: 0.053071801795251186\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47718480229377747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004710063338279724\n",
      "          model: {}\n",
      "          policy_loss: 0.0010850977851077914\n",
      "          total_loss: 345.5923767089844\n",
      "          vf_explained_var: 0.2313084453344345\n",
      "          vf_loss: 345.5912780761719\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.25384615384616\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04385947571064989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04332111506702196\n",
      "    mean_inference_ms: 0.46941637923579527\n",
      "    mean_raw_obs_processing_ms: 0.06153517168942635\n",
      "  time_since_restore: 92.42724227905273\n",
      "  time_this_iter_s: 9.044833421707153\n",
      "  time_total_s: 92.42724227905273\n",
      "  timers:\n",
      "    learn_throughput: 2144.064\n",
      "    learn_time_ms: 1865.616\n",
      "    load_throughput: 28683904.941\n",
      "    load_time_ms: 0.139\n",
      "    sample_throughput: 739.582\n",
      "    sample_time_ms: 5408.464\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1671815041\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:02 (running for 00:01:39.53)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         92.4272</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  371.36</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            371.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:08 (running for 00:01:44.72)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         95.5769</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  393.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            393.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:13 (running for 00:01:49.72)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         95.5769</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  393.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 168</td><td style=\"text-align: right;\">            393.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 416.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 416.25\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 513\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04343664754644053\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04267527449524308\n",
      "      mean_inference_ms: 0.4564027500098341\n",
      "      mean_raw_obs_processing_ms: 0.05314611534209788\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4714004695415497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003993453457951546\n",
      "          model: {}\n",
      "          policy_loss: 0.0001520396617706865\n",
      "          total_loss: 423.7546691894531\n",
      "          vf_explained_var: -0.07950786501169205\n",
      "          vf_loss: 423.7545471191406\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.092307692307692\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043797940187429774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04324728974480015\n",
      "    mean_inference_ms: 0.4690000134410514\n",
      "    mean_raw_obs_processing_ms: 0.061234795857487964\n",
      "  time_since_restore: 104.74563908576965\n",
      "  time_this_iter_s: 9.168755292892456\n",
      "  time_total_s: 104.74563908576965\n",
      "  timers:\n",
      "    learn_throughput: 2164.15\n",
      "    learn_time_ms: 1848.301\n",
      "    load_throughput: 26027328.576\n",
      "    load_time_ms: 0.154\n",
      "    sample_throughput: 700.713\n",
      "    sample_time_ms: 5708.469\n",
      "    update_time_ms: 1.412\n",
      "  timestamp: 1671815054\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:18 (running for 00:01:54.98)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         107.838</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">   437.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">             437.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:23 (running for 00:01:59.99)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         107.838</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">   437.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">             437.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 452.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 452.84\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 529\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.043422737682008175\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04265507557002671\n",
      "      mean_inference_ms: 0.4562508893677911\n",
      "      mean_raw_obs_processing_ms: 0.05308165552606621\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4429979622364044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060556307435035706\n",
      "          model: {}\n",
      "          policy_loss: -0.0003340954426676035\n",
      "          total_loss: 548.2285766601562\n",
      "          vf_explained_var: -0.24296851456165314\n",
      "          vf_loss: 548.2288818359375\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.27692307692308\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043712571798638515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04316308843942986\n",
      "    mean_inference_ms: 0.4682130934293254\n",
      "    mean_raw_obs_processing_ms: 0.060882076186142046\n",
      "  time_since_restore: 116.89160561561584\n",
      "  time_this_iter_s: 9.05402660369873\n",
      "  time_total_s: 116.89160561561584\n",
      "  timers:\n",
      "    learn_throughput: 2166.901\n",
      "    learn_time_ms: 1845.954\n",
      "    load_throughput: 25995066.625\n",
      "    load_time_ms: 0.154\n",
      "    sample_throughput: 671.515\n",
      "    sample_time_ms: 5956.677\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1671815066\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:28 (running for 00:02:05.06)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         116.892</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  452.84</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            452.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:33 (running for 00:02:10.17)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          119.99</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  470.55</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            470.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-04-37\n",
      "  done: false\n",
      "  episode_len_mean: 485.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.23\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 545\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 382.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 382.6\n",
      "    episode_reward_min: 141.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 376\n",
      "      - 398\n",
      "      - 275\n",
      "      - 465\n",
      "      - 298\n",
      "      - 420\n",
      "      - 500\n",
      "      - 477\n",
      "      - 500\n",
      "      - 407\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 218\n",
      "      - 288\n",
      "      - 443\n",
      "      - 325\n",
      "      - 423\n",
      "      - 141\n",
      "      - 299\n",
      "      episode_reward:\n",
      "      - 376.0\n",
      "      - 398.0\n",
      "      - 275.0\n",
      "      - 465.0\n",
      "      - 298.0\n",
      "      - 420.0\n",
      "      - 500.0\n",
      "      - 477.0\n",
      "      - 500.0\n",
      "      - 407.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 218.0\n",
      "      - 288.0\n",
      "      - 443.0\n",
      "      - 325.0\n",
      "      - 423.0\n",
      "      - 141.0\n",
      "      - 299.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.043425581462060325\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04264505412630293\n",
      "      mean_inference_ms: 0.45611216546796135\n",
      "      mean_raw_obs_processing_ms: 0.05306819719685672\n",
      "    timesteps_this_iter: 7652\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4990483224391937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033567126374691725\n",
      "          model: {}\n",
      "          policy_loss: 0.00022396130952984095\n",
      "          total_loss: 498.8554992675781\n",
      "          vf_explained_var: -0.14829346537590027\n",
      "          vf_loss: 498.8553161621094\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.354545454545452\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0436391499465031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043094774630458695\n",
      "    mean_inference_ms: 0.4675944795692921\n",
      "    mean_raw_obs_processing_ms: 0.06056294480800393\n",
      "  time_since_restore: 127.69650316238403\n",
      "  time_this_iter_s: 7.706623315811157\n",
      "  time_total_s: 127.69650316238403\n",
      "  timers:\n",
      "    learn_throughput: 2165.038\n",
      "    learn_time_ms: 1847.543\n",
      "    load_throughput: 25866814.678\n",
      "    load_time_ms: 0.155\n",
      "    sample_throughput: 654.556\n",
      "    sample_time_ms: 6111.011\n",
      "    update_time_ms: 1.356\n",
      "  timestamp: 1671815077\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:39 (running for 00:02:15.90)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         127.697</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  485.23</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            485.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:44 (running for 00:02:21.02)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         130.804</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  483.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            483.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:49 (running for 00:02:26.04)<br>Memory usage on this node: 3.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         130.804</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  483.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            483.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 482.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 482.92\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 563\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04342152012989441\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04262045784480953\n",
      "      mean_inference_ms: 0.4559859042704404\n",
      "      mean_raw_obs_processing_ms: 0.05303335254064737\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46178361773490906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004750081337988377\n",
      "          model: {}\n",
      "          policy_loss: -0.0034458301961421967\n",
      "          total_loss: 403.60284423828125\n",
      "          vf_explained_var: -0.055334169417619705\n",
      "          vf_loss: 403.6063232421875\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.807692307692307\n",
      "    ram_util_percent: 11.099999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04357826973171171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043030758923138526\n",
      "    mean_inference_ms: 0.4671964866087771\n",
      "    mean_raw_obs_processing_ms: 0.060293331948004444\n",
      "  time_since_restore: 139.84445548057556\n",
      "  time_this_iter_s: 9.040372848510742\n",
      "  time_total_s: 139.84445548057556\n",
      "  timers:\n",
      "    learn_throughput: 2165.369\n",
      "    learn_time_ms: 1847.26\n",
      "    load_throughput: 27284462.514\n",
      "    load_time_ms: 0.147\n",
      "    sample_throughput: 669.515\n",
      "    sample_time_ms: 5974.474\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671815089\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:54 (running for 00:02:31.19)<br>Memory usage on this node: 3.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          142.94</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  482.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            482.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:04:59 (running for 00:02:36.23)<br>Memory usage on this node: 4.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          142.94</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  482.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            482.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 483.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 483.13\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 579\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04380117750325066\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.043072882628858646\n",
      "      mean_inference_ms: 0.46322644982390776\n",
      "      mean_raw_obs_processing_ms: 0.05376794279666129\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4401317238807678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004145447630435228\n",
      "          model: {}\n",
      "          policy_loss: -0.0021601009648293257\n",
      "          total_loss: 396.1322021484375\n",
      "          vf_explained_var: -0.11631901562213898\n",
      "          vf_loss: 396.13433837890625\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.07333333333334\n",
      "    ram_util_percent: 13.179999999999998\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04354064431073978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04299401615069755\n",
      "    mean_inference_ms: 0.46696927471719635\n",
      "    mean_raw_obs_processing_ms: 0.06011573001717613\n",
      "  time_since_restore: 153.57497310638428\n",
      "  time_this_iter_s: 10.635316848754883\n",
      "  time_total_s: 153.57497310638428\n",
      "  timers:\n",
      "    learn_throughput: 2102.474\n",
      "    learn_time_ms: 1902.521\n",
      "    load_throughput: 28669200.273\n",
      "    load_time_ms: 0.14\n",
      "    sample_throughput: 668.817\n",
      "    sample_time_ms: 5980.713\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1671815103\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:05 (running for 00:02:41.86)<br>Memory usage on this node: 4.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         153.575</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  483.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            483.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:10 (running for 00:02:47.66)<br>Memory usage on this node: 4.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         157.336</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:15 (running for 00:02:52.66)<br>Memory usage on this node: 4.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         157.336</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 486.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.44\n",
      "  episode_reward_min: 182.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 595\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.044545948503380645\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0439907742739815\n",
      "      mean_inference_ms: 0.47135418163805687\n",
      "      mean_raw_obs_processing_ms: 0.05519261167933222\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3848530352115631\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005204080604016781\n",
      "          model: {}\n",
      "          policy_loss: -0.0019128724234178662\n",
      "          total_loss: 256.19818115234375\n",
      "          vf_explained_var: 0.06299818307161331\n",
      "          vf_loss: 256.2000732421875\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.2125\n",
      "    ram_util_percent: 14.006250000000001\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04361821314524294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04308427553196012\n",
      "    mean_inference_ms: 0.4678886295813628\n",
      "    mean_raw_obs_processing_ms: 0.06015884375134237\n",
      "  time_since_restore: 168.54994940757751\n",
      "  time_this_iter_s: 11.214407920837402\n",
      "  time_total_s: 168.54994940757751\n",
      "  timers:\n",
      "    learn_throughput: 2037.021\n",
      "    learn_time_ms: 1963.652\n",
      "    load_throughput: 28135529.096\n",
      "    load_time_ms: 0.142\n",
      "    sample_throughput: 642.196\n",
      "    sample_time_ms: 6228.626\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1671815118\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:21 (running for 00:02:57.87)<br>Memory usage on this node: 4.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          168.55</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:26 (running for 00:03:03.62)<br>Memory usage on this node: 4.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         172.273</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:31 (running for 00:03:08.65)<br>Memory usage on this node: 4.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         172.273</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-05-33\n",
      "  done: false\n",
      "  episode_len_mean: 486.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.44\n",
      "  episode_reward_min: 182.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 611\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04512452976383123\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04472260968047912\n",
      "      mean_inference_ms: 0.4777925409388499\n",
      "      mean_raw_obs_processing_ms: 0.056392797825950726\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.43780893087387085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002145330421626568\n",
      "          model: {}\n",
      "          policy_loss: -0.0007444307557307184\n",
      "          total_loss: 258.8792419433594\n",
      "          vf_explained_var: 0.22829438745975494\n",
      "          vf_loss: 258.8800048828125\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.15\n",
      "    ram_util_percent: 13.9\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043835709426301335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04333943148590498\n",
      "    mean_inference_ms: 0.470076174340338\n",
      "    mean_raw_obs_processing_ms: 0.06045382786965777\n",
      "  time_since_restore: 183.34960460662842\n",
      "  time_this_iter_s: 11.076725482940674\n",
      "  time_total_s: 183.34960460662842\n",
      "  timers:\n",
      "    learn_throughput: 1979.004\n",
      "    learn_time_ms: 2021.219\n",
      "    load_throughput: 25377728.029\n",
      "    load_time_ms: 0.158\n",
      "    sample_throughput: 615.467\n",
      "    sample_time_ms: 6499.134\n",
      "    update_time_ms: 1.435\n",
      "  timestamp: 1671815133\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:37 (running for 00:03:14.49)<br>Memory usage on this node: 4.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         187.102</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:42 (running for 00:03:19.52)<br>Memory usage on this node: 4.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         187.102</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:47 (running for 00:03:24.52)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         187.102</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  486.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            486.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 485.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.91\n",
      "  episode_reward_min: 182.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 628\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 492.5\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 492.5\n",
      "    episode_reward_min: 381.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 469\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 381\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 469.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 381.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04583546237608157\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.045566431447893636\n",
      "      mean_inference_ms: 0.48572756147317514\n",
      "      mean_raw_obs_processing_ms: 0.05767425384545841\n",
      "    timesteps_this_iter: 9850\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4462510347366333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004479327704757452\n",
      "          model: {}\n",
      "          policy_loss: -0.004477151669561863\n",
      "          total_loss: 361.80584716796875\n",
      "          vf_explained_var: -0.12067339569330215\n",
      "          vf_loss: 361.8102722167969\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.1375\n",
      "    ram_util_percent: 14.100000000000001\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0441868221092792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043749707859326845\n",
      "    mean_inference_ms: 0.47356741208168435\n",
      "    mean_raw_obs_processing_ms: 0.0609890964059135\n",
      "  time_since_restore: 198.51412272453308\n",
      "  time_this_iter_s: 11.41228175163269\n",
      "  time_total_s: 198.51412272453308\n",
      "  timers:\n",
      "    learn_throughput: 1923.594\n",
      "    learn_time_ms: 2079.441\n",
      "    load_throughput: 24371319.001\n",
      "    load_time_ms: 0.164\n",
      "    sample_throughput: 591.448\n",
      "    sample_time_ms: 6763.068\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1671815148\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:53 (running for 00:03:29.73)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         202.304</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  484.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            484.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:05:58 (running for 00:03:34.74)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         202.304</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  484.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            484.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:03 (running for 00:03:39.77)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         202.304</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  484.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            484.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 484.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.46\n",
      "  episode_reward_min: 182.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 645\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.046245759309367994\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.046088261952754846\n",
      "      mean_inference_ms: 0.4901966597878392\n",
      "      mean_raw_obs_processing_ms: 0.058578289420775885\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4060876667499542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003464168170467019\n",
      "          model: {}\n",
      "          policy_loss: -0.0012414673110470176\n",
      "          total_loss: 436.1813659667969\n",
      "          vf_explained_var: 0.006094952113926411\n",
      "          vf_loss: 436.1825866699219\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.768750000000004\n",
      "    ram_util_percent: 14.6\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04464019444738392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04428332191730186\n",
      "    mean_inference_ms: 0.477967775287161\n",
      "    mean_raw_obs_processing_ms: 0.06170670069721682\n",
      "  time_since_restore: 213.39569330215454\n",
      "  time_this_iter_s: 11.091558694839478\n",
      "  time_total_s: 213.39569330215454\n",
      "  timers:\n",
      "    learn_throughput: 1862.852\n",
      "    learn_time_ms: 2147.246\n",
      "    load_throughput: 25497288.754\n",
      "    load_time_ms: 0.157\n",
      "    sample_throughput: 554.907\n",
      "    sample_time_ms: 7208.419\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1671815163\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:08 (running for 00:03:45.64)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         217.161</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:13 (running for 00:03:50.67)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         217.161</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 357.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 661\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.046605594685913646\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04653481478395672\n",
      "      mean_inference_ms: 0.49415140147989345\n",
      "      mean_raw_obs_processing_ms: 0.05932499671078741\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762788286319847e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.401561975479126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006115918513387442\n",
      "          model: {}\n",
      "          policy_loss: -0.005838355049490929\n",
      "          total_loss: 168.950439453125\n",
      "          vf_explained_var: 0.29832062125205994\n",
      "          vf_loss: 168.95626831054688\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.412499999999994\n",
      "    ram_util_percent: 14.6\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04514360265846866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04486318307162897\n",
      "    mean_inference_ms: 0.48293342209304385\n",
      "    mean_raw_obs_processing_ms: 0.06253715162815424\n",
      "  time_since_restore: 228.227454662323\n",
      "  time_this_iter_s: 11.066564559936523\n",
      "  time_total_s: 228.227454662323\n",
      "  timers:\n",
      "    learn_throughput: 1857.519\n",
      "    learn_time_ms: 2153.41\n",
      "    load_throughput: 23471203.134\n",
      "    load_time_ms: 0.17\n",
      "    sample_throughput: 535.281\n",
      "    sample_time_ms: 7472.712\n",
      "    update_time_ms: 1.464\n",
      "  timestamp: 1671815178\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:19 (running for 00:03:55.73)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         228.227</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:24 (running for 00:04:01.52)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         231.976</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:29 (running for 00:04:06.53)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         231.976</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_91c47_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-06-32\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 357.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 677\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.046924700807862194\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.046935141263450976\n",
      "      mean_inference_ms: 0.497695941126977\n",
      "      mean_raw_obs_processing_ms: 0.059964106720538445\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 4bfe98a3ac81472c9aec783a7f1008ba\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3589620590209961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00465289456769824\n",
      "          model: {}\n",
      "          policy_loss: -0.004083945881575346\n",
      "          total_loss: 371.62677001953125\n",
      "          vf_explained_var: 0.33969205617904663\n",
      "          vf_loss: 371.6308898925781\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.52499999999999\n",
      "    ram_util_percent: 14.7\n",
      "  pid: 45490\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045728722417851156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04553969147381323\n",
      "    mean_inference_ms: 0.48864032211501657\n",
      "    mean_raw_obs_processing_ms: 0.06350045061074297\n",
      "  time_since_restore: 243.04421257972717\n",
      "  time_this_iter_s: 11.067892789840698\n",
      "  time_total_s: 243.04421257972717\n",
      "  timers:\n",
      "    learn_throughput: 1863.455\n",
      "    learn_time_ms: 2146.551\n",
      "    load_throughput: 24988406.315\n",
      "    load_time_ms: 0.16\n",
      "    sample_throughput: 534.407\n",
      "    sample_time_ms: 7484.927\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1671815192\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 91c47_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:06:34 (running for 00:04:11.60)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/16.42 GiB heap, 0.0/8.21 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_91c47_00000</td><td>RUNNING </td><td>192.168.0.98:45490</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         243.044</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "         local_dir=\"cartpole_v1\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b92a33-e459-4321-93e2-402794f364dc",
   "metadata": {},
   "source": [
    "### An experiment creates the following files and folders in the results directory\n",
    "\n",
    "```\n",
    "<results dir>\n",
    "└──PPO    # algorithm name\n",
    "    ├── basic-variant-state-2021-06-11_16-01-54.json\n",
    "    ├── experiment_state-2021-06-11_16-01-54.json\n",
    "    └── PPO_CartPole-v1_9424e_00000_0_2021-06-11_16-01-54    # training and evaluation data\n",
    "        ├── events.out.tfevents.1623420114.devbox-x299\n",
    "        ├── params.json\n",
    "        ├── params.pkl\n",
    "        ├── progress.csv\n",
    "        └── result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afbd9d-b321-4556-ac9d-3ae509a3e9ba",
   "metadata": {},
   "source": [
    "## `tensorboard` can visualize the training and evaluation data in real time\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
