{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595528b-6ea8-474d-aae6-02a79c6c99d3",
   "metadata": {},
   "source": [
    "# Visualizing training and evaluation\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ce636-9502-41f9-b6b3-a98460332a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af232e4e-d9dc-4fb3-9654-9e652c80dddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_17-15-36_141070_156401/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_17-15-36_141070_156401/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_17-15-36_141070_156401',\n",
       " 'metrics_export_port': 47844,\n",
       " 'gcs_address': '192.168.0.98:41352',\n",
       " 'address': '192.168.0.98:41352',\n",
       " 'node_id': '4345e9ee42214ecafcc0e53273f47b22676ec90f4057c40c6103cb4a'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0033ea09-1697-4f6e-b8de-578e6aa10c64",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:02,513\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:02,514\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:02,514\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:02,514\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:05,768\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:06,369\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:06 (running for 00:00:05.78)<br>Memory usage on this node: 3.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_780c0_00000</td><td>RUNNING </td><td>192.168.0.98:156509</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:07 (running for 00:00:06.78)<br>Memory usage on this node: 3.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_780c0_00000</td><td>RUNNING </td><td>192.168.0.98:156509</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=156509)\u001b[0m 2022-12-23 17:16:07,722\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2022-12-23 17:16:09,380\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:09 (running for 00:00:08.78)<br>Memory usage on this node: 3.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_780c0_00000</td><td>RUNNING </td><td>192.168.0.98:156509</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:16:09,589\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_780c0_00000]\n",
      "2022-12-23 17:16:09,589\tINFO tune.py:639 -- Total run time: 10.44 seconds (8.78 seconds for the tuning loop).\n",
      "2022-12-23 17:16:09,590\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f0d787c18b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aac37a-e712-47a9-88e2-d13b029f2edd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing a different results directory\n",
    "\n",
    "- Can be set via the `tune.run()` argument `local_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db158dd2-5f3e-4132-b641-2e11ff95383b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:44 (running for 00:00:00.11)<br>Memory usage on this node: 3.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:46,108\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:46,108\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:46,108\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:46,108\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:49,595\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:50 (running for 00:00:06.38)<br>Memory usage on this node: 4.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:50,591\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:16:52,186\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 22.78857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 72.0\n",
      "  episode_reward_mean: 22.78857142857143\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 175\n",
      "  episodes_total: 175\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6655349135398865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027983343228697777\n",
      "          model: {}\n",
      "          policy_loss: -0.04357268661260605\n",
      "          total_loss: 194.89149475097656\n",
      "          vf_explained_var: 0.021424010396003723\n",
      "          vf_loss: 194.9294891357422\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.31666666666666\n",
      "    ram_util_percent: 14.799999999999999\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05485139159001519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05401509880653632\n",
      "    mean_inference_ms: 0.5553058928746163\n",
      "    mean_raw_obs_processing_ms: 0.08928942699082412\n",
      "  time_since_restore: 3.7761261463165283\n",
      "  time_this_iter_s: 3.7761261463165283\n",
      "  time_total_s: 3.7761261463165283\n",
      "  timers:\n",
      "    learn_throughput: 1843.817\n",
      "    learn_time_ms: 2169.412\n",
      "    load_throughput: 21050459.222\n",
      "    load_time_ms: 0.19\n",
      "    sample_throughput: 1543.257\n",
      "    sample_time_ms: 2591.921\n",
      "    update_time_ms: 1.978\n",
      "  timestamp: 1671815814\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:16:54 (running for 00:00:10.21)<br>Memory usage on this node: 4.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.77613</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.7886</td><td style=\"text-align: right;\">                  72</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.7886</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:00 (running for 00:00:16.11)<br>Memory usage on this node: 4.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.67941</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   39.79</td><td style=\"text-align: right;\">                 162</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             39.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 62.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.0\n",
      "  episode_reward_mean: 62.45\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 304\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5763146877288818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008887048810720444\n",
      "          model: {}\n",
      "          policy_loss: -0.019862674176692963\n",
      "          total_loss: 923.13037109375\n",
      "          vf_explained_var: 0.07932927459478378\n",
      "          vf_loss: 923.1475830078125\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.52\n",
      "    ram_util_percent: 15.48\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05430902274715904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05371439389858282\n",
      "    mean_inference_ms: 0.5488035850904319\n",
      "    mean_raw_obs_processing_ms: 0.08410755147498392\n",
      "  time_since_restore: 12.38284969329834\n",
      "  time_this_iter_s: 3.7034413814544678\n",
      "  time_total_s: 12.38284969329834\n",
      "  timers:\n",
      "    learn_throughput: 1880.116\n",
      "    learn_time_ms: 2127.528\n",
      "    load_throughput: 19925434.679\n",
      "    load_time_ms: 0.201\n",
      "    sample_throughput: 1061.963\n",
      "    sample_time_ms: 3766.609\n",
      "    update_time_ms: 1.6\n",
      "  timestamp: 1671815823\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:06 (running for 00:00:21.86)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         12.3828</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   62.45</td><td style=\"text-align: right;\">                 236</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             62.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:11 (running for 00:00:26.87)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         12.3828</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   62.45</td><td style=\"text-align: right;\">                 236</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             62.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-17-11\n",
      "  done: false\n",
      "  episode_len_mean: 97.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 97.65\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 324\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 308.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 308.45\n",
      "    episode_reward_min: 90.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 260\n",
      "      - 304\n",
      "      - 234\n",
      "      - 461\n",
      "      - 237\n",
      "      - 210\n",
      "      - 323\n",
      "      - 324\n",
      "      - 285\n",
      "      - 269\n",
      "      - 241\n",
      "      - 428\n",
      "      - 217\n",
      "      - 488\n",
      "      - 344\n",
      "      - 500\n",
      "      - 379\n",
      "      - 266\n",
      "      - 90\n",
      "      - 309\n",
      "      episode_reward:\n",
      "      - 260.0\n",
      "      - 304.0\n",
      "      - 234.0\n",
      "      - 461.0\n",
      "      - 237.0\n",
      "      - 210.0\n",
      "      - 323.0\n",
      "      - 324.0\n",
      "      - 285.0\n",
      "      - 269.0\n",
      "      - 241.0\n",
      "      - 428.0\n",
      "      - 217.0\n",
      "      - 488.0\n",
      "      - 344.0\n",
      "      - 500.0\n",
      "      - 379.0\n",
      "      - 266.0\n",
      "      - 90.0\n",
      "      - 309.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05108200737086101\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05231886763891387\n",
      "      mean_inference_ms: 0.5535225248877969\n",
      "      mean_raw_obs_processing_ms: 0.0689860701109993\n",
      "    timesteps_this_iter: 6169\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5486755967140198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007675535045564175\n",
      "          model: {}\n",
      "          policy_loss: -0.016586076468229294\n",
      "          total_loss: 813.1764526367188\n",
      "          vf_explained_var: 0.10559539496898651\n",
      "          vf_loss: 813.1907958984375\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.1\n",
      "    ram_util_percent: 15.5\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05469728955237066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05405793919431434\n",
      "    mean_inference_ms: 0.5519249497910672\n",
      "    mean_raw_obs_processing_ms: 0.08374236047746128\n",
      "  time_since_restore: 20.645291328430176\n",
      "  time_this_iter_s: 8.262441635131836\n",
      "  time_total_s: 20.645291328430176\n",
      "  timers:\n",
      "    learn_throughput: 1877.076\n",
      "    learn_time_ms: 2130.974\n",
      "    load_throughput: 20250109.837\n",
      "    load_time_ms: 0.198\n",
      "    sample_throughput: 1062.513\n",
      "    sample_time_ms: 3764.662\n",
      "    update_time_ms: 1.579\n",
      "  timestamp: 1671815831\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:16 (running for 00:00:31.90)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         24.3788</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  132.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            132.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:21 (running for 00:00:36.91)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         24.3788</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  132.05</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            132.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 167.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 167.68\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 345\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 439.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 439.45\n",
      "    episode_reward_min: 125.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 330\n",
      "      - 500\n",
      "      - 437\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 393\n",
      "      - 500\n",
      "      - 442\n",
      "      - 406\n",
      "      - 125\n",
      "      - 500\n",
      "      - 500\n",
      "      - 373\n",
      "      - 500\n",
      "      - 283\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 330.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 393.0\n",
      "      - 500.0\n",
      "      - 442.0\n",
      "      - 406.0\n",
      "      - 125.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 373.0\n",
      "      - 500.0\n",
      "      - 283.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05149802805683076\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05285337329360028\n",
      "      mean_inference_ms: 0.5546937370151988\n",
      "      mean_raw_obs_processing_ms: 0.06891436336984388\n",
      "    timesteps_this_iter: 8789\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5240778923034668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005278068594634533\n",
      "          model: {}\n",
      "          policy_loss: -0.010133651085197926\n",
      "          total_loss: 554.5991821289062\n",
      "          vf_explained_var: 0.09255523979663849\n",
      "          vf_loss: 554.6085205078125\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.16\n",
      "    ram_util_percent: 15.59333333333333\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05506244632012383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054552781200833046\n",
      "    mean_inference_ms: 0.5564013376818037\n",
      "    mean_raw_obs_processing_ms: 0.0835136628303102\n",
      "  time_since_restore: 34.566179037094116\n",
      "  time_this_iter_s: 10.187364339828491\n",
      "  time_total_s: 34.566179037094116\n",
      "  timers:\n",
      "    learn_throughput: 1882.486\n",
      "    learn_time_ms: 2124.85\n",
      "    load_throughput: 20393698.541\n",
      "    load_time_ms: 0.196\n",
      "    sample_throughput: 884.618\n",
      "    sample_time_ms: 4521.727\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1671815845\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:26 (running for 00:00:42.09)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         34.5662</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  167.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            167.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:32 (running for 00:00:47.89)<br>Memory usage on this node: 4.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          38.344</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  202.25</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            202.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:37 (running for 00:00:52.92)<br>Memory usage on this node: 4.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          38.344</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  202.25</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            202.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 239.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 239.74\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 364\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05236223985292297\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05369649693130611\n",
      "      mean_inference_ms: 0.5639417741045168\n",
      "      mean_raw_obs_processing_ms: 0.06970729632998499\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5024950504302979\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038885294925421476\n",
      "          model: {}\n",
      "          policy_loss: -0.008019641041755676\n",
      "          total_loss: 258.6911926269531\n",
      "          vf_explained_var: 0.48038625717163086\n",
      "          vf_loss: 258.6986083984375\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.482352941176465\n",
      "    ram_util_percent: 14.958823529411765\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.055445739370193134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055094822432754015\n",
      "    mean_inference_ms: 0.5612085024154914\n",
      "    mean_raw_obs_processing_ms: 0.08326690390789412\n",
      "  time_since_restore: 49.80820345878601\n",
      "  time_this_iter_s: 11.46421194076538\n",
      "  time_total_s: 49.80820345878601\n",
      "  timers:\n",
      "    learn_throughput: 1864.859\n",
      "    learn_time_ms: 2144.935\n",
      "    load_throughput: 22748767.458\n",
      "    load_time_ms: 0.176\n",
      "    sample_throughput: 778.447\n",
      "    sample_time_ms: 5138.437\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1671815860\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:42 (running for 00:00:58.38)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         49.8082</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  239.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            239.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:47 (running for 00:01:03.42)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         53.7933</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  277.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            277.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:52 (running for 00:01:08.42)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         53.7933</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  277.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            277.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-17-55\n",
      "  done: false\n",
      "  episode_len_mean: 306.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 306.47\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 380\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 493.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 493.85\n",
      "    episode_reward_min: 377.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 377\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 377.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.052166820315145156\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05343672694736705\n",
      "      mean_inference_ms: 0.5607491686690999\n",
      "      mean_raw_obs_processing_ms: 0.06943454454734502\n",
      "    timesteps_this_iter: 9877\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5021220445632935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006358302664011717\n",
      "          model: {}\n",
      "          policy_loss: -0.0038205275777727365\n",
      "          total_loss: 231.4852294921875\n",
      "          vf_explained_var: 0.3254367709159851\n",
      "          vf_loss: 231.4888153076172\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.618750000000006\n",
      "    ram_util_percent: 15.8625\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05594201524161038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055687394205492485\n",
      "    mean_inference_ms: 0.566715354858345\n",
      "    mean_raw_obs_processing_ms: 0.08321438560789296\n",
      "  time_since_restore: 64.70686411857605\n",
      "  time_this_iter_s: 10.913602113723755\n",
      "  time_total_s: 64.70686411857605\n",
      "  timers:\n",
      "    learn_throughput: 1867.32\n",
      "    learn_time_ms: 2142.107\n",
      "    load_throughput: 24181631.594\n",
      "    load_time_ms: 0.165\n",
      "    sample_throughput: 706.736\n",
      "    sample_time_ms: 5659.819\n",
      "    update_time_ms: 1.925\n",
      "  timestamp: 1671815875\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:17:58 (running for 00:01:14.36)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         64.7069</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  306.47</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            306.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:04 (running for 00:01:20.13)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         68.4894</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  337.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            337.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:09 (running for 00:01:25.16)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         68.4894</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  337.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            337.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 366.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 366.97\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 396\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.052070421193820625\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053300526185305466\n",
      "      mean_inference_ms: 0.5592296323875305\n",
      "      mean_raw_obs_processing_ms: 0.06926821090497828\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46982070803642273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006394173484295607\n",
      "          model: {}\n",
      "          policy_loss: -0.003964981064200401\n",
      "          total_loss: 211.1330108642578\n",
      "          vf_explained_var: 0.34884414076805115\n",
      "          vf_loss: 211.13687133789062\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56666666666668\n",
      "    ram_util_percent: 15.800000000000006\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05632484512085508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.056158468199625826\n",
      "    mean_inference_ms: 0.5712878585694336\n",
      "    mean_raw_obs_processing_ms: 0.08312213364834338\n",
      "  time_since_restore: 79.51488184928894\n",
      "  time_this_iter_s: 11.025516986846924\n",
      "  time_total_s: 79.51488184928894\n",
      "  timers:\n",
      "    learn_throughput: 1864.763\n",
      "    learn_time_ms: 2145.045\n",
      "    load_throughput: 23633210.311\n",
      "    load_time_ms: 0.169\n",
      "    sample_throughput: 615.044\n",
      "    sample_time_ms: 6503.602\n",
      "    update_time_ms: 1.869\n",
      "  timestamp: 1671815890\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:15 (running for 00:01:30.92)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         83.2326</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  399.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            399.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:20 (running for 00:01:35.95)<br>Memory usage on this node: 4.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         83.2326</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  399.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            399.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:25 (running for 00:01:40.96)<br>Memory usage on this node: 4.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         83.2326</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  399.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            399.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-18-25\n",
      "  done: false\n",
      "  episode_len_mean: 418.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 418.31\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 412\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05200263661743095\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05327437368667002\n",
      "      mean_inference_ms: 0.5586530827104317\n",
      "      mean_raw_obs_processing_ms: 0.06923039259948664\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4421128034591675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00232648360542953\n",
      "          model: {}\n",
      "          policy_loss: 0.0008145323372446001\n",
      "          total_loss: 465.48199462890625\n",
      "          vf_explained_var: 0.16998711228370667\n",
      "          vf_loss: 465.4811706542969\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.06666666666667\n",
      "    ram_util_percent: 15.29333333333333\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.056688547202838946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05659860003290126\n",
      "    mean_inference_ms: 0.5754496930772003\n",
      "    mean_raw_obs_processing_ms: 0.08303754752123041\n",
      "  time_since_restore: 94.34847044944763\n",
      "  time_this_iter_s: 11.115848302841187\n",
      "  time_total_s: 94.34847044944763\n",
      "  timers:\n",
      "    learn_throughput: 1864.266\n",
      "    learn_time_ms: 2145.616\n",
      "    load_throughput: 25085550.239\n",
      "    load_time_ms: 0.159\n",
      "    sample_throughput: 562.645\n",
      "    sample_time_ms: 7109.284\n",
      "    update_time_ms: 1.873\n",
      "  timestamp: 1671815905\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:31 (running for 00:01:46.86)<br>Memory usage on this node: 4.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         98.0897</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  444.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            444.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:36 (running for 00:01:51.86)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         98.0897</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  444.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 104</td><td style=\"text-align: right;\">            444.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 460.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 460.32\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 429\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051984651517321\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05325308590151826\n",
      "      mean_inference_ms: 0.5585910737088468\n",
      "      mean_raw_obs_processing_ms: 0.06922645764749281\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4217674136161804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033710869029164314\n",
      "          model: {}\n",
      "          policy_loss: -0.0003774218785110861\n",
      "          total_loss: 401.9677429199219\n",
      "          vf_explained_var: 0.1018284410238266\n",
      "          vf_loss: 401.9681091308594\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.2625\n",
      "    ram_util_percent: 15.875000000000002\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.056982584524622445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05699262563619519\n",
      "    mean_inference_ms: 0.5791757615313089\n",
      "    mean_raw_obs_processing_ms: 0.08302182227195978\n",
      "  time_since_restore: 109.67909455299377\n",
      "  time_this_iter_s: 11.58943510055542\n",
      "  time_total_s: 109.67909455299377\n",
      "  timers:\n",
      "    learn_throughput: 1839.978\n",
      "    learn_time_ms: 2173.939\n",
      "    load_throughput: 24833060.983\n",
      "    load_time_ms: 0.161\n",
      "    sample_throughput: 539.441\n",
      "    sample_time_ms: 7415.078\n",
      "    update_time_ms: 1.915\n",
      "  timestamp: 1671815920\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:41 (running for 00:01:57.44)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         109.679</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  460.32</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            460.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:47 (running for 00:02:03.18)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         113.397</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  471.23</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            471.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:52 (running for 00:02:08.21)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         113.397</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  471.23</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            471.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 479.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 479.12\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 445\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051964032157568465\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053249243887119845\n",
      "      mean_inference_ms: 0.559380302989979\n",
      "      mean_raw_obs_processing_ms: 0.0691758110302971\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4284924566745758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004229819867759943\n",
      "          model: {}\n",
      "          policy_loss: -0.003964541479945183\n",
      "          total_loss: 220.12301635742188\n",
      "          vf_explained_var: 0.42318055033683777\n",
      "          vf_loss: 220.12698364257812\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.775000000000006\n",
      "    ram_util_percent: 16.06875\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057201458880314145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0573060193400503\n",
      "    mean_inference_ms: 0.5820047810018039\n",
      "    mean_raw_obs_processing_ms: 0.0830981813249225\n",
      "  time_since_restore: 124.52320146560669\n",
      "  time_this_iter_s: 11.12617540359497\n",
      "  time_total_s: 124.52320146560669\n",
      "  timers:\n",
      "    learn_throughput: 1857.523\n",
      "    learn_time_ms: 2153.405\n",
      "    load_throughput: 23629881.69\n",
      "    load_time_ms: 0.169\n",
      "    sample_throughput: 531.237\n",
      "    sample_time_ms: 7529.596\n",
      "    update_time_ms: 1.871\n",
      "  timestamp: 1671815935\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:18:57 (running for 00:02:13.34)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         124.523</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  479.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            479.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:03 (running for 00:02:19.11)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         128.242</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  481.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            481.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:08 (running for 00:02:24.11)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         128.242</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  481.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            481.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-19-10\n",
      "  done: false\n",
      "  episode_len_mean: 490.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 490.42\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 461\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 489.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 489.0\n",
      "    episode_reward_min: 420.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 459\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 489\n",
      "      - 500\n",
      "      - 500\n",
      "      - 465\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 447\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 420\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 459.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 489.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 465.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 447.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 420.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05194461156872121\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053199262566905194\n",
      "      mean_inference_ms: 0.5587734529748386\n",
      "      mean_raw_obs_processing_ms: 0.06913500902243562\n",
      "    timesteps_this_iter: 9780\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4139842689037323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004126996733248234\n",
      "          model: {}\n",
      "          policy_loss: -0.006916686426848173\n",
      "          total_loss: 224.4652099609375\n",
      "          vf_explained_var: 0.45742765069007874\n",
      "          vf_loss: 224.4721221923828\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.493333333333325\n",
      "    ram_util_percent: 16.0\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05744401495462752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057581403032997545\n",
      "    mean_inference_ms: 0.5845558803683005\n",
      "    mean_raw_obs_processing_ms: 0.08318368972865237\n",
      "  time_since_restore: 139.11639428138733\n",
      "  time_this_iter_s: 10.873984575271606\n",
      "  time_total_s: 139.11639428138733\n",
      "  timers:\n",
      "    learn_throughput: 1861.281\n",
      "    learn_time_ms: 2149.058\n",
      "    load_throughput: 22835464.816\n",
      "    load_time_ms: 0.175\n",
      "    sample_throughput: 535.373\n",
      "    sample_time_ms: 7471.425\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1671815950\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:13 (running for 00:02:29.76)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         142.875</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  492.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            492.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:18 (running for 00:02:34.76)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         142.875</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  492.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            492.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:24 (running for 00:02:39.79)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         142.875</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  492.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            492.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-19-24\n",
      "  done: false\n",
      "  episode_len_mean: 489.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.35\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 479\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 487.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 487.85\n",
      "    episode_reward_min: 407.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 463\n",
      "      - 484\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 449\n",
      "      - 469\n",
      "      - 500\n",
      "      - 407\n",
      "      - 485\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 463.0\n",
      "      - 484.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 449.0\n",
      "      - 469.0\n",
      "      - 500.0\n",
      "      - 407.0\n",
      "      - 485.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05193640210018333\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053170110921099946\n",
      "      mean_inference_ms: 0.558074978730645\n",
      "      mean_raw_obs_processing_ms: 0.0691526301915317\n",
      "    timesteps_this_iter: 9757\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4020386338233948\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004115118645131588\n",
      "          model: {}\n",
      "          policy_loss: -0.00486322958022356\n",
      "          total_loss: 79.47967529296875\n",
      "          vf_explained_var: 0.640857458114624\n",
      "          vf_loss: 79.48454284667969\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24\n",
      "    ram_util_percent: 16.0\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057512988238259545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05768608771257255\n",
      "    mean_inference_ms: 0.5855171396149589\n",
      "    mean_raw_obs_processing_ms: 0.08311856600554374\n",
      "  time_since_restore: 153.7416524887085\n",
      "  time_this_iter_s: 10.866376161575317\n",
      "  time_total_s: 153.7416524887085\n",
      "  timers:\n",
      "    learn_throughput: 1863.6\n",
      "    learn_time_ms: 2146.383\n",
      "    load_throughput: 23807600.397\n",
      "    load_time_ms: 0.168\n",
      "    sample_throughput: 535.891\n",
      "    sample_time_ms: 7464.206\n",
      "    update_time_ms: 1.545\n",
      "  timestamp: 1671815964\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:29 (running for 00:02:45.40)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         157.472</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  487.36</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            487.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:34 (running for 00:02:50.43)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         157.472</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  487.36</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            487.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:39 (running for 00:02:55.43)<br>Memory usage on this node: 5.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         157.472</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  487.36</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            487.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 485.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.35\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 495\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051925469726178355\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053154401552826205\n",
      "      mean_inference_ms: 0.5579364173638066\n",
      "      mean_raw_obs_processing_ms: 0.06918228389295056\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.39080435037612915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0039763906970620155\n",
      "          model: {}\n",
      "          policy_loss: -0.007974880747497082\n",
      "          total_loss: 50.8650016784668\n",
      "          vf_explained_var: 0.7048529982566833\n",
      "          vf_loss: 50.87297821044922\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05625\n",
      "    ram_util_percent: 16.043750000000003\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05757040866016525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05777056616580395\n",
      "    mean_inference_ms: 0.5862231683020692\n",
      "    mean_raw_obs_processing_ms: 0.0830862564006931\n",
      "  time_since_restore: 168.55522537231445\n",
      "  time_this_iter_s: 11.082834005355835\n",
      "  time_total_s: 168.55522537231445\n",
      "  timers:\n",
      "    learn_throughput: 1866.985\n",
      "    learn_time_ms: 2142.492\n",
      "    load_throughput: 22280499.336\n",
      "    load_time_ms: 0.18\n",
      "    sample_throughput: 537.054\n",
      "    sample_time_ms: 7448.045\n",
      "    update_time_ms: 1.542\n",
      "  timestamp: 1671815979\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:45 (running for 00:03:01.30)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         172.297</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  485.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            485.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:50 (running for 00:03:06.30)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         172.297</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  485.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            485.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 485.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.45\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 511\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.05196559303218455\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05321067330876935\n",
      "      mean_inference_ms: 0.5579875253024996\n",
      "      mean_raw_obs_processing_ms: 0.06927605102641024\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.41379833221435547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001836962066590786\n",
      "          model: {}\n",
      "          policy_loss: 0.0008776980685070157\n",
      "          total_loss: 299.8453063964844\n",
      "          vf_explained_var: 0.23064839839935303\n",
      "          vf_loss: 299.84442138671875\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.349999999999994\n",
      "    ram_util_percent: 16.16875\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057622633940111176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05784280353839157\n",
      "    mean_inference_ms: 0.5868633130656817\n",
      "    mean_raw_obs_processing_ms: 0.08307084730138552\n",
      "  time_since_restore: 183.4136028289795\n",
      "  time_this_iter_s: 11.116511821746826\n",
      "  time_total_s: 183.4136028289795\n",
      "  timers:\n",
      "    learn_throughput: 1888.589\n",
      "    learn_time_ms: 2117.983\n",
      "    load_throughput: 23444963.667\n",
      "    load_time_ms: 0.171\n",
      "    sample_throughput: 539.118\n",
      "    sample_time_ms: 7419.519\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1671815994\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:19:55 (running for 00:03:11.42)<br>Memory usage on this node: 4.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         183.414</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  485.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            485.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:01 (running for 00:03:17.33)<br>Memory usage on this node: 4.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         187.307</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:06 (running for 00:03:22.37)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         187.307</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 489.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.33\n",
      "  episode_reward_min: 370.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 527\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.052016765051945156\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05326522607036597\n",
      "      mean_inference_ms: 0.5583929499346052\n",
      "      mean_raw_obs_processing_ms: 0.06934052216529689\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3901015520095825\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0022216790821403265\n",
      "          model: {}\n",
      "          policy_loss: 0.0011523229768499732\n",
      "          total_loss: 230.05108642578125\n",
      "          vf_explained_var: 0.4150034189224243\n",
      "          vf_loss: 230.0499267578125\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.01176470588235\n",
      "    ram_util_percent: 16.03529411764706\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05767466741391006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057908536653753055\n",
      "    mean_inference_ms: 0.5874326644519785\n",
      "    mean_raw_obs_processing_ms: 0.08305651684793414\n",
      "  time_since_restore: 198.84200501441956\n",
      "  time_this_iter_s: 11.534659624099731\n",
      "  time_total_s: 198.84200501441956\n",
      "  timers:\n",
      "    learn_throughput: 1850.825\n",
      "    learn_time_ms: 2161.198\n",
      "    load_throughput: 23118666.115\n",
      "    load_time_ms: 0.173\n",
      "    sample_throughput: 538.833\n",
      "    sample_time_ms: 7423.457\n",
      "    update_time_ms: 1.543\n",
      "  timestamp: 1671816010\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:12 (running for 00:03:27.90)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         198.842</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:17 (running for 00:03:33.69)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         202.592</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:22 (running for 00:03:38.70)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         202.592</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-20-24\n",
      "  done: false\n",
      "  episode_len_mean: 489.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.13\n",
      "  episode_reward_min: 370.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 543\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051982514906390866\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053244187784188976\n",
      "      mean_inference_ms: 0.5578530341504793\n",
      "      mean_raw_obs_processing_ms: 0.06926838992781839\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3459629714488983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002214732114225626\n",
      "          model: {}\n",
      "          policy_loss: -0.005151618737727404\n",
      "          total_loss: 82.77787017822266\n",
      "          vf_explained_var: 0.6397683024406433\n",
      "          vf_loss: 82.78302764892578\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.7625\n",
      "    ram_util_percent: 16.293750000000003\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057672674942337215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05791772318155496\n",
      "    mean_inference_ms: 0.5874991387311098\n",
      "    mean_raw_obs_processing_ms: 0.08299304009002358\n",
      "  time_since_restore: 213.59384965896606\n",
      "  time_this_iter_s: 11.001627922058105\n",
      "  time_total_s: 213.59384965896606\n",
      "  timers:\n",
      "    learn_throughput: 1848.651\n",
      "    learn_time_ms: 2163.74\n",
      "    load_throughput: 22882182.215\n",
      "    load_time_ms: 0.175\n",
      "    sample_throughput: 536.362\n",
      "    sample_time_ms: 7457.656\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1671816024\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:27 (running for 00:03:43.72)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         213.594</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  489.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:33 (running for 00:03:49.45)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         217.336</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  489.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:38 (running for 00:03:54.48)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         217.336</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  489.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 489.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.13\n",
      "  episode_reward_min: 370.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 559\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051983840184940934\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05323347507894553\n",
      "      mean_inference_ms: 0.5576681797249241\n",
      "      mean_raw_obs_processing_ms: 0.06924136734538708\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36841148138046265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002557317726314068\n",
      "          model: {}\n",
      "          policy_loss: 0.0009582023485563695\n",
      "          total_loss: 376.3750915527344\n",
      "          vf_explained_var: 0.11117862164974213\n",
      "          vf_loss: 376.3741455078125\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.95\n",
      "    ram_util_percent: 16.2\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.057670142329075774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057927322463032456\n",
      "    mean_inference_ms: 0.5875779655654679\n",
      "    mean_raw_obs_processing_ms: 0.08294005872169867\n",
      "  time_since_restore: 228.35565900802612\n",
      "  time_this_iter_s: 11.019850969314575\n",
      "  time_total_s: 228.35565900802612\n",
      "  timers:\n",
      "    learn_throughput: 1850.6\n",
      "    learn_time_ms: 2161.461\n",
      "    load_throughput: 22622998.921\n",
      "    load_time_ms: 0.177\n",
      "    sample_throughput: 535.729\n",
      "    sample_time_ms: 7466.464\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1671816039\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:44 (running for 00:04:00.26)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         232.091</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  489.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:49 (running for 00:04:05.30)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         232.091</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  489.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 370</td><td style=\"text-align: right;\">            489.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-20-54\n",
      "  done: false\n",
      "  episode_len_mean: 493.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.3\n",
      "  episode_reward_min: 396.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 575\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051964887870537456\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05321844341525357\n",
      "      mean_inference_ms: 0.5573261789922433\n",
      "      mean_raw_obs_processing_ms: 0.06921510188889986\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3891461193561554\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002210254780948162\n",
      "          model: {}\n",
      "          policy_loss: 0.0008952384232543409\n",
      "          total_loss: 513.3617553710938\n",
      "          vf_explained_var: 0.024490253999829292\n",
      "          vf_loss: 513.36083984375\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.9125\n",
      "    ram_util_percent: 16.24375\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05768112545635179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05794211087946215\n",
      "    mean_inference_ms: 0.5877347493610494\n",
      "    mean_raw_obs_processing_ms: 0.08288965845974303\n",
      "  time_since_restore: 243.12334370613098\n",
      "  time_this_iter_s: 11.03215765953064\n",
      "  time_total_s: 243.12334370613098\n",
      "  timers:\n",
      "    learn_throughput: 1847.987\n",
      "    learn_time_ms: 2164.518\n",
      "    load_throughput: 23895764.136\n",
      "    load_time_ms: 0.167\n",
      "    sample_throughput: 534.34\n",
      "    sample_time_ms: 7485.87\n",
      "    update_time_ms: 1.45\n",
      "  timestamp: 1671816054\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:20:54 (running for 00:04:10.32)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         243.123</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   493.3</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 396</td><td style=\"text-align: right;\">             493.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:21:00 (running for 00:04:16.06)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         246.846</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  495.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 399</td><td style=\"text-align: right;\">            495.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:21:05 (running for 00:04:21.07)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         246.846</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  495.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 399</td><td style=\"text-align: right;\">            495.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_92e7c_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 498.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.67\n",
      "  episode_reward_min: 399.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 591\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.051950247815453335\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.053186831888447285\n",
      "      mean_inference_ms: 0.5569689275254983\n",
      "      mean_raw_obs_processing_ms: 0.06919783068542212\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: 73d5b2b1ea364d79ab4f2a1f3c8d923a\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36656802892684937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00411113491281867\n",
      "          model: {}\n",
      "          policy_loss: 0.0007093067979440093\n",
      "          total_loss: 463.76123046875\n",
      "          vf_explained_var: -0.19789065420627594\n",
      "          vf_loss: 463.760498046875\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24666666666666\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 156510\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05767583924948138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05794213694061225\n",
      "    mean_inference_ms: 0.5877505269363064\n",
      "    mean_raw_obs_processing_ms: 0.08284098073735816\n",
      "  time_since_restore: 257.8591582775116\n",
      "  time_this_iter_s: 11.013612031936646\n",
      "  time_total_s: 257.8591582775116\n",
      "  timers:\n",
      "    learn_throughput: 1851.261\n",
      "    learn_time_ms: 2160.69\n",
      "    load_throughput: 24139879.137\n",
      "    load_time_ms: 0.166\n",
      "    sample_throughput: 534.594\n",
      "    sample_time_ms: 7482.318\n",
      "    update_time_ms: 1.46\n",
      "  timestamp: 1671816069\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 92e7c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:21:10 (running for 00:04:26.11)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         257.859</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  498.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 399</td><td style=\"text-align: right;\">            498.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:21:16 (running for 00:04:31.88)<br>Memory usage on this node: 4.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         261.611</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   499.7</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 480</td><td style=\"text-align: right;\">             499.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:21:19,117\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:21:19 (running for 00:04:34.90)<br>Memory usage on this node: 4.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/17.53 GiB heap, 0.0/8.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_92e7c_00000</td><td>RUNNING </td><td>192.168.0.98:156510</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         261.611</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   499.7</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 480</td><td style=\"text-align: right;\">             499.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m 2022-12-23 17:21:19,167\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 648, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 965, in step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     step_attempt_results = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1073, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     step_results.update(self.evaluate())\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1207, in evaluate\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     self.evaluation_workers.local_worker().sample())\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 759, in sample\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 104, in next\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 266, in get_data\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 683, in _env_runner\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     base_env.send_actions(actions_to_send)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/env/vector_env.py\", line 302, in send_actions\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     self.vector_env.vector_step(action_vector)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/env/vector_env.py\", line 233, in vector_step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     obs, r, done, info = self.envs[i].step(actions[i])\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/gym/wrappers/time_limit.py\", line 18, in step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     observation, reward, done, info = self.env.step(action)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 108, in step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     force = self.force_mag if action == 1 else -self.force_mag\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/worker.py\", line 427, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=156510)\u001b[0m SystemExit: 1\n",
      "2022-12-23 17:21:19,324\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_92e7c_00000]\n",
      "2022-12-23 17:21:19,325\tINFO tune.py:639 -- Total run time: 275.11 seconds (274.90 seconds for the tuning loop).\n",
      "2022-12-23 17:21:19,326\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f0a4fa34880>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "         local_dir=\"cartpole_v1\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b92a33-e459-4321-93e2-402794f364dc",
   "metadata": {},
   "source": [
    "### An experiment creates the following files and folders in the results directory\n",
    "\n",
    "```\n",
    "<results dir>\n",
    "└──PPO    # algorithm name\n",
    "    ├── basic-variant-state-2021-06-11_16-01-54.json\n",
    "    ├── experiment_state-2021-06-11_16-01-54.json\n",
    "    └── PPO_CartPole-v1_9424e_00000_0_2021-06-11_16-01-54    # training and evaluation data\n",
    "        ├── events.out.tfevents.1623420114.devbox-x299\n",
    "        ├── params.json\n",
    "        ├── params.pkl\n",
    "        ├── progress.csv\n",
    "        └── result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afbd9d-b321-4556-ac9d-3ae509a3e9ba",
   "metadata": {},
   "source": [
    "## `tensorboard` can visualize the training and evaluation data in real time\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdeeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
