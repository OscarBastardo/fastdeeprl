{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7804cb7-8f9a-455e-a513-74f0043672a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Solving RL problems with `ray[rllib]`\n",
    "\n",
    "<img src=\"images/cartpole.jpg\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210f6c2-4645-4ab7-a45f-f8f6a667e667",
   "metadata": {},
   "source": [
    "## Step 1: Initialize `ray`\n",
    "\n",
    "- `ray` is a package providing distributed computing primitives. `rllib` is built on `ray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a8302b-1ae1-42c0-9852-ece04bc5a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487',\n",
       " 'metrics_export_port': 65412,\n",
       " 'gcs_address': '192.168.0.98:63541',\n",
       " 'address': '192.168.0.98:63541',\n",
       " 'node_id': 'c654f33cd9d7ab46008a4f0889879874b4cab53d76b0e12fecbd93fe'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb463a9-bef9-4af1-92db-976332c42797",
   "metadata": {},
   "source": [
    "## Step 2: Run an **experiment** to solve RL problems\n",
    "\n",
    "An experiment involves four things\n",
    "- A **RL environment** (e.g. `CartPole-v1`)\n",
    "- A **RL algorithm** to learn in that environment (e.g. Proximal Policy Optimization (PPO))\n",
    "- **Configuration** (algorithm config, experiment config, environment config etc.)\n",
    "- An **experiment runner** (called `tune`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a94021-5497-47fa-bbdf-116ef940b1b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,890\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,890\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,891\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:00 (running for 00:00:05.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:36:00,119\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:01 (running for 00:00:06.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:36:01,474\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 22.666666666666668\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 22.666666666666668\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6677238941192627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0263458751142025\n",
      "          model: {}\n",
      "          policy_loss: -0.031717997044324875\n",
      "          total_loss: 222.7870330810547\n",
      "          vf_explained_var: 0.019372833892703056\n",
      "          vf_loss: 222.8134765625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.24\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044701106750165386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04411039122499129\n",
      "    mean_inference_ms: 0.4843887808395671\n",
      "    mean_raw_obs_processing_ms: 0.07169593727942612\n",
      "  time_since_restore: 3.324431896209717\n",
      "  time_this_iter_s: 3.324431896209717\n",
      "  time_total_s: 3.324431896209717\n",
      "  timers:\n",
      "    learn_throughput: 2043.538\n",
      "    learn_time_ms: 1957.39\n",
      "    load_throughput: 34592197.938\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 2951.819\n",
      "    sample_time_ms: 1355.096\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1671813363\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:06 (running for 00:00:11.55)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32443</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.6667</td><td style=\"text-align: right;\">                  90</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-09\n",
      "  done: false\n",
      "  episode_len_mean: 67.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 281.0\n",
      "  episode_reward_mean: 67.2\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 299\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5797845721244812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010812313295900822\n",
      "          model: {}\n",
      "          policy_loss: -0.02183121256530285\n",
      "          total_loss: 757.8624267578125\n",
      "          vf_explained_var: 0.13413472473621368\n",
      "          vf_loss: 757.8809814453125\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.625\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043607924768946635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043218460326377856\n",
      "    mean_inference_ms: 0.47017684802007476\n",
      "    mean_raw_obs_processing_ms: 0.06619004810555301\n",
      "  time_since_restore: 9.39606261253357\n",
      "  time_this_iter_s: 3.0226731300354004\n",
      "  time_total_s: 9.39606261253357\n",
      "  timers:\n",
      "    learn_throughput: 2173.621\n",
      "    learn_time_ms: 1840.247\n",
      "    load_throughput: 28630061.433\n",
      "    load_time_ms: 0.14\n",
      "    sample_throughput: 1568.095\n",
      "    sample_time_ms: 2550.866\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1671813369\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:11 (running for 00:00:16.63)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.39606</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">    67.2</td><td style=\"text-align: right;\">                 281</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">              67.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-15\n",
      "  done: false\n",
      "  episode_len_mean: 128.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 128.16\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 334\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5438970327377319\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005624917335808277\n",
      "          model: {}\n",
      "          policy_loss: -0.010234670713543892\n",
      "          total_loss: 693.485107421875\n",
      "          vf_explained_var: 0.2024756669998169\n",
      "          vf_loss: 693.4935302734375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.44\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043546603817910236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043183061039808075\n",
      "    mean_inference_ms: 0.46978869255169853\n",
      "    mean_raw_obs_processing_ms: 0.06486679277025255\n",
      "  time_since_restore: 15.547481060028076\n",
      "  time_this_iter_s: 3.056800127029419\n",
      "  time_total_s: 15.547481060028076\n",
      "  timers:\n",
      "    learn_throughput: 2201.832\n",
      "    learn_time_ms: 1816.669\n",
      "    load_throughput: 27749282.17\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 1446.25\n",
      "    sample_time_ms: 2765.774\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671813375\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:16 (running for 00:00:21.81)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.5475</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  128.16</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            128.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:21 (running for 00:00:26.83)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         18.5533</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  161.53</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            161.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 200.43\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 356\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5294512510299683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00397925078868866\n",
      "          model: {}\n",
      "          policy_loss: -0.008955384604632854\n",
      "          total_loss: 438.78582763671875\n",
      "          vf_explained_var: 0.16676998138427734\n",
      "          vf_loss: 438.7941589355469\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.400000000000006\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04360390001764159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04309342188133744\n",
      "    mean_inference_ms: 0.46929274853797476\n",
      "    mean_raw_obs_processing_ms: 0.06368122464050409\n",
      "  time_since_restore: 21.59273600578308\n",
      "  time_this_iter_s: 3.039470672607422\n",
      "  time_total_s: 21.59273600578308\n",
      "  timers:\n",
      "    learn_throughput: 2215.406\n",
      "    learn_time_ms: 1805.538\n",
      "    load_throughput: 29701697.521\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 1406.361\n",
      "    sample_time_ms: 2844.219\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671813381\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:26 (running for 00:00:31.95)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         24.6207</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  230.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            230.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 265.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 265.0\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 374\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.487382709980011\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002882240107282996\n",
      "          model: {}\n",
      "          policy_loss: -0.0043022544123232365\n",
      "          total_loss: 504.5002136230469\n",
      "          vf_explained_var: 0.026964129880070686\n",
      "          vf_loss: 504.5043029785156\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.599999999999994\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043561476193733045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04308956025824391\n",
      "    mean_inference_ms: 0.4692430788083837\n",
      "    mean_raw_obs_processing_ms: 0.06286407142836352\n",
      "  time_since_restore: 27.702523708343506\n",
      "  time_this_iter_s: 3.081804037094116\n",
      "  time_total_s: 27.702523708343506\n",
      "  timers:\n",
      "    learn_throughput: 2219.465\n",
      "    learn_time_ms: 1802.236\n",
      "    load_throughput: 30790159.869\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1382.548\n",
      "    sample_time_ms: 2893.208\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1671813387\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:32 (running for 00:00:37.09)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         30.7445</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  290.94</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            290.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 324.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.9\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 391\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5414067506790161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004195119719952345\n",
      "          model: {}\n",
      "          policy_loss: -0.0025419823359698057\n",
      "          total_loss: 468.4158935546875\n",
      "          vf_explained_var: 0.08807938545942307\n",
      "          vf_loss: 468.4183349609375\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.474999999999994\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043599303534539065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043090271662098056\n",
      "    mean_inference_ms: 0.46930503030592746\n",
      "    mean_raw_obs_processing_ms: 0.06215156518863388\n",
      "  time_since_restore: 33.77014899253845\n",
      "  time_this_iter_s: 3.025679111480713\n",
      "  time_total_s: 33.77014899253845\n",
      "  timers:\n",
      "    learn_throughput: 2243.256\n",
      "    learn_time_ms: 1783.123\n",
      "    load_throughput: 30454194.954\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1298.889\n",
      "    sample_time_ms: 3079.555\n",
      "    update_time_ms: 1.356\n",
      "  timestamp: 1671813394\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:37 (running for 00:00:42.16)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         33.7701</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   324.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             324.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 377.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 377.26\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 407\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5010088682174683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006605195347219706\n",
      "          model: {}\n",
      "          policy_loss: -0.00037172867450863123\n",
      "          total_loss: 484.509765625\n",
      "          vf_explained_var: 0.030472297221422195\n",
      "          vf_loss: 484.51007080078125\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.980000000000004\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043582897374795364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043067216215753296\n",
      "    mean_inference_ms: 0.4689052928266846\n",
      "    mean_raw_obs_processing_ms: 0.06146178704252219\n",
      "  time_since_restore: 39.861185789108276\n",
      "  time_this_iter_s: 3.041245698928833\n",
      "  time_total_s: 39.861185789108276\n",
      "  timers:\n",
      "    learn_throughput: 2241.188\n",
      "    learn_time_ms: 1784.767\n",
      "    load_throughput: 32388447.876\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1306.863\n",
      "    sample_time_ms: 3060.764\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671813400\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:42 (running for 00:00:47.26)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         39.8612</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  377.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            377.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-46\n",
      "  done: false\n",
      "  episode_len_mean: 426.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 426.26\n",
      "  episode_reward_min: 145.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 423\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5119425654411316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002727864310145378\n",
      "          model: {}\n",
      "          policy_loss: -0.0022586756385862827\n",
      "          total_loss: 531.2691650390625\n",
      "          vf_explained_var: -0.018852492794394493\n",
      "          vf_loss: 531.2714233398438\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.425\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04356888150762668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04300869335019948\n",
      "    mean_inference_ms: 0.4681950021281348\n",
      "    mean_raw_obs_processing_ms: 0.06084517978414137\n",
      "  time_since_restore: 45.945841789245605\n",
      "  time_this_iter_s: 3.0422747135162354\n",
      "  time_total_s: 45.945841789245605\n",
      "  timers:\n",
      "    learn_throughput: 2239.461\n",
      "    learn_time_ms: 1786.144\n",
      "    load_throughput: 32338504.241\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1309.229\n",
      "    sample_time_ms: 3055.234\n",
      "    update_time_ms: 1.369\n",
      "  timestamp: 1671813406\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:47 (running for 00:00:52.37)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         45.9458</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  426.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 145</td><td style=\"text-align: right;\">            426.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:52 (running for 00:00:57.44)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         49.0046</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  444.84</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 152</td><td style=\"text-align: right;\">            444.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 458.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 458.9\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 440\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.45621252059936523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0029501868411898613\n",
      "          model: {}\n",
      "          policy_loss: -0.0014081724220886827\n",
      "          total_loss: 563.0380249023438\n",
      "          vf_explained_var: -0.06301168352365494\n",
      "          vf_loss: 563.0394287109375\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.025\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04351516382494466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04296107168788171\n",
      "    mean_inference_ms: 0.4675370247388944\n",
      "    mean_raw_obs_processing_ms: 0.06028324745373462\n",
      "  time_since_restore: 52.02103805541992\n",
      "  time_this_iter_s: 3.016430377960205\n",
      "  time_total_s: 52.02103805541992\n",
      "  timers:\n",
      "    learn_throughput: 2238.559\n",
      "    learn_time_ms: 1786.864\n",
      "    load_throughput: 32232883.766\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1307.784\n",
      "    sample_time_ms: 3058.608\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671813412\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:57 (running for 00:01:02.54)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         55.0431</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  472.96</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            472.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-58\n",
      "  done: false\n",
      "  episode_len_mean: 478.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 478.36\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 456\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4239750802516937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005886388476938009\n",
      "          model: {}\n",
      "          policy_loss: -0.0022193826735019684\n",
      "          total_loss: 529.1740112304688\n",
      "          vf_explained_var: -0.021180717274546623\n",
      "          vf_loss: 529.1762084960938\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.839999999999996\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04353004167542327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04292108372584675\n",
      "    mean_inference_ms: 0.46713100252918416\n",
      "    mean_raw_obs_processing_ms: 0.059906187369080065\n",
      "  time_since_restore: 58.090163469314575\n",
      "  time_this_iter_s: 3.0470263957977295\n",
      "  time_total_s: 58.090163469314575\n",
      "  timers:\n",
      "    learn_throughput: 2239.068\n",
      "    learn_time_ms: 1786.458\n",
      "    load_throughput: 31324152.353\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1309.644\n",
      "    sample_time_ms: 3054.265\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1671813418\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:02 (running for 00:01:07.61)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         61.1105</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  481.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            481.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-04\n",
      "  done: false\n",
      "  episode_len_mean: 488.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.19\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 472\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3980620205402374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037860707379877567\n",
      "          model: {}\n",
      "          policy_loss: -0.0029902078676968813\n",
      "          total_loss: 501.73126220703125\n",
      "          vf_explained_var: 0.03084975853562355\n",
      "          vf_loss: 501.73419189453125\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.25\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0434964826763005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04289641520668185\n",
      "    mean_inference_ms: 0.46675199210354307\n",
      "    mean_raw_obs_processing_ms: 0.05960073849467742\n",
      "  time_since_restore: 64.13966298103333\n",
      "  time_this_iter_s: 3.0291850566864014\n",
      "  time_total_s: 64.13966298103333\n",
      "  timers:\n",
      "    learn_throughput: 2239.589\n",
      "    learn_time_ms: 1786.042\n",
      "    load_throughput: 30251020.555\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1310.68\n",
      "    sample_time_ms: 3051.851\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1671813424\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:07 (running for 00:01:12.67)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         64.1397</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  488.19</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            488.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-10\n",
      "  done: false\n",
      "  episode_len_mean: 491.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 491.45\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 488\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.39177200198173523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006307192612439394\n",
      "          model: {}\n",
      "          policy_loss: -0.004075607750564814\n",
      "          total_loss: 513.6663818359375\n",
      "          vf_explained_var: 0.03715536370873451\n",
      "          vf_loss: 513.67041015625\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.85\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04348124121861787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0428735465670551\n",
      "    mean_inference_ms: 0.46646163840782523\n",
      "    mean_raw_obs_processing_ms: 0.05936134210496759\n",
      "  time_since_restore: 70.33358669281006\n",
      "  time_this_iter_s: 3.105516195297241\n",
      "  time_total_s: 70.33358669281006\n",
      "  timers:\n",
      "    learn_throughput: 2230.41\n",
      "    learn_time_ms: 1793.392\n",
      "    load_throughput: 30289250.767\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1309.266\n",
      "    sample_time_ms: 3055.146\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671813430\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:12 (running for 00:01:17.87)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         70.3336</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  491.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            491.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 504\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38255375623703003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004798873793333769\n",
      "          model: {}\n",
      "          policy_loss: -0.002857438288629055\n",
      "          total_loss: 499.1186828613281\n",
      "          vf_explained_var: -0.08942034095525742\n",
      "          vf_loss: 499.1215515136719\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.275000000000006\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043456026695483006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04284503143551925\n",
      "    mean_inference_ms: 0.46611646032293147\n",
      "    mean_raw_obs_processing_ms: 0.05915616744860538\n",
      "  time_since_restore: 76.38708782196045\n",
      "  time_this_iter_s: 3.034912347793579\n",
      "  time_total_s: 76.38708782196045\n",
      "  timers:\n",
      "    learn_throughput: 2230.214\n",
      "    learn_time_ms: 1793.55\n",
      "    load_throughput: 30942855.035\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1307.879\n",
      "    sample_time_ms: 3058.386\n",
      "    update_time_ms: 1.345\n",
      "  timestamp: 1671813436\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:17 (running for 00:01:22.96)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         76.3871</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:22 (running for 00:01:28.02)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         79.4227</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 520\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3938816487789154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0015072141541168094\n",
      "          model: {}\n",
      "          policy_loss: 0.00038365216460078955\n",
      "          total_loss: 539.7266235351562\n",
      "          vf_explained_var: -0.0835423469543457\n",
      "          vf_loss: 539.7262573242188\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.0\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04342695082088102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04281428676654714\n",
      "    mean_inference_ms: 0.4657748444827307\n",
      "    mean_raw_obs_processing_ms: 0.058980607314950396\n",
      "  time_since_restore: 82.4312036037445\n",
      "  time_this_iter_s: 3.008547782897949\n",
      "  time_total_s: 82.4312036037445\n",
      "  timers:\n",
      "    learn_throughput: 2229.837\n",
      "    learn_time_ms: 1793.853\n",
      "    load_throughput: 31040177.613\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1308.779\n",
      "    sample_time_ms: 3056.283\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671813442\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:28 (running for 00:01:33.14)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         85.4967</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 536\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38424333930015564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006487591657787561\n",
      "          model: {}\n",
      "          policy_loss: -0.0018671861616894603\n",
      "          total_loss: 507.647216796875\n",
      "          vf_explained_var: -0.022645175457000732\n",
      "          vf_loss: 507.6490478515625\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.475\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04340134687576476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04278862797043205\n",
      "    mean_inference_ms: 0.465469203657461\n",
      "    mean_raw_obs_processing_ms: 0.05884142547052737\n",
      "  time_since_restore: 88.5537428855896\n",
      "  time_this_iter_s: 3.0570731163024902\n",
      "  time_total_s: 88.5537428855896\n",
      "  timers:\n",
      "    learn_throughput: 2230.31\n",
      "    learn_time_ms: 1793.473\n",
      "    load_throughput: 31974873.261\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1305.674\n",
      "    sample_time_ms: 3063.551\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671813449\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:33 (running for 00:01:38.24)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         91.5883</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 552\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4100276827812195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002599272644147277\n",
      "          model: {}\n",
      "          policy_loss: -0.0005167347262613475\n",
      "          total_loss: 534.5759887695312\n",
      "          vf_explained_var: 0.02402246743440628\n",
      "          vf_loss: 534.5764770507812\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.225\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04339062856707493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04277819869065562\n",
      "    mean_inference_ms: 0.4653591828523603\n",
      "    mean_raw_obs_processing_ms: 0.05873912744601162\n",
      "  time_since_restore: 94.697509765625\n",
      "  time_this_iter_s: 3.109220027923584\n",
      "  time_total_s: 94.697509765625\n",
      "  timers:\n",
      "    learn_throughput: 2229.484\n",
      "    learn_time_ms: 1794.137\n",
      "    load_throughput: 34204313.965\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1302.439\n",
      "    sample_time_ms: 3071.162\n",
      "    update_time_ms: 1.328\n",
      "  timestamp: 1671813455\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:38 (running for 00:01:43.40)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         94.6975</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 568\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4069482982158661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005019642412662506\n",
      "          model: {}\n",
      "          policy_loss: 8.927865565055981e-05\n",
      "          total_loss: 497.6102600097656\n",
      "          vf_explained_var: -0.0994281992316246\n",
      "          vf_loss: 497.6101379394531\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.05\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338835206418242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04277502355118303\n",
      "    mean_inference_ms: 0.4653988122359949\n",
      "    mean_raw_obs_processing_ms: 0.058662392389679886\n",
      "  time_since_restore: 100.76534819602966\n",
      "  time_this_iter_s: 3.012232542037964\n",
      "  time_total_s: 100.76534819602966\n",
      "  timers:\n",
      "    learn_throughput: 2238.176\n",
      "    learn_time_ms: 1787.169\n",
      "    load_throughput: 31980968.357\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1304.348\n",
      "    sample_time_ms: 3066.667\n",
      "    update_time_ms: 1.315\n",
      "  timestamp: 1671813461\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:43 (running for 00:01:48.47)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         100.765</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 584\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3817431926727295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031131699215620756\n",
      "          model: {}\n",
      "          policy_loss: -0.0003612424770835787\n",
      "          total_loss: 545.5170288085938\n",
      "          vf_explained_var: -0.13232421875\n",
      "          vf_loss: 545.5173950195312\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.279999999999994\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338027509435271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042765930230909646\n",
      "    mean_inference_ms: 0.46533296087520787\n",
      "    mean_raw_obs_processing_ms: 0.058584593140984464\n",
      "  time_since_restore: 106.84234118461609\n",
      "  time_this_iter_s: 3.0423429012298584\n",
      "  time_total_s: 106.84234118461609\n",
      "  timers:\n",
      "    learn_throughput: 2237.05\n",
      "    learn_time_ms: 1788.069\n",
      "    load_throughput: 33427407.85\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1306.208\n",
      "    sample_time_ms: 3062.3\n",
      "    update_time_ms: 1.316\n",
      "  timestamp: 1671813467\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:48 (running for 00:01:53.58)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         106.842</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:53 (running for 00:01:58.63)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         109.881</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 600\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3397999703884125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002356970449909568\n",
      "          model: {}\n",
      "          policy_loss: 9.650844731368124e-05\n",
      "          total_loss: 500.5468444824219\n",
      "          vf_explained_var: -0.057527437806129456\n",
      "          vf_loss: 500.54669189453125\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.525\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337508963092878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042759413030790026\n",
      "    mean_inference_ms: 0.46531775045657353\n",
      "    mean_raw_obs_processing_ms: 0.05852268570035012\n",
      "  time_since_restore: 112.93902492523193\n",
      "  time_this_iter_s: 3.0579326152801514\n",
      "  time_total_s: 112.93902492523193\n",
      "  timers:\n",
      "    learn_throughput: 2235.84\n",
      "    learn_time_ms: 1789.037\n",
      "    load_throughput: 31371009.723\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1304.808\n",
      "    sample_time_ms: 3065.585\n",
      "    update_time_ms: 1.294\n",
      "  timestamp: 1671813473\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:58 (running for 00:02:03.83)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         116.032</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 616\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3305110037326813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003707656404003501\n",
      "          model: {}\n",
      "          policy_loss: -0.0004925053799524903\n",
      "          total_loss: 542.630126953125\n",
      "          vf_explained_var: -0.0788637027144432\n",
      "          vf_loss: 542.630615234375\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.15\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433798125225252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276228873145193\n",
      "    mean_inference_ms: 0.46546237641781957\n",
      "    mean_raw_obs_processing_ms: 0.05848631114565947\n",
      "  time_since_restore: 119.0727334022522\n",
      "  time_this_iter_s: 3.040776014328003\n",
      "  time_total_s: 119.0727334022522\n",
      "  timers:\n",
      "    learn_throughput: 2235.346\n",
      "    learn_time_ms: 1789.432\n",
      "    load_throughput: 31441559.22\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1304.452\n",
      "    sample_time_ms: 3066.421\n",
      "    update_time_ms: 1.32\n",
      "  timestamp: 1671813479\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:03 (running for 00:02:08.90)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         122.091</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 632\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31372615694999695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007560974918305874\n",
      "          model: {}\n",
      "          policy_loss: -0.001454669632948935\n",
      "          total_loss: 440.6395263671875\n",
      "          vf_explained_var: -0.10135575383901596\n",
      "          vf_loss: 440.6409912109375\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.300000000000004\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338159494423964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276235310416606\n",
      "    mean_inference_ms: 0.4655717864048512\n",
      "    mean_raw_obs_processing_ms: 0.05845090618972977\n",
      "  time_since_restore: 125.10599422454834\n",
      "  time_this_iter_s: 3.01479172706604\n",
      "  time_total_s: 125.10599422454834\n",
      "  timers:\n",
      "    learn_throughput: 2235.87\n",
      "    learn_time_ms: 1789.013\n",
      "    load_throughput: 31453348.331\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1308.565\n",
      "    sample_time_ms: 3056.783\n",
      "    update_time_ms: 1.315\n",
      "  timestamp: 1671813485\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:08 (running for 00:02:13.96)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         125.106</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 648\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31648343801498413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001370706595480442\n",
      "          model: {}\n",
      "          policy_loss: 0.002188962185755372\n",
      "          total_loss: 590.8506469726562\n",
      "          vf_explained_var: -0.33437660336494446\n",
      "          vf_loss: 590.8485107421875\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.480000000000004\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337976460803132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04275853246292382\n",
      "    mean_inference_ms: 0.46563096150647837\n",
      "    mean_raw_obs_processing_ms: 0.05841406440055217\n",
      "  time_since_restore: 131.22538375854492\n",
      "  time_this_iter_s: 3.042673110961914\n",
      "  time_total_s: 131.22538375854492\n",
      "  timers:\n",
      "    learn_throughput: 2236.824\n",
      "    learn_time_ms: 1788.249\n",
      "    load_throughput: 33601474.064\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 1306.641\n",
      "    sample_time_ms: 3061.285\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671813492\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:14 (running for 00:02:19.08)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         131.225</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 664\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351742678949904e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30818021297454834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004482596181333065\n",
      "          model: {}\n",
      "          policy_loss: -0.0021901451982557774\n",
      "          total_loss: 534.6130981445312\n",
      "          vf_explained_var: -0.13855034112930298\n",
      "          vf_loss: 534.6152954101562\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.625\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04336968981322421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04274706912241399\n",
      "    mean_inference_ms: 0.4656025769762755\n",
      "    mean_raw_obs_processing_ms: 0.05836811215692574\n",
      "  time_since_restore: 137.3073651790619\n",
      "  time_this_iter_s: 3.0500004291534424\n",
      "  time_total_s: 137.3073651790619\n",
      "  timers:\n",
      "    learn_throughput: 2240.176\n",
      "    learn_time_ms: 1785.574\n",
      "    load_throughput: 33341049.285\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1305.959\n",
      "    sample_time_ms: 3062.884\n",
      "    update_time_ms: 1.347\n",
      "  timestamp: 1671813498\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:19 (running for 00:02:24.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         137.307</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:24 (running for 00:02:29.27)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         140.364</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 680\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3042510747909546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00245378608815372\n",
      "          model: {}\n",
      "          policy_loss: -0.001417657476849854\n",
      "          total_loss: 499.1313781738281\n",
      "          vf_explained_var: -0.06330925971269608\n",
      "          vf_loss: 499.1327819824219\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.9\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337061683270438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04274897796074966\n",
      "    mean_inference_ms: 0.4656969482090039\n",
      "    mean_raw_obs_processing_ms: 0.05834609071398031\n",
      "  time_since_restore: 143.43842768669128\n",
      "  time_this_iter_s: 3.074427843093872\n",
      "  time_total_s: 143.43842768669128\n",
      "  timers:\n",
      "    learn_throughput: 2240.68\n",
      "    learn_time_ms: 1785.172\n",
      "    load_throughput: 35703800.809\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1304.756\n",
      "    sample_time_ms: 3065.707\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671813504\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:29 (running for 00:02:34.46)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         146.509</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 696\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.396983917434369e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2733024060726166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002296850783750415\n",
      "          model: {}\n",
      "          policy_loss: -0.0009159276378341019\n",
      "          total_loss: 471.1797790527344\n",
      "          vf_explained_var: 0.13280321657657623\n",
      "          vf_loss: 471.18072509765625\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043378002237498985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04275719098005286\n",
      "    mean_inference_ms: 0.46583206841731534\n",
      "    mean_raw_obs_processing_ms: 0.058333711113715624\n",
      "  time_since_restore: 149.545015335083\n",
      "  time_this_iter_s: 3.0361175537109375\n",
      "  time_total_s: 149.545015335083\n",
      "  timers:\n",
      "    learn_throughput: 2240.355\n",
      "    learn_time_ms: 1785.432\n",
      "    load_throughput: 35734219.382\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1306.21\n",
      "    sample_time_ms: 3062.294\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671813510\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:34 (running for 00:02:39.53)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         152.577</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 712\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4924597935859225e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27746322751045227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006151467561721802\n",
      "          model: {}\n",
      "          policy_loss: -0.0030704454984515905\n",
      "          total_loss: 481.7795715332031\n",
      "          vf_explained_var: -0.29841360449790955\n",
      "          vf_loss: 481.78265380859375\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.8\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337844922411784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042757985244062026\n",
      "    mean_inference_ms: 0.4658473618278681\n",
      "    mean_raw_obs_processing_ms: 0.058309056758720966\n",
      "  time_since_restore: 155.60638856887817\n",
      "  time_this_iter_s: 3.029507637023926\n",
      "  time_total_s: 155.60638856887817\n",
      "  timers:\n",
      "    learn_throughput: 2238.026\n",
      "    learn_time_ms: 1787.289\n",
      "    load_throughput: 34613608.418\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1305.567\n",
      "    sample_time_ms: 3063.803\n",
      "    update_time_ms: 1.363\n",
      "  timestamp: 1671813516\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:39 (running for 00:02:44.61)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         155.606</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 728\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7462298967929613e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2986508011817932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013733146479353309\n",
      "          model: {}\n",
      "          policy_loss: 0.000683144957292825\n",
      "          total_loss: 486.56427001953125\n",
      "          vf_explained_var: -0.2059945911169052\n",
      "          vf_loss: 486.5635986328125\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043381283263195944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276231064570618\n",
      "    mean_inference_ms: 0.4658708441569494\n",
      "    mean_raw_obs_processing_ms: 0.058289807945067516\n",
      "  time_since_restore: 161.70151042938232\n",
      "  time_this_iter_s: 3.0203309059143066\n",
      "  time_total_s: 161.70151042938232\n",
      "  timers:\n",
      "    learn_throughput: 2237.062\n",
      "    learn_time_ms: 1788.059\n",
      "    load_throughput: 34670832.817\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1305.689\n",
      "    sample_time_ms: 3063.517\n",
      "    update_time_ms: 1.333\n",
      "  timestamp: 1671813522\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:44 (running for 00:02:49.71)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         161.702</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-48\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 744\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574741982403e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2781405746936798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003607149003073573\n",
      "          model: {}\n",
      "          policy_loss: -0.00031350748031400144\n",
      "          total_loss: 477.109619140625\n",
      "          vf_explained_var: -0.11387369781732559\n",
      "          vf_loss: 477.1099548339844\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.125\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338260187779113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042765959743893064\n",
      "    mean_inference_ms: 0.46586541982889423\n",
      "    mean_raw_obs_processing_ms: 0.05826954516918967\n",
      "  time_since_restore: 167.7967541217804\n",
      "  time_this_iter_s: 3.0590336322784424\n",
      "  time_total_s: 167.7967541217804\n",
      "  timers:\n",
      "    learn_throughput: 2234.602\n",
      "    learn_time_ms: 1790.028\n",
      "    load_throughput: 34937975.843\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1305.696\n",
      "    sample_time_ms: 3063.5\n",
      "    update_time_ms: 1.324\n",
      "  timestamp: 1671813528\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:49 (running for 00:02:54.84)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         167.797</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:54 (running for 00:02:59.88)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         170.827</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 760\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827873709912016e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2417421042919159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024954811669886112\n",
      "          model: {}\n",
      "          policy_loss: 0.00023529196914751083\n",
      "          total_loss: 499.0150451660156\n",
      "          vf_explained_var: -0.09930148720741272\n",
      "          vf_loss: 499.01483154296875\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.78\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433813347799769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042767207832479615\n",
      "    mean_inference_ms: 0.46582492527203967\n",
      "    mean_raw_obs_processing_ms: 0.05824659703123883\n",
      "  time_since_restore: 173.84184432029724\n",
      "  time_this_iter_s: 3.014643430709839\n",
      "  time_total_s: 173.84184432029724\n",
      "  timers:\n",
      "    learn_throughput: 2233.651\n",
      "    learn_time_ms: 1790.79\n",
      "    load_throughput: 34843646.937\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1309.024\n",
      "    sample_time_ms: 3055.711\n",
      "    update_time_ms: 1.319\n",
      "  timestamp: 1671813534\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\n",
    "             \"env\": \"CartPole-v1\",\n",
    "                 # other configurations go here, if none provided, then default configurations will be used\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c39ff-9dca-476c-8452-272f495c7695",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "These configurations are applied in sequence\n",
    "\n",
    "1. [Common config](https://docs.ray.io/en/master/rllib-training.html#common-parameters)\n",
    "2. [Algorithm specific config (overrides common config)](https://docs.ray.io/en/master/rllib-algorithms.html#ppo)\n",
    "3. User defined config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a495bd-1aa6-41b8-8d63-548c77748d42",
   "metadata": {},
   "source": [
    "### Anatomy of an experiment\n",
    "\n",
    "<img src=\"images/ex/2.png\" width=\"750\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185adb02-d8b2-43c7-bed2-7efb3f89fe67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11132)\u001b[0m 2022-01-03 13:43:20,793\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:21,485\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:21,980\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:22 (running for 00:00:05.69)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:22,625\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:23 (running for 00:00:06.70)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:25,775\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:28 (running for 00:00:11.71)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 22.429378531073446\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 22.429378531073446\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 177\n",
      "  episodes_total: 177\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6658107042312622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028826190158724785\n",
      "          model: {}\n",
      "          policy_loss: -0.04175363481044769\n",
      "          total_loss: 234.28335571289062\n",
      "          vf_explained_var: 0.026140272617340088\n",
      "          vf_loss: 234.3193359375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.040000000000001\n",
      "    ram_util_percent: 11.89\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07999153254498818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0827080066954169\n",
      "    mean_inference_ms: 1.1430074741710328\n",
      "    mean_raw_obs_processing_ms: 0.1307997557624138\n",
      "  time_since_restore: 6.783697605133057\n",
      "  time_this_iter_s: 6.783697605133057\n",
      "  time_total_s: 6.783697605133057\n",
      "  timers:\n",
      "    learn_throughput: 1106.175\n",
      "    learn_time_ms: 3616.064\n",
      "    load_throughput: 6579300.392\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 1054.176\n",
      "    sample_time_ms: 3794.432\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1641213809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:34 (running for 00:00:17.52)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          6.7837</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4294</td><td style=\"text-align: right;\">                 106</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           22.4294</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 43.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 179.0\n",
      "  episode_reward_mean: 43.34\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 83\n",
      "  episodes_total: 260\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 74.05\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 234.0\n",
      "    episode_reward_mean: 74.05\n",
      "    episode_reward_min: 11.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 32\n",
      "      - 118\n",
      "      - 157\n",
      "      - 50\n",
      "      - 38\n",
      "      - 46\n",
      "      - 70\n",
      "      - 18\n",
      "      - 62\n",
      "      - 11\n",
      "      - 81\n",
      "      - 18\n",
      "      - 234\n",
      "      - 24\n",
      "      - 97\n",
      "      - 171\n",
      "      - 21\n",
      "      - 34\n",
      "      - 106\n",
      "      - 93\n",
      "      episode_reward:\n",
      "      - 32.0\n",
      "      - 118.0\n",
      "      - 157.0\n",
      "      - 50.0\n",
      "      - 38.0\n",
      "      - 46.0\n",
      "      - 70.0\n",
      "      - 18.0\n",
      "      - 62.0\n",
      "      - 11.0\n",
      "      - 81.0\n",
      "      - 18.0\n",
      "      - 234.0\n",
      "      - 24.0\n",
      "      - 97.0\n",
      "      - 171.0\n",
      "      - 21.0\n",
      "      - 34.0\n",
      "      - 106.0\n",
      "      - 93.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1034942554880572\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10477450855991297\n",
      "      mean_inference_ms: 1.421327050398236\n",
      "      mean_raw_obs_processing_ms: 0.134013925004102\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6172718405723572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016548188403248787\n",
      "          model: {}\n",
      "          policy_loss: -0.02616402879357338\n",
      "          total_loss: 411.5581970214844\n",
      "          vf_explained_var: 0.06493451446294785\n",
      "          vf_loss: 411.5793762207031\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.823076923076922\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07969391794581968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08267410284811398\n",
      "    mean_inference_ms: 1.1404213830478316\n",
      "    mean_raw_obs_processing_ms: 0.1258646349669711\n",
      "  time_since_restore: 16.069972276687622\n",
      "  time_this_iter_s: 9.286274671554565\n",
      "  time_total_s: 16.069972276687622\n",
      "  timers:\n",
      "    learn_throughput: 1123.892\n",
      "    learn_time_ms: 3559.06\n",
      "    load_throughput: 6836681.337\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 760.484\n",
      "    sample_time_ms: 5259.809\n",
      "    update_time_ms: 3.044\n",
      "  timestamp: 1641213818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:39 (running for 00:00:22.85)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           16.07</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   43.34</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             43.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:44 (running for 00:00:27.86)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           16.07</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   43.34</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             43.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 66.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.0\n",
      "  episode_reward_mean: 66.58\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 306\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5684974789619446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010665263049304485\n",
      "          model: {}\n",
      "          policy_loss: -0.019676998257637024\n",
      "          total_loss: 515.532958984375\n",
      "          vf_explained_var: 0.09138352423906326\n",
      "          vf_loss: 515.5494384765625\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.4\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07940025778872509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08240435142986875\n",
      "    mean_inference_ms: 1.136490450824648\n",
      "    mean_raw_obs_processing_ms: 0.12282318029898445\n",
      "  time_since_restore: 22.610326051712036\n",
      "  time_this_iter_s: 6.540353775024414\n",
      "  time_total_s: 22.610326051712036\n",
      "  timers:\n",
      "    learn_throughput: 1131.933\n",
      "    learn_time_ms: 3533.778\n",
      "    load_throughput: 7349831.776\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 604.227\n",
      "    sample_time_ms: 6620.025\n",
      "    update_time_ms: 2.673\n",
      "  timestamp: 1641213825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:50 (running for 00:00:33.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:55 (running for 00:00:38.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:00 (running for 00:00:43.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 96.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 360.0\n",
      "  episode_reward_mean: 96.8\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 328\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 293.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 293.0\n",
      "    episode_reward_min: 91.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 91\n",
      "      - 121\n",
      "      - 221\n",
      "      - 203\n",
      "      - 271\n",
      "      - 285\n",
      "      - 325\n",
      "      - 343\n",
      "      - 275\n",
      "      - 403\n",
      "      - 385\n",
      "      - 500\n",
      "      - 268\n",
      "      - 471\n",
      "      - 299\n",
      "      - 301\n",
      "      - 307\n",
      "      - 285\n",
      "      - 238\n",
      "      - 268\n",
      "      episode_reward:\n",
      "      - 91.0\n",
      "      - 121.0\n",
      "      - 221.0\n",
      "      - 203.0\n",
      "      - 271.0\n",
      "      - 285.0\n",
      "      - 325.0\n",
      "      - 343.0\n",
      "      - 275.0\n",
      "      - 403.0\n",
      "      - 385.0\n",
      "      - 500.0\n",
      "      - 268.0\n",
      "      - 471.0\n",
      "      - 299.0\n",
      "      - 301.0\n",
      "      - 307.0\n",
      "      - 285.0\n",
      "      - 238.0\n",
      "      - 268.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10427221294580773\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1071216429458656\n",
      "      mean_inference_ms: 1.4545869775313314\n",
      "      mean_raw_obs_processing_ms: 0.12946557492074579\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5436378121376038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008456626906991005\n",
      "          model: {}\n",
      "          policy_loss: -0.0228178258985281\n",
      "          total_loss: 670.0186767578125\n",
      "          vf_explained_var: 0.18131940066814423\n",
      "          vf_loss: 670.0390014648438\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.25\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07878085177973043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08179089944402097\n",
      "    mean_inference_ms: 1.1281064536294392\n",
      "    mean_raw_obs_processing_ms: 0.12036713776419888\n",
      "  time_since_restore: 39.70125675201416\n",
      "  time_this_iter_s: 17.090930700302124\n",
      "  time_total_s: 39.70125675201416\n",
      "  timers:\n",
      "    learn_throughput: 1137.099\n",
      "    learn_time_ms: 3517.724\n",
      "    load_throughput: 8051453.389\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 606.929\n",
      "    sample_time_ms: 6590.555\n",
      "    update_time_ms: 2.693\n",
      "  timestamp: 1641213842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:05 (running for 00:00:48.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         39.7013</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">    96.8</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">              96.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 131.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 131.82\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 340\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5397278070449829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006242944393306971\n",
      "          model: {}\n",
      "          policy_loss: -0.01558975875377655\n",
      "          total_loss: 694.2406616210938\n",
      "          vf_explained_var: 0.2767491638660431\n",
      "          vf_loss: 694.2542724609375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.87\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07900307051123598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08201445653199607\n",
      "    mean_inference_ms: 1.1313530922681776\n",
      "    mean_raw_obs_processing_ms: 0.1199765393807445\n",
      "  time_since_restore: 46.35895276069641\n",
      "  time_this_iter_s: 6.657696008682251\n",
      "  time_total_s: 46.35895276069641\n",
      "  timers:\n",
      "    learn_throughput: 1140.089\n",
      "    learn_time_ms: 3508.498\n",
      "    load_throughput: 7838355.448\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 457.703\n",
      "    sample_time_ms: 8739.294\n",
      "    update_time_ms: 2.707\n",
      "  timestamp: 1641213849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:11 (running for 00:00:54.26)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:16 (running for 00:00:59.27)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:21 (running for 00:01:04.28)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:26 (running for 00:01:09.29)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-28\n",
      "  done: false\n",
      "  episode_len_mean: 163.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 163.4\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 365.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 365.55\n",
      "    episode_reward_min: 133.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 406\n",
      "      - 500\n",
      "      - 281\n",
      "      - 404\n",
      "      - 400\n",
      "      - 279\n",
      "      - 271\n",
      "      - 458\n",
      "      - 441\n",
      "      - 304\n",
      "      - 440\n",
      "      - 397\n",
      "      - 296\n",
      "      - 500\n",
      "      - 133\n",
      "      - 262\n",
      "      - 414\n",
      "      - 473\n",
      "      - 270\n",
      "      - 382\n",
      "      episode_reward:\n",
      "      - 406.0\n",
      "      - 500.0\n",
      "      - 281.0\n",
      "      - 404.0\n",
      "      - 400.0\n",
      "      - 279.0\n",
      "      - 271.0\n",
      "      - 458.0\n",
      "      - 441.0\n",
      "      - 304.0\n",
      "      - 440.0\n",
      "      - 397.0\n",
      "      - 296.0\n",
      "      - 500.0\n",
      "      - 133.0\n",
      "      - 262.0\n",
      "      - 414.0\n",
      "      - 473.0\n",
      "      - 270.0\n",
      "      - 382.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10014727425918575\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10307793697627146\n",
      "      mean_inference_ms: 1.4106295812189358\n",
      "      mean_raw_obs_processing_ms: 0.12409709008050959\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5589821338653564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0039726984687149525\n",
      "          model: {}\n",
      "          policy_loss: -0.010850244201719761\n",
      "          total_loss: 441.4124450683594\n",
      "          vf_explained_var: 0.3547799587249756\n",
      "          vf_loss: 441.4220886230469\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.774074074074074\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07927501205942958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08230437368200383\n",
      "    mean_inference_ms: 1.1353438701977427\n",
      "    mean_raw_obs_processing_ms: 0.11964107912874493\n",
      "  time_since_restore: 65.38984847068787\n",
      "  time_this_iter_s: 19.030895709991455\n",
      "  time_total_s: 65.38984847068787\n",
      "  timers:\n",
      "    learn_throughput: 1143.818\n",
      "    learn_time_ms: 3497.06\n",
      "    load_throughput: 8657718.758\n",
      "    load_time_ms: 0.462\n",
      "    sample_throughput: 475.646\n",
      "    sample_time_ms: 8409.612\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1641213868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:31 (running for 00:01:14.34)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         65.3898</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   163.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             163.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 193.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 193.4\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5351145267486572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00829379539936781\n",
      "          model: {}\n",
      "          policy_loss: -0.011822070926427841\n",
      "          total_loss: 362.6712341308594\n",
      "          vf_explained_var: 0.2681409418582916\n",
      "          vf_loss: 362.6817932128906\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.166666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949524436539876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08253404854247841\n",
      "    mean_inference_ms: 1.1386905747585974\n",
      "    mean_raw_obs_processing_ms: 0.1191516615134056\n",
      "  time_since_restore: 71.94238519668579\n",
      "  time_this_iter_s: 6.552536725997925\n",
      "  time_total_s: 71.94238519668579\n",
      "  timers:\n",
      "    learn_throughput: 1145.349\n",
      "    learn_time_ms: 3492.385\n",
      "    load_throughput: 8423505.379\n",
      "    load_time_ms: 0.475\n",
      "    sample_throughput: 403.575\n",
      "    sample_time_ms: 9911.416\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1641213874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:36 (running for 00:01:19.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:41 (running for 00:01:24.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:46 (running for 00:01:29.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:51 (running for 00:01:34.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:56 (running for 00:01:39.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 225.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 225.28\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 369\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 468.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 468.1\n",
      "    episode_reward_min: 316.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 431\n",
      "      - 443\n",
      "      - 316\n",
      "      - 343\n",
      "      - 474\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 499\n",
      "      - 500\n",
      "      - 500\n",
      "      - 356\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 431.0\n",
      "      - 443.0\n",
      "      - 316.0\n",
      "      - 343.0\n",
      "      - 474.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 356.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10106344657864592\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10413185348367779\n",
      "      mean_inference_ms: 1.4217966227836218\n",
      "      mean_raw_obs_processing_ms: 0.12453373288105556\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5113347172737122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004379384219646454\n",
      "          model: {}\n",
      "          policy_loss: -0.007106391713023186\n",
      "          total_loss: 328.7005310058594\n",
      "          vf_explained_var: 0.28214773535728455\n",
      "          vf_loss: 328.7070007324219\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.499999999999998\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07931333257735361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08237211894150098\n",
      "    mean_inference_ms: 1.1366604480079978\n",
      "    mean_raw_obs_processing_ms: 0.11816209307487437\n",
      "  time_since_restore: 95.2527289390564\n",
      "  time_this_iter_s: 23.310343742370605\n",
      "  time_total_s: 95.2527289390564\n",
      "  timers:\n",
      "    learn_throughput: 1146.529\n",
      "    learn_time_ms: 3488.791\n",
      "    load_throughput: 8094182.125\n",
      "    load_time_ms: 0.494\n",
      "    sample_throughput: 420.724\n",
      "    sample_time_ms: 9507.418\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1641213898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:02 (running for 00:01:45.30)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         95.2527</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  225.28</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            225.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 260.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 260.1\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 378\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.48875856399536133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005784942302852869\n",
      "          model: {}\n",
      "          policy_loss: -0.004727379884570837\n",
      "          total_loss: 388.62091064453125\n",
      "          vf_explained_var: 0.15254899859428406\n",
      "          vf_loss: 388.6252136230469\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.766666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07913206874047868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08220464437410616\n",
      "    mean_inference_ms: 1.134642407090073\n",
      "    mean_raw_obs_processing_ms: 0.1171311694544639\n",
      "  time_since_restore: 101.90212655067444\n",
      "  time_this_iter_s: 6.649397611618042\n",
      "  time_total_s: 101.90212655067444\n",
      "  timers:\n",
      "    learn_throughput: 1144.962\n",
      "    learn_time_ms: 3493.567\n",
      "    load_throughput: 7803356.279\n",
      "    load_time_ms: 0.513\n",
      "    sample_throughput: 362.298\n",
      "    sample_time_ms: 11040.629\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1641213904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:07 (running for 00:01:50.99)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:12 (running for 00:01:56.00)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:17 (running for 00:02:01.01)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:22 (running for 00:02:06.02)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 294.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 294.48\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 389\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 459.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 459.65\n",
      "    episode_reward_min: 381.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 421\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 471\n",
      "      - 500\n",
      "      - 500\n",
      "      - 456\n",
      "      - 469\n",
      "      - 458\n",
      "      - 406\n",
      "      - 396\n",
      "      - 394\n",
      "      - 449\n",
      "      - 500\n",
      "      - 381\n",
      "      - 392\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 421.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 471.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 469.0\n",
      "      - 458.0\n",
      "      - 406.0\n",
      "      - 396.0\n",
      "      - 394.0\n",
      "      - 449.0\n",
      "      - 500.0\n",
      "      - 381.0\n",
      "      - 392.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10099773003778753\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1041771087035361\n",
      "      mean_inference_ms: 1.422287756722969\n",
      "      mean_raw_obs_processing_ms: 0.12433040690577138\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5206699967384338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006092665251344442\n",
      "          model: {}\n",
      "          policy_loss: -0.006521477364003658\n",
      "          total_loss: 176.64041137695312\n",
      "          vf_explained_var: 0.539181649684906\n",
      "          vf_loss: 176.64646911621094\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.1375\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07918270639411773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08226233495294545\n",
      "    mean_inference_ms: 1.1356321120490847\n",
      "    mean_raw_obs_processing_ms: 0.11634345193573531\n",
      "  time_since_restore: 124.45768737792969\n",
      "  time_this_iter_s: 22.55556082725525\n",
      "  time_total_s: 124.45768737792969\n",
      "  timers:\n",
      "    learn_throughput: 1145.753\n",
      "    learn_time_ms: 3491.154\n",
      "    load_throughput: 7670285.74\n",
      "    load_time_ms: 0.521\n",
      "    sample_throughput: 377.97\n",
      "    sample_time_ms: 10582.837\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1641213927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:28 (running for 00:02:11.60)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         124.458</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  294.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            294.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:33 (running for 00:02:16.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         124.458</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  294.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            294.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 324.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.49\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 397\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5341718196868896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008021753281354904\n",
      "          model: {}\n",
      "          policy_loss: -0.004115492105484009\n",
      "          total_loss: 140.74203491210938\n",
      "          vf_explained_var: 0.48715221881866455\n",
      "          vf_loss: 140.74554443359375\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.470000000000002\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794586475353722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08255020108052168\n",
      "    mean_inference_ms: 1.139531941899478\n",
      "    mean_raw_obs_processing_ms: 0.11614808624770369\n",
      "  time_since_restore: 131.24266409873962\n",
      "  time_this_iter_s: 6.7849767208099365\n",
      "  time_total_s: 131.24266409873962\n",
      "  timers:\n",
      "    learn_throughput: 1151.312\n",
      "    learn_time_ms: 3474.298\n",
      "    load_throughput: 8219693.303\n",
      "    load_time_ms: 0.487\n",
      "    sample_throughput: 319.776\n",
      "    sample_time_ms: 12508.759\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1641213934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:39 (running for 00:02:22.42)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:44 (running for 00:02:27.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:49 (running for 00:02:32.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:54 (running for 00:02:37.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 357.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 357.11\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 406\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 492.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 492.4\n",
      "    episode_reward_min: 380.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 380\n",
      "      - 500\n",
      "      - 500\n",
      "      - 468\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 380.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 468.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10025475779150288\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10338942276328436\n",
      "      mean_inference_ms: 1.4138633834522745\n",
      "      mean_raw_obs_processing_ms: 0.12335116448609725\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5026039481163025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003474506549537182\n",
      "          model: {}\n",
      "          policy_loss: -0.0017497400986030698\n",
      "          total_loss: 309.4649963378906\n",
      "          vf_explained_var: 0.3694356679916382\n",
      "          vf_loss: 309.4664611816406\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.087878787878788\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07973361019686534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08283335170955519\n",
      "    mean_inference_ms: 1.1434501056490693\n",
      "    mean_raw_obs_processing_ms: 0.11585680569932451\n",
      "  time_since_restore: 154.50243520736694\n",
      "  time_this_iter_s: 23.25977110862732\n",
      "  time_total_s: 154.50243520736694\n",
      "  timers:\n",
      "    learn_throughput: 1152.721\n",
      "    learn_time_ms: 3470.051\n",
      "    load_throughput: 8441366.541\n",
      "    load_time_ms: 0.474\n",
      "    sample_throughput: 320.537\n",
      "    sample_time_ms: 12479.05\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1641213957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:59 (running for 00:02:42.72)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         154.502</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  357.11</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            357.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 381.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 381.1\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 414\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5165870785713196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005959213245660067\n",
      "          model: {}\n",
      "          policy_loss: -0.0020698525477200747\n",
      "          total_loss: 180.06182861328125\n",
      "          vf_explained_var: 0.5193233489990234\n",
      "          vf_loss: 180.0636749267578\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07967921933349016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08279823550903932\n",
      "    mean_inference_ms: 1.1432283706147028\n",
      "    mean_raw_obs_processing_ms: 0.11522529024906841\n",
      "  time_since_restore: 161.2825345993042\n",
      "  time_this_iter_s: 6.780099391937256\n",
      "  time_total_s: 161.2825345993042\n",
      "  timers:\n",
      "    learn_throughput: 1151.081\n",
      "    learn_time_ms: 3474.994\n",
      "    load_throughput: 8237855.249\n",
      "    load_time_ms: 0.486\n",
      "    sample_throughput: 287.649\n",
      "    sample_time_ms: 13905.856\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1641213964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:05 (running for 00:02:48.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:10 (running for 00:02:53.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:15 (running for 00:02:58.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:20 (running for 00:03:03.57)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:25 (running for 00:03:08.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 406.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 406.44\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 422\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 495.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 495.85\n",
      "    episode_reward_min: 417.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.100465290110699\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1036170394321652\n",
      "      mean_inference_ms: 1.4179030730874522\n",
      "      mean_raw_obs_processing_ms: 0.1238538678226698\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5151876211166382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0054946839809417725\n",
      "          model: {}\n",
      "          policy_loss: -0.0007648436003364623\n",
      "          total_loss: 276.9347839355469\n",
      "          vf_explained_var: 0.4184960424900055\n",
      "          vf_loss: 276.9353332519531\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.285714285714285\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07992168915611161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08305190723397197\n",
      "    mean_inference_ms: 1.146826994029727\n",
      "    mean_raw_obs_processing_ms: 0.11502705733966839\n",
      "  time_since_restore: 185.71392345428467\n",
      "  time_this_iter_s: 24.43138885498047\n",
      "  time_total_s: 185.71392345428467\n",
      "  timers:\n",
      "    learn_throughput: 1151.121\n",
      "    learn_time_ms: 3474.873\n",
      "    load_throughput: 7839087.936\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 286.755\n",
      "    sample_time_ms: 13949.167\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1641213988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:30 (running for 00:03:14.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         185.714</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  406.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            406.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 432.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 432.59\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 430\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5273802876472473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00733661325648427\n",
      "          model: {}\n",
      "          policy_loss: 0.0002818219072651118\n",
      "          total_loss: 304.7541809082031\n",
      "          vf_explained_var: 0.2888195216655731\n",
      "          vf_loss: 304.7536315917969\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.4\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08017815411389745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08331997253915105\n",
      "    mean_inference_ms: 1.150923691646074\n",
      "    mean_raw_obs_processing_ms: 0.11487508726545222\n",
      "  time_since_restore: 192.54755973815918\n",
      "  time_this_iter_s: 6.833636283874512\n",
      "  time_total_s: 192.54755973815918\n",
      "  timers:\n",
      "    learn_throughput: 1151.35\n",
      "    learn_time_ms: 3474.182\n",
      "    load_throughput: 7810258.368\n",
      "    load_time_ms: 0.512\n",
      "    sample_throughput: 272.753\n",
      "    sample_time_ms: 14665.272\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1641213995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:36 (running for 00:03:19.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:41 (running for 00:03:24.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:46 (running for 00:03:29.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:51 (running for 00:03:34.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 444.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 444.6\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 438\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 414.8\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 414.8\n",
      "    episode_reward_min: 144.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 216\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 278\n",
      "      - 500\n",
      "      - 500\n",
      "      - 386\n",
      "      - 388\n",
      "      - 379\n",
      "      - 144\n",
      "      - 367\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 221\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 216.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 278.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 386.0\n",
      "      - 388.0\n",
      "      - 379.0\n",
      "      - 144.0\n",
      "      - 367.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 221.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10018910292198142\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10337515402961692\n",
      "      mean_inference_ms: 1.414373075126012\n",
      "      mean_raw_obs_processing_ms: 0.12350604226305173\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5323923826217651\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004643614869564772\n",
      "          model: {}\n",
      "          policy_loss: -0.0038051274605095387\n",
      "          total_loss: 152.39059448242188\n",
      "          vf_explained_var: 0.44824594259262085\n",
      "          vf_loss: 152.39422607421875\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.958620689655175\n",
      "    ram_util_percent: 11.837931034482763\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.080265079143969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08341594832395156\n",
      "    mean_inference_ms: 1.1527488471778489\n",
      "    mean_raw_obs_processing_ms: 0.11456734938602098\n",
      "  time_since_restore: 213.35079216957092\n",
      "  time_this_iter_s: 20.803232431411743\n",
      "  time_total_s: 213.35079216957092\n",
      "  timers:\n",
      "    learn_throughput: 1151.035\n",
      "    learn_time_ms: 3475.134\n",
      "    load_throughput: 7780557.436\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 273.103\n",
      "    sample_time_ms: 14646.479\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1641214016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:57 (running for 00:03:40.76)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.351</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   444.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             444.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:02 (running for 00:03:45.76)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.351</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   444.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             444.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-03\n",
      "  done: false\n",
      "  episode_len_mean: 453.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 453.71\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 447\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4969511032104492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005400669761002064\n",
      "          model: {}\n",
      "          policy_loss: -0.011936291120946407\n",
      "          total_loss: 270.889404296875\n",
      "          vf_explained_var: 0.40124747157096863\n",
      "          vf_loss: 270.9012451171875\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.89\n",
      "    ram_util_percent: 11.76\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025624234848648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340322857139394\n",
      "    mean_inference_ms: 1.1532691326338969\n",
      "    mean_raw_obs_processing_ms: 0.11415906490683135\n",
      "  time_since_restore: 219.7460675239563\n",
      "  time_this_iter_s: 6.395275354385376\n",
      "  time_total_s: 219.7460675239563\n",
      "  timers:\n",
      "    learn_throughput: 1151.531\n",
      "    learn_time_ms: 3473.637\n",
      "    load_throughput: 7698089.382\n",
      "    load_time_ms: 0.52\n",
      "    sample_throughput: 269.779\n",
      "    sample_time_ms: 14826.967\n",
      "    update_time_ms: 2.294\n",
      "  timestamp: 1641214023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:08 (running for 00:03:51.19)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:13 (running for 00:03:56.20)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:18 (running for 00:04:01.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:23 (running for 00:04:06.22)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 462.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 462.61\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 455\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 493.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 493.9\n",
      "    episode_reward_min: 378.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 378\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 378.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09978511039342158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10303779970507634\n",
      "      mean_inference_ms: 1.4098502244946258\n",
      "      mean_raw_obs_processing_ms: 0.12326797487389016\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47515296936035156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007485141512006521\n",
      "          model: {}\n",
      "          policy_loss: -0.004375540651381016\n",
      "          total_loss: 225.72230529785156\n",
      "          vf_explained_var: 0.5140494108200073\n",
      "          vf_loss: 225.72653198242188\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.212121212121213\n",
      "    ram_util_percent: 11.884848484848485\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08029113218771436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0834396962903968\n",
      "    mean_inference_ms: 1.154212622440793\n",
      "    mean_raw_obs_processing_ms: 0.11389584928969652\n",
      "  time_since_restore: 243.26226925849915\n",
      "  time_this_iter_s: 23.516201734542847\n",
      "  time_total_s: 243.26226925849915\n",
      "  timers:\n",
      "    learn_throughput: 1152.033\n",
      "    learn_time_ms: 3472.122\n",
      "    load_throughput: 8258129.553\n",
      "    load_time_ms: 0.484\n",
      "    sample_throughput: 269.805\n",
      "    sample_time_ms: 14825.515\n",
      "    update_time_ms: 2.275\n",
      "  timestamp: 1641214046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:28 (running for 00:04:11.74)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         243.262</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  462.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            462.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 465.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 465.42\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 463\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46219807863235474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004400151781737804\n",
      "          model: {}\n",
      "          policy_loss: -0.002735947957262397\n",
      "          total_loss: 140.81980895996094\n",
      "          vf_explained_var: 0.56380695104599\n",
      "          vf_loss: 140.82244873046875\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.944444444444445\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08029197087482284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08342756636082208\n",
      "    mean_inference_ms: 1.154621186293605\n",
      "    mean_raw_obs_processing_ms: 0.11362897572342391\n",
      "  time_since_restore: 249.51125812530518\n",
      "  time_this_iter_s: 6.24898886680603\n",
      "  time_total_s: 249.51125812530518\n",
      "  timers:\n",
      "    learn_throughput: 1154.233\n",
      "    learn_time_ms: 3465.505\n",
      "    load_throughput: 8391964.786\n",
      "    load_time_ms: 0.477\n",
      "    sample_throughput: 270.068\n",
      "    sample_time_ms: 14811.097\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1641214052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:33 (running for 00:04:17.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:38 (running for 00:04:22.04)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:43 (running for 00:04:27.05)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:48 (running for 00:04:32.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:54 (running for 00:04:37.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-57\n",
      "  done: false\n",
      "  episode_len_mean: 468.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 468.27\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 471\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.100105893292095\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10337834525863725\n",
      "      mean_inference_ms: 1.4144293368483374\n",
      "      mean_raw_obs_processing_ms: 0.12340765953569317\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.45820188522338867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031977591570466757\n",
      "          model: {}\n",
      "          policy_loss: 0.0005085796001367271\n",
      "          total_loss: 181.96998596191406\n",
      "          vf_explained_var: 0.5409795641899109\n",
      "          vf_loss: 181.96945190429688\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.577142857142858\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08028119486507443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340499456734475\n",
      "    mean_inference_ms: 1.1548213229188307\n",
      "    mean_raw_obs_processing_ms: 0.11338280886639858\n",
      "  time_since_restore: 273.92436146736145\n",
      "  time_this_iter_s: 24.413103342056274\n",
      "  time_total_s: 273.92436146736145\n",
      "  timers:\n",
      "    learn_throughput: 1155.931\n",
      "    learn_time_ms: 3460.415\n",
      "    load_throughput: 8470774.513\n",
      "    load_time_ms: 0.472\n",
      "    sample_throughput: 269.785\n",
      "    sample_time_ms: 14826.643\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1641214077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:59 (running for 00:04:42.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         273.924</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  468.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            468.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 473.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 473.35\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 479\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4432362914085388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00505047058686614\n",
      "          model: {}\n",
      "          policy_loss: 0.0021547339856624603\n",
      "          total_loss: 284.6266784667969\n",
      "          vf_explained_var: 0.5598533153533936\n",
      "          vf_loss: 284.6245422363281\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.066666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024151803154615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08335548716827214\n",
      "    mean_inference_ms: 1.1546486884257041\n",
      "    mean_raw_obs_processing_ms: 0.1131199240899339\n",
      "  time_since_restore: 280.30135583877563\n",
      "  time_this_iter_s: 6.376994371414185\n",
      "  time_total_s: 280.30135583877563\n",
      "  timers:\n",
      "    learn_throughput: 1156.786\n",
      "    learn_time_ms: 3457.858\n",
      "    load_throughput: 7911542.016\n",
      "    load_time_ms: 0.506\n",
      "    sample_throughput: 267.526\n",
      "    sample_time_ms: 14951.83\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1641214083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:04 (running for 00:04:47.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:09 (running for 00:04:52.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:14 (running for 00:04:57.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:19 (running for 00:05:02.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:24 (running for 00:05:07.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 481.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 481.06\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 487\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09983109608077598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10304116590148046\n",
      "      mean_inference_ms: 1.4101553587376912\n",
      "      mean_raw_obs_processing_ms: 0.12321155504192804\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.41949203610420227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030744285322725773\n",
      "          model: {}\n",
      "          policy_loss: 0.0008549469639547169\n",
      "          total_loss: 126.43228912353516\n",
      "          vf_explained_var: 0.48506686091423035\n",
      "          vf_loss: 126.4314193725586\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.458823529411763\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024448382338494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08336119542740951\n",
      "    mean_inference_ms: 1.1550906718792857\n",
      "    mean_raw_obs_processing_ms: 0.11292802768969422\n",
      "  time_since_restore: 303.8563857078552\n",
      "  time_this_iter_s: 23.55502986907959\n",
      "  time_total_s: 303.8563857078552\n",
      "  timers:\n",
      "    learn_throughput: 1157.294\n",
      "    learn_time_ms: 3456.338\n",
      "    load_throughput: 7744283.604\n",
      "    load_time_ms: 0.517\n",
      "    sample_throughput: 267.318\n",
      "    sample_time_ms: 14963.456\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1641214107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:30 (running for 00:05:13.51)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         303.856</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  481.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            481.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 484.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.83\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 495\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4058391749858856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034382410813122988\n",
      "          model: {}\n",
      "          policy_loss: -0.0004056684556417167\n",
      "          total_loss: 167.3172149658203\n",
      "          vf_explained_var: 0.7687280774116516\n",
      "          vf_loss: 167.31761169433594\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.200000000000001\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08022232712990335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08333284623699147\n",
      "    mean_inference_ms: 1.1550813329526606\n",
      "    mean_raw_obs_processing_ms: 0.11272178383415525\n",
      "  time_since_restore: 310.25225472450256\n",
      "  time_this_iter_s: 6.395869016647339\n",
      "  time_total_s: 310.25225472450256\n",
      "  timers:\n",
      "    learn_throughput: 1160.233\n",
      "    learn_time_ms: 3447.583\n",
      "    load_throughput: 7775148.763\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 267.578\n",
      "    sample_time_ms: 14948.893\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1641214113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:35 (running for 00:05:18.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:40 (running for 00:05:23.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:45 (running for 00:05:28.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:50 (running for 00:05:33.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:55 (running for 00:05:38.98)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 503\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09998848846356197\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10319030098548938\n",
      "      mean_inference_ms: 1.411754838645783\n",
      "      mean_raw_obs_processing_ms: 0.12325853495637581\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3959515392780304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035098567605018616\n",
      "          model: {}\n",
      "          policy_loss: -0.004721165634691715\n",
      "          total_loss: 111.00982666015625\n",
      "          vf_explained_var: 0.6156907677650452\n",
      "          vf_loss: 111.0145492553711\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.414285714285715\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08022994097113151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08334166029313898\n",
      "    mean_inference_ms: 1.1555068953752567\n",
      "    mean_raw_obs_processing_ms: 0.11257086602261233\n",
      "  time_since_restore: 334.6890869140625\n",
      "  time_this_iter_s: 24.436832189559937\n",
      "  time_total_s: 334.6890869140625\n",
      "  timers:\n",
      "    learn_throughput: 1160.553\n",
      "    learn_time_ms: 3446.632\n",
      "    load_throughput: 7717210.672\n",
      "    load_time_ms: 0.518\n",
      "    sample_throughput: 267.784\n",
      "    sample_time_ms: 14937.428\n",
      "    update_time_ms: 2.25\n",
      "  timestamp: 1641214138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:01 (running for 00:05:44.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         334.689</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 511\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3711017966270447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00191035820171237\n",
      "          model: {}\n",
      "          policy_loss: 0.0005888226442039013\n",
      "          total_loss: 270.68115234375\n",
      "          vf_explained_var: 0.4633130133152008\n",
      "          vf_loss: 270.6805419921875\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.122222222222222\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0802107632924208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08331584076486034\n",
      "    mean_inference_ms: 1.1554800397796539\n",
      "    mean_raw_obs_processing_ms: 0.11240112800129712\n",
      "  time_since_restore: 341.08573842048645\n",
      "  time_this_iter_s: 6.39665150642395\n",
      "  time_total_s: 341.08573842048645\n",
      "  timers:\n",
      "    learn_throughput: 1161.701\n",
      "    learn_time_ms: 3443.227\n",
      "    load_throughput: 8316668.815\n",
      "    load_time_ms: 0.481\n",
      "    sample_throughput: 268.448\n",
      "    sample_time_ms: 14900.464\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1641214144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:06 (running for 00:05:49.87)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:11 (running for 00:05:54.88)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:16 (running for 00:05:59.89)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:21 (running for 00:06:04.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:26 (running for 00:06:09.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-27\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 519\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09954568683295094\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.102662717970965\n",
      "      mean_inference_ms: 1.4051876212950658\n",
      "      mean_raw_obs_processing_ms: 0.12253929868607154\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3576214015483856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004535712767392397\n",
      "          model: {}\n",
      "          policy_loss: -0.0013734217500314116\n",
      "          total_loss: 222.8177490234375\n",
      "          vf_explained_var: 0.4276777505874634\n",
      "          vf_loss: 222.8191375732422\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.427272727272728\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08013078155598903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08322334788237092\n",
      "    mean_inference_ms: 1.1545560525014955\n",
      "    mean_raw_obs_processing_ms: 0.11216874833685868\n",
      "  time_since_restore: 364.052127122879\n",
      "  time_this_iter_s: 22.966388702392578\n",
      "  time_total_s: 364.052127122879\n",
      "  timers:\n",
      "    learn_throughput: 1161.712\n",
      "    learn_time_ms: 3443.194\n",
      "    load_throughput: 8078008.57\n",
      "    load_time_ms: 0.495\n",
      "    sample_throughput: 268.677\n",
      "    sample_time_ms: 14887.748\n",
      "    update_time_ms: 2.151\n",
      "  timestamp: 1641214167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:32 (running for 00:06:15.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         364.052</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-34\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 527\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36017468571662903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034243902191519737\n",
      "          model: {}\n",
      "          policy_loss: 0.0010374116245657206\n",
      "          total_loss: 281.56884765625\n",
      "          vf_explained_var: 0.37897738814353943\n",
      "          vf_loss: 281.5677795410156\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.033333333333335\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08001243148732361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08309049227250064\n",
      "    mean_inference_ms: 1.1529038434635428\n",
      "    mean_raw_obs_processing_ms: 0.11190238371479752\n",
      "  time_since_restore: 370.43571066856384\n",
      "  time_this_iter_s: 6.3835835456848145\n",
      "  time_total_s: 370.43571066856384\n",
      "  timers:\n",
      "    learn_throughput: 1162.242\n",
      "    learn_time_ms: 3441.623\n",
      "    load_throughput: 8171252.679\n",
      "    load_time_ms: 0.49\n",
      "    sample_throughput: 264.653\n",
      "    sample_time_ms: 15114.132\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1641214174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:38 (running for 00:06:21.31)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:43 (running for 00:06:26.32)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:48 (running for 00:06:31.33)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:53 (running for 00:06:36.34)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-57\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 535\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09942641215691356\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10265161153002715\n",
      "      mean_inference_ms: 1.4040633232008823\n",
      "      mean_raw_obs_processing_ms: 0.12241957140361297\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36790016293525696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0020432034507393837\n",
      "          model: {}\n",
      "          policy_loss: 0.0010514894966036081\n",
      "          total_loss: 349.08477783203125\n",
      "          vf_explained_var: 0.06388817727565765\n",
      "          vf_loss: 349.083740234375\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.458823529411763\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07990143629753838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08296411776238405\n",
      "    mean_inference_ms: 1.151195582617503\n",
      "    mean_raw_obs_processing_ms: 0.11165261981322569\n",
      "  time_since_restore: 394.09208583831787\n",
      "  time_this_iter_s: 23.65637516975403\n",
      "  time_total_s: 394.09208583831787\n",
      "  timers:\n",
      "    learn_throughput: 1162.646\n",
      "    learn_time_ms: 3440.427\n",
      "    load_throughput: 7685042.371\n",
      "    load_time_ms: 0.52\n",
      "    sample_throughput: 264.987\n",
      "    sample_time_ms: 15095.101\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1641214197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:58 (running for 00:06:42.02)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         394.092</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:03 (running for 00:06:47.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         394.092</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 492.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.87\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 543\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3628605008125305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037267031148076057\n",
      "          model: {}\n",
      "          policy_loss: -0.0016175990458577871\n",
      "          total_loss: 299.8031005859375\n",
      "          vf_explained_var: 0.33579498529434204\n",
      "          vf_loss: 299.80462646484375\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.233333333333333\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07979603981221543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08284418398483377\n",
      "    mean_inference_ms: 1.1495919600449391\n",
      "    mean_raw_obs_processing_ms: 0.11141823261985014\n",
      "  time_since_restore: 400.56104707717896\n",
      "  time_this_iter_s: 6.468961238861084\n",
      "  time_total_s: 400.56104707717896\n",
      "  timers:\n",
      "    learn_throughput: 1163.158\n",
      "    learn_time_ms: 3438.914\n",
      "    load_throughput: 7657332.725\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 264.004\n",
      "    sample_time_ms: 15151.276\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1641214204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:09 (running for 00:06:52.53)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:14 (running for 00:06:57.54)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:19 (running for 00:07:02.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:24 (running for 00:07:07.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-28\n",
      "  done: false\n",
      "  episode_len_mean: 494.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.29\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 551\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09949913197389361\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10266152362793961\n",
      "      mean_inference_ms: 1.4039306472210185\n",
      "      mean_raw_obs_processing_ms: 0.12231681913493524\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36582711338996887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003521433100104332\n",
      "          model: {}\n",
      "          policy_loss: 0.0003125047078356147\n",
      "          total_loss: 399.474365234375\n",
      "          vf_explained_var: 0.05885005742311478\n",
      "          vf_loss: 399.4740295410156\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.314705882352941\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07971463370052113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08275123618675483\n",
      "    mean_inference_ms: 1.1483230802275544\n",
      "    mean_raw_obs_processing_ms: 0.11122250748701966\n",
      "  time_since_restore: 424.4192636013031\n",
      "  time_this_iter_s: 23.858216524124146\n",
      "  time_total_s: 424.4192636013031\n",
      "  timers:\n",
      "    learn_throughput: 1163.37\n",
      "    learn_time_ms: 3438.287\n",
      "    load_throughput: 7660479.43\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 264.064\n",
      "    sample_time_ms: 15147.847\n",
      "    update_time_ms: 2.1\n",
      "  timestamp: 1641214228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:30 (running for 00:07:13.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         424.419</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  494.29</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            494.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 499.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.09\n",
      "  episode_reward_min: 409.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 559\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3726036250591278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030164276249706745\n",
      "          model: {}\n",
      "          policy_loss: 0.0016188398003578186\n",
      "          total_loss: 363.93804931640625\n",
      "          vf_explained_var: 0.2855881154537201\n",
      "          vf_loss: 363.93646240234375\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.955555555555556\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0796333275130944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08265892413336313\n",
      "    mean_inference_ms: 1.14707785259026\n",
      "    mean_raw_obs_processing_ms: 0.11103458462736569\n",
      "  time_since_restore: 430.7374367713928\n",
      "  time_this_iter_s: 6.318173170089722\n",
      "  time_total_s: 430.7374367713928\n",
      "  timers:\n",
      "    learn_throughput: 1162.619\n",
      "    learn_time_ms: 3440.508\n",
      "    load_throughput: 7618388.884\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 265.154\n",
      "    sample_time_ms: 15085.549\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1641214234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:35 (running for 00:07:18.79)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:40 (running for 00:07:23.79)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:45 (running for 00:07:28.80)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:50 (running for 00:07:33.81)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:55 (running for 00:07:38.82)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 567\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.6\n",
      "    episode_reward_min: 452.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 452\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09941306876026418\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10259009065571885\n",
      "      mean_inference_ms: 1.4029271923729152\n",
      "      mean_raw_obs_processing_ms: 0.12218759803706276\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38642317056655884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028554166201502085\n",
      "          model: {}\n",
      "          policy_loss: 0.002737188246101141\n",
      "          total_loss: 469.2366638183594\n",
      "          vf_explained_var: -0.008716159500181675\n",
      "          vf_loss: 469.23394775390625\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.661764705882355\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07956828918094931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08258411462560852\n",
      "    mean_inference_ms: 1.1460408600055312\n",
      "    mean_raw_obs_processing_ms: 0.11087609551514761\n",
      "  time_since_restore: 454.4033613204956\n",
      "  time_this_iter_s: 23.665924549102783\n",
      "  time_total_s: 454.4033613204956\n",
      "  timers:\n",
      "    learn_throughput: 1163.44\n",
      "    learn_time_ms: 3438.08\n",
      "    load_throughput: 7601819.665\n",
      "    load_time_ms: 0.526\n",
      "    sample_throughput: 265.019\n",
      "    sample_time_ms: 15093.272\n",
      "    update_time_ms: 2.111\n",
      "  timestamp: 1641214258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:01 (running for 00:07:44.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         454.403</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 575\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3741539716720581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032545658759772778\n",
      "          model: {}\n",
      "          policy_loss: -0.0036664640065282583\n",
      "          total_loss: 258.59686279296875\n",
      "          vf_explained_var: 0.35744550824165344\n",
      "          vf_loss: 258.6005554199219\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.266666666666664\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07953178572130858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0825386767939392\n",
      "    mean_inference_ms: 1.1453670318288705\n",
      "    mean_raw_obs_processing_ms: 0.11076034848128394\n",
      "  time_since_restore: 460.95565938949585\n",
      "  time_this_iter_s: 6.552298069000244\n",
      "  time_total_s: 460.95565938949585\n",
      "  timers:\n",
      "    learn_throughput: 1164.393\n",
      "    learn_time_ms: 3435.266\n",
      "    load_throughput: 8106110.064\n",
      "    load_time_ms: 0.493\n",
      "    sample_throughput: 264.587\n",
      "    sample_time_ms: 15117.895\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1641214264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:07 (running for 00:07:50.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:12 (running for 00:07:55.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:17 (running for 00:08:00.11)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:22 (running for 00:08:05.12)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:27 (running for 00:08:10.13)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-28\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 583\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09939444434400664\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10261330896684975\n",
      "      mean_inference_ms: 1.4029501707320542\n",
      "      mean_raw_obs_processing_ms: 0.12211571331520385\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34169572591781616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034873634576797485\n",
      "          model: {}\n",
      "          policy_loss: -0.0019346944754943252\n",
      "          total_loss: 361.1817321777344\n",
      "          vf_explained_var: 0.34877005219459534\n",
      "          vf_loss: 361.18365478515625\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.429411764705883\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0795083969860858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08250855917557369\n",
      "    mean_inference_ms: 1.1448883509574266\n",
      "    mean_raw_obs_processing_ms: 0.11066939700750612\n",
      "  time_since_restore: 484.88394808769226\n",
      "  time_this_iter_s: 23.92828869819641\n",
      "  time_total_s: 484.88394808769226\n",
      "  timers:\n",
      "    learn_throughput: 1164.621\n",
      "    learn_time_ms: 3434.594\n",
      "    load_throughput: 8269934.441\n",
      "    load_time_ms: 0.484\n",
      "    sample_throughput: 264.998\n",
      "    sample_time_ms: 15094.433\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1641214288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:33 (running for 00:08:16.08)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         484.884</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 591\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3170822858810425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0040565794333815575\n",
      "          model: {}\n",
      "          policy_loss: -0.0009620698401704431\n",
      "          total_loss: 325.4109802246094\n",
      "          vf_explained_var: 0.35483792424201965\n",
      "          vf_loss: 325.4119567871094\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.66\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949190811164271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08248702672896545\n",
      "    mean_inference_ms: 1.1445445225731237\n",
      "    mean_raw_obs_processing_ms: 0.11059374707721133\n",
      "  time_since_restore: 491.37184953689575\n",
      "  time_this_iter_s: 6.487901449203491\n",
      "  time_total_s: 491.37184953689575\n",
      "  timers:\n",
      "    learn_throughput: 1164.387\n",
      "    learn_time_ms: 3435.284\n",
      "    load_throughput: 8152194.363\n",
      "    load_time_ms: 0.491\n",
      "    sample_throughput: 265.383\n",
      "    sample_time_ms: 15072.545\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1641214295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:38 (running for 00:08:21.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:43 (running for 00:08:26.62)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:48 (running for 00:08:31.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:53 (running for 00:08:36.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:58 (running for 00:08:41.64)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 599\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09946977952560254\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1026684843078071\n",
      "      mean_inference_ms: 1.4036112275235868\n",
      "      mean_raw_obs_processing_ms: 0.12226953902445337\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34436705708503723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025885074865072966\n",
      "          model: {}\n",
      "          policy_loss: -0.003418699838221073\n",
      "          total_loss: 229.7168731689453\n",
      "          vf_explained_var: 0.23247107863426208\n",
      "          vf_loss: 229.7202606201172\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.452941176470588\n",
      "    ram_util_percent: 11.870588235294118\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07947543049787381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08246425203070337\n",
      "    mean_inference_ms: 1.1441923956151887\n",
      "    mean_raw_obs_processing_ms: 0.11052214775048806\n",
      "  time_since_restore: 515.6011052131653\n",
      "  time_this_iter_s: 24.22925567626953\n",
      "  time_total_s: 515.6011052131653\n",
      "  timers:\n",
      "    learn_throughput: 1163.944\n",
      "    learn_time_ms: 3436.592\n",
      "    load_throughput: 7854869.61\n",
      "    load_time_ms: 0.509\n",
      "    sample_throughput: 264.876\n",
      "    sample_time_ms: 15101.413\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1641214319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:03 (running for 00:08:46.89)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         515.601</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 607\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3192770779132843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038329511880874634\n",
      "          model: {}\n",
      "          policy_loss: -0.0026558642275631428\n",
      "          total_loss: 372.1199951171875\n",
      "          vf_explained_var: 0.2135278582572937\n",
      "          vf_loss: 372.1226501464844\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.929999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794510540005532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08243323290971885\n",
      "    mean_inference_ms: 1.1437380434740003\n",
      "    mean_raw_obs_processing_ms: 0.11044263750401666\n",
      "  time_since_restore: 522.1010003089905\n",
      "  time_this_iter_s: 6.499895095825195\n",
      "  time_total_s: 522.1010003089905\n",
      "  timers:\n",
      "    learn_throughput: 1164.569\n",
      "    learn_time_ms: 3434.747\n",
      "    load_throughput: 8368940.989\n",
      "    load_time_ms: 0.478\n",
      "    sample_throughput: 262.928\n",
      "    sample_time_ms: 15213.262\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1641214326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:09 (running for 00:08:52.42)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:14 (running for 00:08:57.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:19 (running for 00:09:02.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:24 (running for 00:09:07.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:29 (running for 00:09:12.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 615\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09948361939194289\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10270222213565239\n",
      "      mean_inference_ms: 1.4040903515341654\n",
      "      mean_raw_obs_processing_ms: 0.12227653393190181\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31491655111312866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033327066339552402\n",
      "          model: {}\n",
      "          policy_loss: -0.0007229884504340589\n",
      "          total_loss: 494.2162780761719\n",
      "          vf_explained_var: 0.005715495441108942\n",
      "          vf_loss: 494.2170104980469\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.305882352941177\n",
      "    ram_util_percent: 11.820588235294117\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07945064011080472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08242891952645134\n",
      "    mean_inference_ms: 1.1436184922174488\n",
      "    mean_raw_obs_processing_ms: 0.11039615979228397\n",
      "  time_since_restore: 546.2564222812653\n",
      "  time_this_iter_s: 24.15542197227478\n",
      "  time_total_s: 546.2564222812653\n",
      "  timers:\n",
      "    learn_throughput: 1164.203\n",
      "    learn_time_ms: 3435.827\n",
      "    load_throughput: 9085463.013\n",
      "    load_time_ms: 0.44\n",
      "    sample_throughput: 262.547\n",
      "    sample_time_ms: 15235.358\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1641214350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:34 (running for 00:09:17.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         546.256</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-37\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 623\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762788286319847e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33596229553222656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003063222859054804\n",
      "          model: {}\n",
      "          policy_loss: -0.0007319195428863168\n",
      "          total_loss: 502.2861328125\n",
      "          vf_explained_var: -0.01001955009996891\n",
      "          vf_loss: 502.2868347167969\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.933333333333334\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07946011831913216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0824354710751524\n",
      "    mean_inference_ms: 1.1436666500625277\n",
      "    mean_raw_obs_processing_ms: 0.11036115238870597\n",
      "  time_since_restore: 552.7793071269989\n",
      "  time_this_iter_s: 6.522884845733643\n",
      "  time_total_s: 552.7793071269989\n",
      "  timers:\n",
      "    learn_throughput: 1164.516\n",
      "    learn_time_ms: 3434.904\n",
      "    load_throughput: 9100247.342\n",
      "    load_time_ms: 0.44\n",
      "    sample_throughput: 262.011\n",
      "    sample_time_ms: 15266.558\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1641214357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:40 (running for 00:09:23.19)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:45 (running for 00:09:28.20)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:50 (running for 00:09:33.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:55 (running for 00:09:38.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:00 (running for 00:09:43.23)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-00\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 631\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 490.95\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 490.95\n",
      "    episode_reward_min: 319.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 319\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 319.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09946971032991077\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10267935775545815\n",
      "      mean_inference_ms: 1.4039944099567374\n",
      "      mean_raw_obs_processing_ms: 0.12224188563527427\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3237580358982086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028110574930906296\n",
      "          model: {}\n",
      "          policy_loss: -0.00015313291805796325\n",
      "          total_loss: 514.155029296875\n",
      "          vf_explained_var: -0.09653975814580917\n",
      "          vf_loss: 514.1551513671875\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.614705882352938\n",
      "    ram_util_percent: 11.814705882352941\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07948881806111809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08246290306069122\n",
      "    mean_inference_ms: 1.1439807044381347\n",
      "    mean_raw_obs_processing_ms: 0.110351855341977\n",
      "  time_since_restore: 576.4864287376404\n",
      "  time_this_iter_s: 23.70712161064148\n",
      "  time_total_s: 576.4864287376404\n",
      "  timers:\n",
      "    learn_throughput: 1163.565\n",
      "    learn_time_ms: 3437.711\n",
      "    load_throughput: 8904631.389\n",
      "    load_time_ms: 0.449\n",
      "    sample_throughput: 261.773\n",
      "    sample_time_ms: 15280.425\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1641214380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:05 (running for 00:09:48.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         576.486</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 639\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.940697071579962e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.35207512974739075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028695587534457445\n",
      "          model: {}\n",
      "          policy_loss: -0.005121050402522087\n",
      "          total_loss: 273.1157531738281\n",
      "          vf_explained_var: 0.08673982322216034\n",
      "          vf_loss: 273.1208801269531\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.059999999999997\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07951985567033365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08249402599508113\n",
      "    mean_inference_ms: 1.1443259011003093\n",
      "    mean_raw_obs_processing_ms: 0.1103508887481165\n",
      "  time_since_restore: 582.979861497879\n",
      "  time_this_iter_s: 6.4934327602386475\n",
      "  time_total_s: 582.979861497879\n",
      "  timers:\n",
      "    learn_throughput: 1164.283\n",
      "    learn_time_ms: 3435.592\n",
      "    load_throughput: 9757599.162\n",
      "    load_time_ms: 0.41\n",
      "    sample_throughput: 261.938\n",
      "    sample_time_ms: 15270.811\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1641214387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:11 (running for 00:09:54.48)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:16 (running for 00:09:59.49)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:21 (running for 00:10:04.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:26 (running for 00:10:09.51)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 647\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 471.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 471.9\n",
      "    episode_reward_min: 242.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 254\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 242\n",
      "      - 473\n",
      "      - 500\n",
      "      - 500\n",
      "      - 469\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 254.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 242.0\n",
      "      - 473.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 469.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09965025751636167\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10281629507669261\n",
      "      mean_inference_ms: 1.4054618489181412\n",
      "      mean_raw_obs_processing_ms: 0.1223282085178143\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3788440525531769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017615531105548143\n",
      "          model: {}\n",
      "          policy_loss: -0.001946029718965292\n",
      "          total_loss: 427.7779846191406\n",
      "          vf_explained_var: -0.09892924875020981\n",
      "          vf_loss: 427.7799377441406\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.457575757575759\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07955387161304833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08252783961940695\n",
      "    mean_inference_ms: 1.1447204782187546\n",
      "    mean_raw_obs_processing_ms: 0.11035193494428737\n",
      "  time_since_restore: 606.4087183475494\n",
      "  time_this_iter_s: 23.42885684967041\n",
      "  time_total_s: 606.4087183475494\n",
      "  timers:\n",
      "    learn_throughput: 1163.901\n",
      "    learn_time_ms: 3436.719\n",
      "    load_throughput: 10648144.199\n",
      "    load_time_ms: 0.376\n",
      "    sample_throughput: 261.79\n",
      "    sample_time_ms: 15279.408\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1641214410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:31 (running for 00:10:14.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         606.409</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:36 (running for 00:10:19.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         606.409</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 498.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.92\n",
      "  episode_reward_min: 392.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 656\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351742678949904e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3735828697681427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004065442830324173\n",
      "          model: {}\n",
      "          policy_loss: -0.0010430102702230215\n",
      "          total_loss: 464.3570556640625\n",
      "          vf_explained_var: 0.017288917675614357\n",
      "          vf_loss: 464.3581237792969\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.839999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07961279127955698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08259065100470758\n",
      "    mean_inference_ms: 1.145475742766385\n",
      "    mean_raw_obs_processing_ms: 0.11038388157426014\n",
      "  time_since_restore: 613.0383384227753\n",
      "  time_this_iter_s: 6.62962007522583\n",
      "  time_total_s: 613.0383384227753\n",
      "  timers:\n",
      "    learn_throughput: 1163.255\n",
      "    learn_time_ms: 3438.628\n",
      "    load_throughput: 10523909.171\n",
      "    load_time_ms: 0.38\n",
      "    sample_throughput: 262.294\n",
      "    sample_time_ms: 15250.054\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1641214417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:42 (running for 00:10:25.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:47 (running for 00:10:30.62)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:52 (running for 00:10:35.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:57 (running for 00:10:40.64)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 664\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 494.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 494.9\n",
      "    episode_reward_min: 399.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 499\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09971305891514728\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10286078802189898\n",
      "      mean_inference_ms: 1.4063051596587663\n",
      "      mean_raw_obs_processing_ms: 0.12235596499347381\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1175871339474952e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.37061235308647156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032300094608217478\n",
      "          model: {}\n",
      "          policy_loss: -0.006143657024949789\n",
      "          total_loss: 378.09521484375\n",
      "          vf_explained_var: 0.14620283246040344\n",
      "          vf_loss: 378.10137939453125\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.344117647058823\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0796581589068329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08263862276650341\n",
      "    mean_inference_ms: 1.1460402830098035\n",
      "    mean_raw_obs_processing_ms: 0.11040482993997289\n",
      "  time_since_restore: 636.8817765712738\n",
      "  time_this_iter_s: 23.843438148498535\n",
      "  time_total_s: 636.8817765712738\n",
      "  timers:\n",
      "    learn_throughput: 1164.196\n",
      "    learn_time_ms: 3435.848\n",
      "    load_throughput: 10515993.481\n",
      "    load_time_ms: 0.38\n",
      "    sample_throughput: 262.459\n",
      "    sample_time_ms: 15240.49\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1641214441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:03 (running for 00:10:46.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         636.882</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 672\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3455185294151306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016453824937343597\n",
      "          model: {}\n",
      "          policy_loss: -0.001856086659245193\n",
      "          total_loss: 263.1059265136719\n",
      "          vf_explained_var: 0.15679922699928284\n",
      "          vf_loss: 263.1077880859375\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.322222222222223\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07969534660773528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08267752127570777\n",
      "    mean_inference_ms: 1.1465005841811662\n",
      "    mean_raw_obs_processing_ms: 0.1104197732897764\n",
      "  time_since_restore: 643.3969051837921\n",
      "  time_this_iter_s: 6.5151286125183105\n",
      "  time_total_s: 643.3969051837921\n",
      "  timers:\n",
      "    learn_throughput: 1164.311\n",
      "    learn_time_ms: 3435.509\n",
      "    load_throughput: 9742866.434\n",
      "    load_time_ms: 0.411\n",
      "    sample_throughput: 262.375\n",
      "    sample_time_ms: 15245.338\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1641214447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:09 (running for 00:10:52.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:14 (running for 00:10:57.07)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:19 (running for 00:11:02.08)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:24 (running for 00:11:07.09)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:29 (running for 00:11:12.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 680\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976714310559924\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10284341906678422\n",
      "      mean_inference_ms: 1.4063103343901902\n",
      "      mean_raw_obs_processing_ms: 0.12231234045793518\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.793967834868738e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33968988060951233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0019635462667793036\n",
      "          model: {}\n",
      "          policy_loss: -0.0013499544002115726\n",
      "          total_loss: 367.69903564453125\n",
      "          vf_explained_var: 0.03755499795079231\n",
      "          vf_loss: 367.70037841796875\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.461764705882352\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07973023632468008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08271456330389965\n",
      "    mean_inference_ms: 1.1469383131402473\n",
      "    mean_raw_obs_processing_ms: 0.1104306162093986\n",
      "  time_since_restore: 667.5081536769867\n",
      "  time_this_iter_s: 24.11124849319458\n",
      "  time_total_s: 667.5081536769867\n",
      "  timers:\n",
      "    learn_throughput: 1164.357\n",
      "    learn_time_ms: 3435.372\n",
      "    load_throughput: 9548241.99\n",
      "    load_time_ms: 0.419\n",
      "    sample_throughput: 262.385\n",
      "    sample_time_ms: 15244.755\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1641214472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:34 (running for 00:11:17.22)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         667.508</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 688\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.396983917434369e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34701216220855713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024114095140248537\n",
      "          model: {}\n",
      "          policy_loss: 0.00015137766604311764\n",
      "          total_loss: 370.8667297363281\n",
      "          vf_explained_var: -0.011806507594883442\n",
      "          vf_loss: 370.8665771484375\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.669999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07977179852190504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0827573745106352\n",
      "    mean_inference_ms: 1.1474494306306835\n",
      "    mean_raw_obs_processing_ms: 0.11044989737693695\n",
      "  time_since_restore: 674.0614902973175\n",
      "  time_this_iter_s: 6.5533366203308105\n",
      "  time_total_s: 674.0614902973175\n",
      "  timers:\n",
      "    learn_throughput: 1163.344\n",
      "    learn_time_ms: 3438.363\n",
      "    load_throughput: 8897075.887\n",
      "    load_time_ms: 0.45\n",
      "    sample_throughput: 262.549\n",
      "    sample_time_ms: 15235.259\n",
      "    update_time_ms: 2.324\n",
      "  timestamp: 1641214478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:39 (running for 00:11:22.81)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:44 (running for 00:11:27.82)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:49 (running for 00:11:32.83)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:54 (running for 00:11:37.83)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:59 (running for 00:11:42.84)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-02\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 696\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 496.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 496.55\n",
      "    episode_reward_min: 431.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 431\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 431.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09983354106727835\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10290856197559965\n",
      "      mean_inference_ms: 1.4073860516169066\n",
      "      mean_raw_obs_processing_ms: 0.12239292282595482\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.984919587171845e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33118343353271484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025007727090269327\n",
      "          model: {}\n",
      "          policy_loss: -0.0012579438043758273\n",
      "          total_loss: 270.6835021972656\n",
      "          vf_explained_var: 0.38449805974960327\n",
      "          vf_loss: 270.68475341796875\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.43529411764706\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07980871882638868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08279560733589987\n",
      "    mean_inference_ms: 1.1478979040697703\n",
      "    mean_raw_obs_processing_ms: 0.11046145646450796\n",
      "  time_since_restore: 698.238331079483\n",
      "  time_this_iter_s: 24.176840782165527\n",
      "  time_total_s: 698.238331079483\n",
      "  timers:\n",
      "    learn_throughput: 1163.772\n",
      "    learn_time_ms: 3437.1\n",
      "    load_throughput: 8284226.743\n",
      "    load_time_ms: 0.483\n",
      "    sample_throughput: 262.59\n",
      "    sample_time_ms: 15232.863\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1641214502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:04 (running for 00:11:48.04)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         698.238</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 704\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4924597935859225e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28589412569999695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026714280247688293\n",
      "          model: {}\n",
      "          policy_loss: -0.001123060705140233\n",
      "          total_loss: 340.09228515625\n",
      "          vf_explained_var: 0.23291447758674622\n",
      "          vf_loss: 340.0933532714844\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.979999999999999\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07984557957922908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08283436602376197\n",
      "    mean_inference_ms: 1.148349744115344\n",
      "    mean_raw_obs_processing_ms: 0.11047810317648876\n",
      "  time_since_restore: 704.7191970348358\n",
      "  time_this_iter_s: 6.480865955352783\n",
      "  time_total_s: 704.7191970348358\n",
      "  timers:\n",
      "    learn_throughput: 1164.736\n",
      "    learn_time_ms: 3434.256\n",
      "    load_throughput: 8351859.817\n",
      "    load_time_ms: 0.479\n",
      "    sample_throughput: 262.472\n",
      "    sample_time_ms: 15239.705\n",
      "    update_time_ms: 2.312\n",
      "  timestamp: 1641214509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:10 (running for 00:11:53.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:15 (running for 00:11:58.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:20 (running for 00:12:03.57)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:25 (running for 00:12:08.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:30 (running for 00:12:13.59)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 712\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09973444154478656\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10281132505136674\n",
      "      mean_inference_ms: 1.4064705656434335\n",
      "      mean_raw_obs_processing_ms: 0.12226821385175272\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7462298967929613e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28462663292884827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030475114472210407\n",
      "          model: {}\n",
      "          policy_loss: -0.0005468263407237828\n",
      "          total_loss: 327.8911437988281\n",
      "          vf_explained_var: 0.3008342683315277\n",
      "          vf_loss: 327.8916931152344\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.266666666666666\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07987973208792945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08287053758138593\n",
      "    mean_inference_ms: 1.1487766856984327\n",
      "    mean_raw_obs_processing_ms: 0.11049268285016811\n",
      "  time_since_restore: 728.3583090305328\n",
      "  time_this_iter_s: 23.63911199569702\n",
      "  time_total_s: 728.3583090305328\n",
      "  timers:\n",
      "    learn_throughput: 1164.687\n",
      "    learn_time_ms: 3434.4\n",
      "    load_throughput: 8688356.292\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 262.812\n",
      "    sample_time_ms: 15220.034\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1641214533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:36 (running for 00:12:19.25)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         728.358</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 720\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.731149483964806e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2651657164096832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003235872834920883\n",
      "          model: {}\n",
      "          policy_loss: -0.00046385437599383295\n",
      "          total_loss: 410.3289794921875\n",
      "          vf_explained_var: 0.14666806161403656\n",
      "          vf_loss: 410.32940673828125\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.079912490231387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08290536445603257\n",
      "    mean_inference_ms: 1.1491775600808136\n",
      "    mean_raw_obs_processing_ms: 0.11050723811283886\n",
      "  time_since_restore: 734.9852323532104\n",
      "  time_this_iter_s: 6.626923322677612\n",
      "  time_total_s: 734.9852323532104\n",
      "  timers:\n",
      "    learn_throughput: 1163.672\n",
      "    learn_time_ms: 3437.396\n",
      "    load_throughput: 8540196.488\n",
      "    load_time_ms: 0.468\n",
      "    sample_throughput: 262.455\n",
      "    sample_time_ms: 15240.733\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1641214539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:41 (running for 00:12:24.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:46 (running for 00:12:29.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:51 (running for 00:12:34.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:56 (running for 00:12:39.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:01 (running for 00:12:44.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 728\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976090878042859\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10285603627104975\n",
      "      mean_inference_ms: 1.4069825283660182\n",
      "      mean_raw_obs_processing_ms: 0.12229781694092796\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574741982403e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.32048898935317993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004733977373689413\n",
      "          model: {}\n",
      "          policy_loss: -0.008360615000128746\n",
      "          total_loss: 257.9974670410156\n",
      "          vf_explained_var: 0.47615835070610046\n",
      "          vf_loss: 258.005859375\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.488235294117647\n",
      "    ram_util_percent: 11.805882352941175\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07994238130834193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08293769930998028\n",
      "    mean_inference_ms: 1.1495395952376788\n",
      "    mean_raw_obs_processing_ms: 0.11052202672905281\n",
      "  time_since_restore: 759.1922824382782\n",
      "  time_this_iter_s: 24.20705008506775\n",
      "  time_total_s: 759.1922824382782\n",
      "  timers:\n",
      "    learn_throughput: 1162.878\n",
      "    learn_time_ms: 3439.741\n",
      "    load_throughput: 7971688.682\n",
      "    load_time_ms: 0.502\n",
      "    sample_throughput: 262.464\n",
      "    sample_time_ms: 15240.186\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1641214564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:07 (running for 00:12:50.18)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         759.192</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 736\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827873709912016e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28173762559890747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032018960919231176\n",
      "          model: {}\n",
      "          policy_loss: -0.0011629423825070262\n",
      "          total_loss: 304.3319091796875\n",
      "          vf_explained_var: 0.37079381942749023\n",
      "          vf_loss: 304.33306884765625\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 11.85\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0799674059460663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08296538792707649\n",
      "    mean_inference_ms: 1.1498512246663766\n",
      "    mean_raw_obs_processing_ms: 0.11052903807749895\n",
      "  time_since_restore: 765.6568610668182\n",
      "  time_this_iter_s: 6.464578628540039\n",
      "  time_total_s: 765.6568610668182\n",
      "  timers:\n",
      "    learn_throughput: 1162.294\n",
      "    learn_time_ms: 3441.469\n",
      "    load_throughput: 7554582.133\n",
      "    load_time_ms: 0.529\n",
      "    sample_throughput: 261.357\n",
      "    sample_time_ms: 15304.765\n",
      "    update_time_ms: 2.504\n",
      "  timestamp: 1641214570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:12 (running for 00:12:55.68)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:17 (running for 00:13:00.69)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:22 (running for 00:13:05.70)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:27 (running for 00:13:10.71)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:32 (running for 00:13:15.72)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 744\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09978637612975969\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10289952802479876\n",
      "      mean_inference_ms: 1.4080818673795648\n",
      "      mean_raw_obs_processing_ms: 0.12232249671802926\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0913936854956008e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3075120747089386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003919175360351801\n",
      "          model: {}\n",
      "          policy_loss: -0.002855784958228469\n",
      "          total_loss: 225.8036346435547\n",
      "          vf_explained_var: 0.4734848141670227\n",
      "          vf_loss: 225.80648803710938\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.482352941176472\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0799951621512265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08299653071822777\n",
      "    mean_inference_ms: 1.1502019761011724\n",
      "    mean_raw_obs_processing_ms: 0.11054397667879921\n",
      "  time_since_restore: 789.9095482826233\n",
      "  time_this_iter_s: 24.252687215805054\n",
      "  time_total_s: 789.9095482826233\n",
      "  timers:\n",
      "    learn_throughput: 1162.254\n",
      "    learn_time_ms: 3441.589\n",
      "    load_throughput: 7461182.958\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 261.134\n",
      "    sample_time_ms: 15317.803\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1641214594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:37 (running for 00:13:20.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          789.91</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 752\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.456968427478004e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3077785074710846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001683449256233871\n",
      "          model: {}\n",
      "          policy_loss: -0.00023939917446114123\n",
      "          total_loss: 297.7902526855469\n",
      "          vf_explained_var: 0.4101290702819824\n",
      "          vf_loss: 297.79052734375\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.511111111111111\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08000520479022466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08300801537829423\n",
      "    mean_inference_ms: 1.1503124822190636\n",
      "    mean_raw_obs_processing_ms: 0.1105389686971247\n",
      "  time_since_restore: 796.3458256721497\n",
      "  time_this_iter_s: 6.436277389526367\n",
      "  time_total_s: 796.3458256721497\n",
      "  timers:\n",
      "    learn_throughput: 1162.023\n",
      "    learn_time_ms: 3442.273\n",
      "    load_throughput: 7511289.398\n",
      "    load_time_ms: 0.533\n",
      "    sample_throughput: 260.784\n",
      "    sample_time_ms: 15338.367\n",
      "    update_time_ms: 2.548\n",
      "  timestamp: 1641214601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:43 (running for 00:13:26.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:48 (running for 00:13:31.46)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:53 (running for 00:13:36.47)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:58 (running for 00:13:41.47)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:03 (running for 00:13:46.48)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-04\n",
      "  done: false\n",
      "  episode_len_mean: 497.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.99\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 760\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 476.3\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 476.3\n",
      "    episode_reward_min: 352.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 352\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 359\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 353\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 462\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 352.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 359.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 353.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 462.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09984009959303886\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10294986878947832\n",
      "      mean_inference_ms: 1.4088995640454605\n",
      "      mean_raw_obs_processing_ms: 0.12238530637763405\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.728484213739002e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2721678912639618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006919963285326958\n",
      "          model: {}\n",
      "          policy_loss: -0.004764837678521872\n",
      "          total_loss: 341.0838623046875\n",
      "          vf_explained_var: 0.24915827810764313\n",
      "          vf_loss: 341.0886535644531\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.638235294117646\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08002320923740176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08302857086602768\n",
      "    mean_inference_ms: 1.150534127225943\n",
      "    mean_raw_obs_processing_ms: 0.11054501727885163\n",
      "  time_since_restore: 819.8854575157166\n",
      "  time_this_iter_s: 23.539631843566895\n",
      "  time_total_s: 819.8854575157166\n",
      "  timers:\n",
      "    learn_throughput: 1162.541\n",
      "    learn_time_ms: 3440.739\n",
      "    load_throughput: 7659779.939\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 260.771\n",
      "    sample_time_ms: 15339.1\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1641214624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:08 (running for 00:13:52.05)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         819.885</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  497.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            497.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 498.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.75\n",
      "  episode_reward_min: 459.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 768\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.728484213739002e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27726444602012634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002490389160811901\n",
      "          model: {}\n",
      "          policy_loss: -0.0012705748667940497\n",
      "          total_loss: 212.70944213867188\n",
      "          vf_explained_var: 0.4870787262916565\n",
      "          vf_loss: 212.71072387695312\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.066666666666666\n",
      "    ram_util_percent: 11.877777777777778\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08003794685929408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0830455903386086\n",
      "    mean_inference_ms: 1.1507252604753953\n",
      "    mean_raw_obs_processing_ms: 0.11054702976800378\n",
      "  time_since_restore: 826.3715515136719\n",
      "  time_this_iter_s: 6.486093997955322\n",
      "  time_total_s: 826.3715515136719\n",
      "  timers:\n",
      "    learn_throughput: 1162.56\n",
      "    learn_time_ms: 3440.684\n",
      "    load_throughput: 7645468.465\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 261.845\n",
      "    sample_time_ms: 15276.206\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1641214631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:14 (running for 00:13:57.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:19 (running for 00:14:02.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:24 (running for 00:14:07.59)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:29 (running for 00:14:12.60)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:34 (running for 00:14:17.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 497.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.89\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 776\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 496.05\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 496.05\n",
      "    episode_reward_min: 447.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 447\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 484\n",
      "      - 490\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 447.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 484.0\n",
      "      - 490.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0997692268760024\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10283119178938802\n",
      "      mean_inference_ms: 1.4076848126861221\n",
      "      mean_raw_obs_processing_ms: 0.12223860666725266\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.364242106869501e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2788257300853729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002548348158597946\n",
      "          model: {}\n",
      "          policy_loss: -0.001480271341279149\n",
      "          total_loss: 185.6182098388672\n",
      "          vf_explained_var: 0.5972358584403992\n",
      "          vf_loss: 185.61968994140625\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.591176470588236\n",
      "    ram_util_percent: 11.80294117647059\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08004803677071749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08305722379350482\n",
      "    mean_inference_ms: 1.150849956281451\n",
      "    mean_raw_obs_processing_ms: 0.11054440264789316\n",
      "  time_since_restore: 849.8774726390839\n",
      "  time_this_iter_s: 23.505921125411987\n",
      "  time_total_s: 849.8774726390839\n",
      "  timers:\n",
      "    learn_throughput: 1162.076\n",
      "    learn_time_ms: 3442.114\n",
      "    load_throughput: 7623235.187\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 261.842\n",
      "    sample_time_ms: 15276.417\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1641214655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:40 (running for 00:14:23.12)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         849.877</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  497.89</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 497.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.62\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 784\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.821210534347505e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2768877446651459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002392576541751623\n",
      "          model: {}\n",
      "          policy_loss: -0.0008955386583693326\n",
      "          total_loss: 288.79681396484375\n",
      "          vf_explained_var: 0.2627524137496948\n",
      "          vf_loss: 288.7977294921875\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.56666666666667\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.080058435280725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08306925872963983\n",
      "    mean_inference_ms: 1.1509798152026431\n",
      "    mean_raw_obs_processing_ms: 0.11054508493370786\n",
      "  time_since_restore: 856.478374004364\n",
      "  time_this_iter_s: 6.600901365280151\n",
      "  time_total_s: 856.478374004364\n",
      "  timers:\n",
      "    learn_throughput: 1160.273\n",
      "    learn_time_ms: 3447.465\n",
      "    load_throughput: 7579496.725\n",
      "    load_time_ms: 0.528\n",
      "    sample_throughput: 262.905\n",
      "    sample_time_ms: 15214.624\n",
      "    update_time_ms: 2.455\n",
      "  timestamp: 1641214661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:45 (running for 00:14:28.76)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:50 (running for 00:14:33.77)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:55 (running for 00:14:38.78)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:00 (running for 00:14:43.78)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 792\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976934566782643\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10276283948817229\n",
      "      mean_inference_ms: 1.407192663059987\n",
      "      mean_raw_obs_processing_ms: 0.12216108791504979\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4106052671737525e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29833635687828064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005802231375128031\n",
      "          model: {}\n",
      "          policy_loss: -0.0041998811066150665\n",
      "          total_loss: 221.92808532714844\n",
      "          vf_explained_var: 0.399289608001709\n",
      "          vf_loss: 221.93231201171875\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.38823529411765\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006413229682677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08307639196592119\n",
      "    mean_inference_ms: 1.151053500467093\n",
      "    mean_raw_obs_processing_ms: 0.11053999618092711\n",
      "  time_since_restore: 880.211318731308\n",
      "  time_this_iter_s: 23.73294472694397\n",
      "  time_total_s: 880.211318731308\n",
      "  timers:\n",
      "    learn_throughput: 1160.431\n",
      "    learn_time_ms: 3446.997\n",
      "    load_throughput: 7464834.705\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 262.815\n",
      "    sample_time_ms: 15219.837\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1641214685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:06 (running for 00:14:49.53)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         880.211</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:11 (running for 00:14:54.54)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         880.211</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 800\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4106052671737525e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27059662342071533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037121737841516733\n",
      "          model: {}\n",
      "          policy_loss: -0.0013116763439029455\n",
      "          total_loss: 359.6751708984375\n",
      "          vf_explained_var: 0.24808254837989807\n",
      "          vf_loss: 359.6764831542969\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.01\n",
      "    ram_util_percent: 11.639999999999997\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006899929365378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08308271113012815\n",
      "    mean_inference_ms: 1.1511144327498926\n",
      "    mean_raw_obs_processing_ms: 0.11053576293399457\n",
      "  time_since_restore: 886.7374684810638\n",
      "  time_this_iter_s: 6.526149749755859\n",
      "  time_total_s: 886.7374684810638\n",
      "  timers:\n",
      "    learn_throughput: 1161.623\n",
      "    learn_time_ms: 3443.458\n",
      "    load_throughput: 7295393.312\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 262.784\n",
      "    sample_time_ms: 15221.654\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1641214691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:17 (running for 00:15:00.11)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:22 (running for 00:15:05.11)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:27 (running for 00:15:10.12)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:32 (running for 00:15:15.13)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 808\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 498.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 498.6\n",
      "    episode_reward_min: 472.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 472\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 472.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09979632241253603\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10279804331082583\n",
      "      mean_inference_ms: 1.4078386921465798\n",
      "      mean_raw_obs_processing_ms: 0.12217542268668727\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7053026335868762e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29947254061698914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016222784761339426\n",
      "          model: {}\n",
      "          policy_loss: -3.099422247032635e-05\n",
      "          total_loss: 379.5009460449219\n",
      "          vf_explained_var: 0.20277585089206696\n",
      "          vf_loss: 379.5009460449219\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.311764705882354\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08007952608832712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08309573476078519\n",
      "    mean_inference_ms: 1.1512581942874394\n",
      "    mean_raw_obs_processing_ms: 0.11053790665641436\n",
      "  time_since_restore: 911.0552768707275\n",
      "  time_this_iter_s: 24.317808389663696\n",
      "  time_total_s: 911.0552768707275\n",
      "  timers:\n",
      "    learn_throughput: 1160.381\n",
      "    learn_time_ms: 3447.144\n",
      "    load_throughput: 7535241.859\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 262.739\n",
      "    sample_time_ms: 15224.239\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1641214716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:37 (running for 00:15:20.46)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         911.055</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:42 (running for 00:15:25.47)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         911.055</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 816\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.526513167934381e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29696959257125854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003980488050729036\n",
      "          model: {}\n",
      "          policy_loss: -0.002883958863094449\n",
      "          total_loss: 318.4349060058594\n",
      "          vf_explained_var: 0.21819797158241272\n",
      "          vf_loss: 318.43780517578125\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.059999999999999\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08008427241833951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08310188992836416\n",
      "    mean_inference_ms: 1.1513195327906394\n",
      "    mean_raw_obs_processing_ms: 0.11053224961197167\n",
      "  time_since_restore: 917.5175948143005\n",
      "  time_this_iter_s: 6.462317943572998\n",
      "  time_total_s: 917.5175948143005\n",
      "  timers:\n",
      "    learn_throughput: 1161.434\n",
      "    learn_time_ms: 3444.019\n",
      "    load_throughput: 8057833.918\n",
      "    load_time_ms: 0.496\n",
      "    sample_throughput: 262.606\n",
      "    sample_time_ms: 15231.927\n",
      "    update_time_ms: 2.317\n",
      "  timestamp: 1641214722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:47 (running for 00:15:30.97)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:52 (running for 00:15:35.98)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:57 (running for 00:15:40.98)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:02 (running for 00:15:45.99)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-06\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 824\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09981694013766426\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10280622675018387\n",
      "      mean_inference_ms: 1.4079228930650882\n",
      "      mean_raw_obs_processing_ms: 0.12216947691168205\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.2632565839671906e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2887202203273773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00418429309502244\n",
      "          model: {}\n",
      "          policy_loss: -0.004043938592076302\n",
      "          total_loss: 210.58938598632812\n",
      "          vf_explained_var: 0.32004860043525696\n",
      "          vf_loss: 210.59341430664062\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.941176470588236\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08009064129429111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08311008750641999\n",
      "    mean_inference_ms: 1.1514064395225148\n",
      "    mean_raw_obs_processing_ms: 0.11052659452047892\n",
      "  time_since_restore: 941.5922567844391\n",
      "  time_this_iter_s: 24.07466197013855\n",
      "  time_total_s: 941.5922567844391\n",
      "  timers:\n",
      "    learn_throughput: 1160.555\n",
      "    learn_time_ms: 3446.626\n",
      "    load_throughput: 8614302.732\n",
      "    load_time_ms: 0.464\n",
      "    sample_throughput: 262.561\n",
      "    sample_time_ms: 15234.572\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1641214746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:08 (running for 00:15:51.09)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         941.592</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:13 (running for 00:15:56.10)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         941.592</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 832\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1316282919835953e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3001687526702881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005532457958906889\n",
      "          model: {}\n",
      "          policy_loss: -0.0025179916992783546\n",
      "          total_loss: 415.4772644042969\n",
      "          vf_explained_var: 0.09152212738990784\n",
      "          vf_loss: 415.4797668457031\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.144444444444444\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08009958874335484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08312176771071565\n",
      "    mean_inference_ms: 1.1515184400249967\n",
      "    mean_raw_obs_processing_ms: 0.11052816011746053\n",
      "  time_since_restore: 948.113970041275\n",
      "  time_this_iter_s: 6.5217132568359375\n",
      "  time_total_s: 948.113970041275\n",
      "  timers:\n",
      "    learn_throughput: 1160.149\n",
      "    learn_time_ms: 3447.833\n",
      "    load_throughput: 8535417.175\n",
      "    load_time_ms: 0.469\n",
      "    sample_throughput: 262.826\n",
      "    sample_time_ms: 15219.175\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1641214753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:18 (running for 00:16:01.65)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:23 (running for 00:16:06.66)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:28 (running for 00:16:11.67)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:33 (running for 00:16:16.68)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 840\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 494.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 494.65\n",
      "    episode_reward_min: 399.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 494\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 494.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09989183278780296\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10291821737926685\n",
      "      mean_inference_ms: 1.4092371092524651\n",
      "      mean_raw_obs_processing_ms: 0.12222442483607858\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1316282919835953e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3224461078643799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003274397226050496\n",
      "          model: {}\n",
      "          policy_loss: -0.002108244923874736\n",
      "          total_loss: 322.9053039550781\n",
      "          vf_explained_var: 0.12203709781169891\n",
      "          vf_loss: 322.9073791503906\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.665714285714287\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08010812866505933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08313288745019998\n",
      "    mean_inference_ms: 1.15160513062406\n",
      "    mean_raw_obs_processing_ms: 0.11052948807726296\n",
      "  time_since_restore: 972.208221912384\n",
      "  time_this_iter_s: 24.09425187110901\n",
      "  time_total_s: 972.208221912384\n",
      "  timers:\n",
      "    learn_throughput: 1161.333\n",
      "    learn_time_ms: 3444.316\n",
      "    load_throughput: 9147383.458\n",
      "    load_time_ms: 0.437\n",
      "    sample_throughput: 263.279\n",
      "    sample_time_ms: 15193.01\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1641214777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:38 (running for 00:16:21.80)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         972.208</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:43 (running for 00:16:26.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         972.208</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 497.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.75\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 848\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0658141459917976e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3097001016139984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002771975938230753\n",
      "          model: {}\n",
      "          policy_loss: -0.003007206367328763\n",
      "          total_loss: 239.74151611328125\n",
      "          vf_explained_var: 0.2675352692604065\n",
      "          vf_loss: 239.74452209472656\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.309999999999999\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0801234646827389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08315262206165483\n",
      "    mean_inference_ms: 1.1517882543068423\n",
      "    mean_raw_obs_processing_ms: 0.11053882892200324\n",
      "  time_since_restore: 979.170768737793\n",
      "  time_this_iter_s: 6.9625468254089355\n",
      "  time_total_s: 979.170768737793\n",
      "  timers:\n",
      "    learn_throughput: 1153.135\n",
      "    learn_time_ms: 3468.806\n",
      "    load_throughput: 9043346.27\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 261.46\n",
      "    sample_time_ms: 15298.698\n",
      "    update_time_ms: 2.332\n",
      "  timestamp: 1641214784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:49 (running for 00:16:32.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:54 (running for 00:16:37.82)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:59 (running for 00:16:42.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:04 (running for 00:16:47.84)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:09 (running for 00:16:52.85)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 498.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.16\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 856\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 499.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 499.1\n",
      "    episode_reward_min: 484.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 484\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 498\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 484.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 498.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10020298272581836\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1032840614585954\n",
      "      mean_inference_ms: 1.413428283674976\n",
      "      mean_raw_obs_processing_ms: 0.12253733467458183\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.329070729958988e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2968416213989258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004095285199582577\n",
      "          model: {}\n",
      "          policy_loss: -0.0013736553955823183\n",
      "          total_loss: 223.34593200683594\n",
      "          vf_explained_var: 0.23436591029167175\n",
      "          vf_loss: 223.34730529785156\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.113513513513512\n",
      "    ram_util_percent: 11.600000000000003\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0801438035736974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08317796131058935\n",
      "    mean_inference_ms: 1.1520383664547724\n",
      "    mean_raw_obs_processing_ms: 0.11055292898037589\n",
      "  time_since_restore: 1005.1589820384979\n",
      "  time_this_iter_s: 25.988213300704956\n",
      "  time_total_s: 1005.1589820384979\n",
      "  timers:\n",
      "    learn_throughput: 1149.17\n",
      "    learn_time_ms: 3480.773\n",
      "    load_throughput: 9050175.855\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 260.747\n",
      "    sample_time_ms: 15340.545\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1641214810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:15 (running for 00:16:58.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1005.16</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  498.16</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 498.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.45\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 864\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.664535364979494e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30517786741256714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003519083373248577\n",
      "          model: {}\n",
      "          policy_loss: -0.003675314364954829\n",
      "          total_loss: 180.7675018310547\n",
      "          vf_explained_var: 0.4545365571975708\n",
      "          vf_loss: 180.77117919921875\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.45\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08016040714287424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08320022588534645\n",
      "    mean_inference_ms: 1.152233572022073\n",
      "    mean_raw_obs_processing_ms: 0.11056228542274027\n",
      "  time_since_restore: 1012.0019028186798\n",
      "  time_this_iter_s: 6.842920780181885\n",
      "  time_total_s: 1012.0019028186798\n",
      "  timers:\n",
      "    learn_throughput: 1137.101\n",
      "    learn_time_ms: 3517.718\n",
      "    load_throughput: 9057504.724\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 257.103\n",
      "    sample_time_ms: 15557.993\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1641214817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:21 (running for 00:17:04.71)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:26 (running for 00:17:09.72)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:31 (running for 00:17:14.73)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:36 (running for 00:17:19.74)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:41 (running for 00:17:24.74)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 499.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.45\n",
      "  episode_reward_min: 473.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 872\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10020614010090115\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10332594148566636\n",
      "      mean_inference_ms: 1.4135846093682924\n",
      "      mean_raw_obs_processing_ms: 0.12253645668128085\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.332267682489747e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30894914269447327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013987822458148003\n",
      "          model: {}\n",
      "          policy_loss: -0.00022974552121013403\n",
      "          total_loss: 284.0566101074219\n",
      "          vf_explained_var: 0.3465968370437622\n",
      "          vf_loss: 284.0568542480469\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.005882352941176\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08017626258577297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0832225227046527\n",
      "    mean_inference_ms: 1.152411373739642\n",
      "    mean_raw_obs_processing_ms: 0.11057217404297028\n",
      "  time_since_restore: 1036.4369163513184\n",
      "  time_this_iter_s: 24.43501353263855\n",
      "  time_total_s: 1036.4369163513184\n",
      "  timers:\n",
      "    learn_throughput: 1124.16\n",
      "    learn_time_ms: 3558.213\n",
      "    load_throughput: 9403741.943\n",
      "    load_time_ms: 0.425\n",
      "    sample_throughput: 256.527\n",
      "    sample_time_ms: 15592.919\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1641214842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:47 (running for 00:17:30.19)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1036.44</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  499.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">            499.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 499.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.5\n",
      "  episode_reward_min: 473.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 880\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.661338412448735e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33786264061927795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016626294236630201\n",
      "          model: {}\n",
      "          policy_loss: -0.0017250650562345982\n",
      "          total_loss: 257.88360595703125\n",
      "          vf_explained_var: 0.2730449438095093\n",
      "          vf_loss: 257.88531494140625\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.55\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08019006074290559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08324356824201036\n",
      "    mean_inference_ms: 1.1525640234997365\n",
      "    mean_raw_obs_processing_ms: 0.11057836872730675\n",
      "  time_since_restore: 1043.0214354991913\n",
      "  time_this_iter_s: 6.584519147872925\n",
      "  time_total_s: 1043.0214354991913\n",
      "  timers:\n",
      "    learn_throughput: 1124.365\n",
      "    learn_time_ms: 3557.562\n",
      "    load_throughput: 9055549.198\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 255.224\n",
      "    sample_time_ms: 15672.527\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1641214848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:52 (running for 00:17:35.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:57 (running for 00:17:40.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:02 (running for 00:17:45.82)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:07 (running for 00:17:50.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:12 (running for 00:17:55.84)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 888\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.55\n",
      "    episode_reward_min: 451.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 451\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 451.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10027534232670468\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10340226980917995\n",
      "      mean_inference_ms: 1.4145307414740955\n",
      "      mean_raw_obs_processing_ms: 0.12264937259858495\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3306692062243676e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3179149329662323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0014973991783335805\n",
      "          model: {}\n",
      "          policy_loss: 0.0008643927285447717\n",
      "          total_loss: 356.0588684082031\n",
      "          vf_explained_var: 0.19004207849502563\n",
      "          vf_loss: 356.0579833984375\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.732352941176472\n",
      "    ram_util_percent: 11.705882352941176\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08019768375382848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0832582054780674\n",
      "    mean_inference_ms: 1.152641538502277\n",
      "    mean_raw_obs_processing_ms: 0.11057740979924692\n",
      "  time_since_restore: 1067.282835483551\n",
      "  time_this_iter_s: 24.26139998435974\n",
      "  time_total_s: 1067.282835483551\n",
      "  timers:\n",
      "    learn_throughput: 1125.662\n",
      "    learn_time_ms: 3553.464\n",
      "    load_throughput: 8689256.267\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 255.676\n",
      "    sample_time_ms: 15644.797\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1641214873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:18 (running for 00:18:01.12)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1067.28</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 896\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6653346031121838e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3299762010574341\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003860413795337081\n",
      "          model: {}\n",
      "          policy_loss: -0.0030521831940859556\n",
      "          total_loss: 199.1850128173828\n",
      "          vf_explained_var: 0.3740776479244232\n",
      "          vf_loss: 199.1880645751953\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.090000000000003\n",
      "    ram_util_percent: 11.76\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08021284110887222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08328197262888487\n",
      "    mean_inference_ms: 1.1528163419678576\n",
      "    mean_raw_obs_processing_ms: 0.11058702929412653\n",
      "  time_since_restore: 1074.238920211792\n",
      "  time_this_iter_s: 6.956084728240967\n",
      "  time_total_s: 1074.238920211792\n",
      "  timers:\n",
      "    learn_throughput: 1117.416\n",
      "    learn_time_ms: 3579.689\n",
      "    load_throughput: 8080342.918\n",
      "    load_time_ms: 0.495\n",
      "    sample_throughput: 254.964\n",
      "    sample_time_ms: 15688.517\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1641214880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:24 (running for 00:18:07.10)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:29 (running for 00:18:12.11)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:34 (running for 00:18:17.12)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:39 (running for 00:18:22.13)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:44 (running for 00:18:27.14)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-46\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 904\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10062712371422684\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10376565666472744\n",
      "      mean_inference_ms: 1.4189139694667139\n",
      "      mean_raw_obs_processing_ms: 0.12303206260355178\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.326673015560919e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30264073610305786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006416999269276857\n",
      "          model: {}\n",
      "          policy_loss: -0.004021155647933483\n",
      "          total_loss: 235.52613830566406\n",
      "          vf_explained_var: 0.4266692101955414\n",
      "          vf_loss: 235.5301513671875\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.557894736842103\n",
      "    ram_util_percent: 11.76578947368421\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08023011628365052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08330935179634663\n",
      "    mean_inference_ms: 1.1530076809114365\n",
      "    mean_raw_obs_processing_ms: 0.11059816253902949\n",
      "  time_since_restore: 1100.5570487976074\n",
      "  time_this_iter_s: 26.31812858581543\n",
      "  time_total_s: 1100.5570487976074\n",
      "  timers:\n",
      "    learn_throughput: 1110.958\n",
      "    learn_time_ms: 3600.495\n",
      "    load_throughput: 7632946.315\n",
      "    load_time_ms: 0.524\n",
      "    sample_throughput: 254.447\n",
      "    sample_time_ms: 15720.355\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1641214906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:49 (running for 00:18:32.47)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1100.56</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 912\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.326673015560919e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30977368354797363\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003388606710359454\n",
      "          model: {}\n",
      "          policy_loss: -0.001255176728591323\n",
      "          total_loss: 306.4930725097656\n",
      "          vf_explained_var: 0.17501209676265717\n",
      "          vf_loss: 306.4943542480469\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.1\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024956236733362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08334016864980617\n",
      "    mean_inference_ms: 1.1532353017663581\n",
      "    mean_raw_obs_processing_ms: 0.11061414418288137\n",
      "  time_since_restore: 1107.3738012313843\n",
      "  time_this_iter_s: 6.8167524337768555\n",
      "  time_total_s: 1107.3738012313843\n",
      "  timers:\n",
      "    learn_throughput: 1106.783\n",
      "    learn_time_ms: 3614.078\n",
      "    load_throughput: 7641637.896\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 250.737\n",
      "    sample_time_ms: 15953.001\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1641214913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:55 (running for 00:18:38.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:00 (running for 00:18:43.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:05 (running for 00:18:48.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:10 (running for 00:18:53.35)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:15 (running for 00:18:58.36)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 499.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.08\n",
      "  episode_reward_min: 456.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 921\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.8\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.8\n",
      "    episode_reward_min: 479.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 497\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 480\n",
      "      - 479\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 497.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 480.0\n",
      "      - 479.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10069513217696986\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10384755988703583\n",
      "      mean_inference_ms: 1.4196638936439678\n",
      "      mean_raw_obs_processing_ms: 0.12307972347654825\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.1633365077804595e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28842437267303467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0022505007218569517\n",
      "          model: {}\n",
      "          policy_loss: -0.0027017544489353895\n",
      "          total_loss: 155.21408081054688\n",
      "          vf_explained_var: 0.5348164439201355\n",
      "          vf_loss: 155.21676635742188\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.837142857142858\n",
      "    ram_util_percent: 11.748571428571429\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026674625280418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08336479144224679\n",
      "    mean_inference_ms: 1.1533740748844306\n",
      "    mean_raw_obs_processing_ms: 0.11062193273009764\n",
      "  time_since_restore: 1131.9896500110626\n",
      "  time_this_iter_s: 24.615848779678345\n",
      "  time_total_s: 1131.9896500110626\n",
      "  timers:\n",
      "    learn_throughput: 1096.307\n",
      "    learn_time_ms: 3648.613\n",
      "    load_throughput: 7651744.96\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 250.404\n",
      "    sample_time_ms: 15974.163\n",
      "    update_time_ms: 2.612\n",
      "  timestamp: 1641214937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:20 (running for 00:19:03.98)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1131.99</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-24\n",
      "  done: false\n",
      "  episode_len_mean: 499.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.08\n",
      "  episode_reward_min: 456.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 929\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0816682538902298e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2569127678871155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003834979608654976\n",
      "          model: {}\n",
      "          policy_loss: -0.0022537545301020145\n",
      "          total_loss: 184.89852905273438\n",
      "          vf_explained_var: 0.5317127108573914\n",
      "          vf_loss: 184.90077209472656\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.722222222222221\n",
      "    ram_util_percent: 11.755555555555555\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026415978511454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08337157980322107\n",
      "    mean_inference_ms: 1.1533092912504341\n",
      "    mean_raw_obs_processing_ms: 0.11061006060274411\n",
      "  time_since_restore: 1138.2685542106628\n",
      "  time_this_iter_s: 6.27890419960022\n",
      "  time_total_s: 1138.2685542106628\n",
      "  timers:\n",
      "    learn_throughput: 1100.653\n",
      "    learn_time_ms: 3634.205\n",
      "    load_throughput: 8235429.02\n",
      "    load_time_ms: 0.486\n",
      "    sample_throughput: 250.57\n",
      "    sample_time_ms: 15963.617\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1641214944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:26 (running for 00:19:09.30)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:31 (running for 00:19:14.31)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:36 (running for 00:19:19.31)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:41 (running for 00:19:24.32)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:46 (running for 00:19:29.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 494.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.06\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 938\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 484.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 484.45\n",
      "    episode_reward_min: 407.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 477\n",
      "      - 491\n",
      "      - 500\n",
      "      - 500\n",
      "      - 468\n",
      "      - 440\n",
      "      - 500\n",
      "      - 500\n",
      "      - 421\n",
      "      - 500\n",
      "      - 500\n",
      "      - 407\n",
      "      - 485\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 477.0\n",
      "      - 491.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 468.0\n",
      "      - 440.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 421.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 407.0\n",
      "      - 485.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10077220689618134\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10394825696670647\n",
      "      mean_inference_ms: 1.4209386167556604\n",
      "      mean_raw_obs_processing_ms: 0.12317528789592354\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0408341269451149e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.25869643688201904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0027097088750451803\n",
      "          model: {}\n",
      "          policy_loss: -0.006009688600897789\n",
      "          total_loss: 96.60514068603516\n",
      "          vf_explained_var: 0.6104920506477356\n",
      "          vf_loss: 96.61115264892578\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.564705882352941\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026830720500518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08338698422603116\n",
      "    mean_inference_ms: 1.153344187201906\n",
      "    mean_raw_obs_processing_ms: 0.11060474001492959\n",
      "  time_since_restore: 1162.4961895942688\n",
      "  time_this_iter_s: 24.227635383605957\n",
      "  time_total_s: 1162.4961895942688\n",
      "  timers:\n",
      "    learn_throughput: 1101.606\n",
      "    learn_time_ms: 3631.063\n",
      "    load_throughput: 8183608.604\n",
      "    load_time_ms: 0.489\n",
      "    sample_throughput: 251.184\n",
      "    sample_time_ms: 15924.578\n",
      "    update_time_ms: 2.506\n",
      "  timestamp: 1641214968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:51 (running for 00:19:34.58)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1162.5</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  494.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            494.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 492.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.45\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 946\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.204170634725574e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2483358383178711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003815557574853301\n",
      "          model: {}\n",
      "          policy_loss: -0.00015161468763835728\n",
      "          total_loss: 176.90817260742188\n",
      "          vf_explained_var: 0.5207358002662659\n",
      "          vf_loss: 176.90834045410156\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.01\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026873338852676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08339684334243128\n",
      "    mean_inference_ms: 1.1533358543620529\n",
      "    mean_raw_obs_processing_ms: 0.11059580191219995\n",
      "  time_since_restore: 1169.2003734111786\n",
      "  time_this_iter_s: 6.70418381690979\n",
      "  time_total_s: 1169.2003734111786\n",
      "  timers:\n",
      "    learn_throughput: 1109.076\n",
      "    learn_time_ms: 3606.606\n",
      "    load_throughput: 8209637.894\n",
      "    load_time_ms: 0.487\n",
      "    sample_throughput: 253.399\n",
      "    sample_time_ms: 15785.377\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1641214975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:57 (running for 00:19:40.32)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:02 (running for 00:19:45.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:07 (running for 00:19:50.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:12 (running for 00:19:55.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:17 (running for 00:20:00.35)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-20\n",
      "  done: false\n",
      "  episode_len_mean: 492.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.45\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 954\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.55\n",
      "    episode_reward_min: 451.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 451\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 451.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10091182007568471\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1040961749408513\n",
      "      mean_inference_ms: 1.4227154702618872\n",
      "      mean_raw_obs_processing_ms: 0.12325830412241379\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.602085317362787e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2623783051967621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008481194381602108\n",
      "          model: {}\n",
      "          policy_loss: 0.0006717838114127517\n",
      "          total_loss: 256.2461242675781\n",
      "          vf_explained_var: 0.3934962749481201\n",
      "          vf_loss: 256.2454528808594\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.527777777777777\n",
      "    ram_util_percent: 11.699999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026127029794097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08339792577048381\n",
      "    mean_inference_ms: 1.1532257950723988\n",
      "    mean_raw_obs_processing_ms: 0.1105779512975858\n",
      "  time_since_restore: 1194.269511938095\n",
      "  time_this_iter_s: 25.069138526916504\n",
      "  time_total_s: 1194.269511938095\n",
      "  timers:\n",
      "    learn_throughput: 1114.3\n",
      "    learn_time_ms: 3589.697\n",
      "    load_throughput: 8339820.053\n",
      "    load_time_ms: 0.48\n",
      "    sample_throughput: 253.713\n",
      "    sample_time_ms: 15765.855\n",
      "    update_time_ms: 2.507\n",
      "  timestamp: 1641215000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:22 (running for 00:20:05.44)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1194.27</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 962\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3010426586813936e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.26068851351737976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004601421300321817\n",
      "          model: {}\n",
      "          policy_loss: -0.00252857175655663\n",
      "          total_loss: 327.0229797363281\n",
      "          vf_explained_var: 0.399458646774292\n",
      "          vf_loss: 327.0255126953125\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.544444444444444\n",
      "    ram_util_percent: 11.822222222222223\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025798667890836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340253170878308\n",
      "    mean_inference_ms: 1.1531596377499853\n",
      "    mean_raw_obs_processing_ms: 0.11056490037888685\n",
      "  time_since_restore: 1200.9417476654053\n",
      "  time_this_iter_s: 6.672235727310181\n",
      "  time_total_s: 1200.9417476654053\n",
      "  timers:\n",
      "    learn_throughput: 1113.89\n",
      "    learn_time_ms: 3591.02\n",
      "    load_throughput: 8393644.187\n",
      "    load_time_ms: 0.477\n",
      "    sample_throughput: 252.657\n",
      "    sample_time_ms: 15831.709\n",
      "    update_time_ms: 2.554\n",
      "  timestamp: 1641215007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:28 (running for 00:20:11.15)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:33 (running for 00:20:16.16)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:38 (running for 00:20:21.17)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:43 (running for 00:20:26.17)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:48 (running for 00:20:31.18)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 970\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.75\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.75\n",
      "    episode_reward_min: 455.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 455\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 455.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10093963057855689\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10412633695622056\n",
      "      mean_inference_ms: 1.423294915576578\n",
      "      mean_raw_obs_processing_ms: 0.12327615221576915\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.505213293406968e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2688307464122772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021669436246156693\n",
      "          model: {}\n",
      "          policy_loss: 0.0009603818180039525\n",
      "          total_loss: 484.1625061035156\n",
      "          vf_explained_var: -0.077602818608284\n",
      "          vf_loss: 484.1615295410156\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.737142857142857\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025302047742677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340485100631874\n",
      "    mean_inference_ms: 1.1530862613238626\n",
      "    mean_raw_obs_processing_ms: 0.11054963019123512\n",
      "  time_since_restore: 1225.1908042430878\n",
      "  time_this_iter_s: 24.249056577682495\n",
      "  time_total_s: 1225.1908042430878\n",
      "  timers:\n",
      "    learn_throughput: 1113.912\n",
      "    learn_time_ms: 3590.948\n",
      "    load_throughput: 8748613.443\n",
      "    load_time_ms: 0.457\n",
      "    sample_throughput: 252.583\n",
      "    sample_time_ms: 15836.407\n",
      "    update_time_ms: 2.579\n",
      "  timestamp: 1641215031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:53 (running for 00:20:36.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1225.19</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 978\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.252606646703484e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2761077880859375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002532893093302846\n",
      "          model: {}\n",
      "          policy_loss: -0.0015050852671265602\n",
      "          total_loss: 263.3819885253906\n",
      "          vf_explained_var: 0.48334449529647827\n",
      "          vf_loss: 263.3835144042969\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.860000000000003\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025120618213681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08341052334973513\n",
      "    mean_inference_ms: 1.153057153688451\n",
      "    mean_raw_obs_processing_ms: 0.11053798827725313\n",
      "  time_since_restore: 1231.981814622879\n",
      "  time_this_iter_s: 6.79101037979126\n",
      "  time_total_s: 1231.981814622879\n",
      "  timers:\n",
      "    learn_throughput: 1119.859\n",
      "    learn_time_ms: 3571.88\n",
      "    load_throughput: 8687456.504\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 252.595\n",
      "    sample_time_ms: 15835.609\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1641215038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:59 (running for 00:20:42.28)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:04 (running for 00:20:47.29)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:09 (running for 00:20:52.30)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:14 (running for 00:20:57.31)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2,    # num of training iter between evaluations\n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 \"num_gpus\": 2\n",
    "                 }\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d547c2d-aa55-48f4-b586-f183b763a301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
