{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7804cb7-8f9a-455e-a513-74f0043672a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Solving RL problems with `ray[rllib]`\n",
    "\n",
    "<img src=\"images/cartpole.jpg\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210f6c2-4645-4ab7-a45f-f8f6a667e667",
   "metadata": {},
   "source": [
    "## Step 1: Initialize `ray`\n",
    "\n",
    "- `ray` is a package providing distributed computing primitives. `rllib` is built on `ray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a8302b-1ae1-42c0-9852-ece04bc5a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_16-35-03_047201_3487',\n",
       " 'metrics_export_port': 65412,\n",
       " 'gcs_address': '192.168.0.98:63541',\n",
       " 'address': '192.168.0.98:63541',\n",
       " 'node_id': 'c654f33cd9d7ab46008a4f0889879874b4cab53d76b0e12fecbd93fe'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb463a9-bef9-4af1-92db-976332c42797",
   "metadata": {},
   "source": [
    "## Step 2: Run an **experiment** to solve RL problems\n",
    "\n",
    "An experiment involves four things\n",
    "- A **RL environment** (e.g. `CartPole-v1`)\n",
    "- A **RL algorithm** to learn in that environment (e.g. Proximal Policy Optimization (PPO))\n",
    "- **Configuration** (algorithm config, experiment config, environment config etc.)\n",
    "- An **experiment runner** (called `tune`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a94021-5497-47fa-bbdf-116ef940b1b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,890\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,890\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:35:56,891\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:00 (running for 00:00:05.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:36:00,119\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:01 (running for 00:00:06.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:36:01,474\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 22.666666666666668\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 90.0\n",
      "  episode_reward_mean: 22.666666666666668\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6677238941192627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0263458751142025\n",
      "          model: {}\n",
      "          policy_loss: -0.031717997044324875\n",
      "          total_loss: 222.7870330810547\n",
      "          vf_explained_var: 0.019372833892703056\n",
      "          vf_loss: 222.8134765625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.24\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044701106750165386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04411039122499129\n",
      "    mean_inference_ms: 0.4843887808395671\n",
      "    mean_raw_obs_processing_ms: 0.07169593727942612\n",
      "  time_since_restore: 3.324431896209717\n",
      "  time_this_iter_s: 3.324431896209717\n",
      "  time_total_s: 3.324431896209717\n",
      "  timers:\n",
      "    learn_throughput: 2043.538\n",
      "    learn_time_ms: 1957.39\n",
      "    load_throughput: 34592197.938\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 2951.819\n",
      "    sample_time_ms: 1355.096\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1671813363\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:06 (running for 00:00:11.55)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32443</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.6667</td><td style=\"text-align: right;\">                  90</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-09\n",
      "  done: false\n",
      "  episode_len_mean: 67.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 281.0\n",
      "  episode_reward_mean: 67.2\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 299\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5797845721244812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010812313295900822\n",
      "          model: {}\n",
      "          policy_loss: -0.02183121256530285\n",
      "          total_loss: 757.8624267578125\n",
      "          vf_explained_var: 0.13413472473621368\n",
      "          vf_loss: 757.8809814453125\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.625\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043607924768946635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043218460326377856\n",
      "    mean_inference_ms: 0.47017684802007476\n",
      "    mean_raw_obs_processing_ms: 0.06619004810555301\n",
      "  time_since_restore: 9.39606261253357\n",
      "  time_this_iter_s: 3.0226731300354004\n",
      "  time_total_s: 9.39606261253357\n",
      "  timers:\n",
      "    learn_throughput: 2173.621\n",
      "    learn_time_ms: 1840.247\n",
      "    load_throughput: 28630061.433\n",
      "    load_time_ms: 0.14\n",
      "    sample_throughput: 1568.095\n",
      "    sample_time_ms: 2550.866\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1671813369\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:11 (running for 00:00:16.63)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         9.39606</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">    67.2</td><td style=\"text-align: right;\">                 281</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">              67.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-15\n",
      "  done: false\n",
      "  episode_len_mean: 128.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 128.16\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 334\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5438970327377319\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005624917335808277\n",
      "          model: {}\n",
      "          policy_loss: -0.010234670713543892\n",
      "          total_loss: 693.485107421875\n",
      "          vf_explained_var: 0.2024756669998169\n",
      "          vf_loss: 693.4935302734375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.44\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043546603817910236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043183061039808075\n",
      "    mean_inference_ms: 0.46978869255169853\n",
      "    mean_raw_obs_processing_ms: 0.06486679277025255\n",
      "  time_since_restore: 15.547481060028076\n",
      "  time_this_iter_s: 3.056800127029419\n",
      "  time_total_s: 15.547481060028076\n",
      "  timers:\n",
      "    learn_throughput: 2201.832\n",
      "    learn_time_ms: 1816.669\n",
      "    load_throughput: 27749282.17\n",
      "    load_time_ms: 0.144\n",
      "    sample_throughput: 1446.25\n",
      "    sample_time_ms: 2765.774\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1671813375\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:16 (running for 00:00:21.81)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         15.5475</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  128.16</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            128.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:21 (running for 00:00:26.83)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         18.5533</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  161.53</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            161.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 200.43\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 356\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5294512510299683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00397925078868866\n",
      "          model: {}\n",
      "          policy_loss: -0.008955384604632854\n",
      "          total_loss: 438.78582763671875\n",
      "          vf_explained_var: 0.16676998138427734\n",
      "          vf_loss: 438.7941589355469\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.400000000000006\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04360390001764159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04309342188133744\n",
      "    mean_inference_ms: 0.46929274853797476\n",
      "    mean_raw_obs_processing_ms: 0.06368122464050409\n",
      "  time_since_restore: 21.59273600578308\n",
      "  time_this_iter_s: 3.039470672607422\n",
      "  time_total_s: 21.59273600578308\n",
      "  timers:\n",
      "    learn_throughput: 2215.406\n",
      "    learn_time_ms: 1805.538\n",
      "    load_throughput: 29701697.521\n",
      "    load_time_ms: 0.135\n",
      "    sample_throughput: 1406.361\n",
      "    sample_time_ms: 2844.219\n",
      "    update_time_ms: 1.362\n",
      "  timestamp: 1671813381\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:26 (running for 00:00:31.95)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         24.6207</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  230.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            230.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 265.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 265.0\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 374\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.487382709980011\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002882240107282996\n",
      "          model: {}\n",
      "          policy_loss: -0.0043022544123232365\n",
      "          total_loss: 504.5002136230469\n",
      "          vf_explained_var: 0.026964129880070686\n",
      "          vf_loss: 504.5043029785156\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.599999999999994\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043561476193733045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04308956025824391\n",
      "    mean_inference_ms: 0.4692430788083837\n",
      "    mean_raw_obs_processing_ms: 0.06286407142836352\n",
      "  time_since_restore: 27.702523708343506\n",
      "  time_this_iter_s: 3.081804037094116\n",
      "  time_total_s: 27.702523708343506\n",
      "  timers:\n",
      "    learn_throughput: 2219.465\n",
      "    learn_time_ms: 1802.236\n",
      "    load_throughput: 30790159.869\n",
      "    load_time_ms: 0.13\n",
      "    sample_throughput: 1382.548\n",
      "    sample_time_ms: 2893.208\n",
      "    update_time_ms: 1.361\n",
      "  timestamp: 1671813387\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:32 (running for 00:00:37.09)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         30.7445</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  290.94</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            290.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 324.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.9\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 391\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5414067506790161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004195119719952345\n",
      "          model: {}\n",
      "          policy_loss: -0.0025419823359698057\n",
      "          total_loss: 468.4158935546875\n",
      "          vf_explained_var: 0.08807938545942307\n",
      "          vf_loss: 468.4183349609375\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.474999999999994\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043599303534539065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043090271662098056\n",
      "    mean_inference_ms: 0.46930503030592746\n",
      "    mean_raw_obs_processing_ms: 0.06215156518863388\n",
      "  time_since_restore: 33.77014899253845\n",
      "  time_this_iter_s: 3.025679111480713\n",
      "  time_total_s: 33.77014899253845\n",
      "  timers:\n",
      "    learn_throughput: 2243.256\n",
      "    learn_time_ms: 1783.123\n",
      "    load_throughput: 30454194.954\n",
      "    load_time_ms: 0.131\n",
      "    sample_throughput: 1298.889\n",
      "    sample_time_ms: 3079.555\n",
      "    update_time_ms: 1.356\n",
      "  timestamp: 1671813394\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:37 (running for 00:00:42.16)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         33.7701</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   324.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             324.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 377.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 377.26\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 407\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5010088682174683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006605195347219706\n",
      "          model: {}\n",
      "          policy_loss: -0.00037172867450863123\n",
      "          total_loss: 484.509765625\n",
      "          vf_explained_var: 0.030472297221422195\n",
      "          vf_loss: 484.51007080078125\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.980000000000004\n",
      "    ram_util_percent: 9.0\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043582897374795364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043067216215753296\n",
      "    mean_inference_ms: 0.4689052928266846\n",
      "    mean_raw_obs_processing_ms: 0.06146178704252219\n",
      "  time_since_restore: 39.861185789108276\n",
      "  time_this_iter_s: 3.041245698928833\n",
      "  time_total_s: 39.861185789108276\n",
      "  timers:\n",
      "    learn_throughput: 2241.188\n",
      "    learn_time_ms: 1784.767\n",
      "    load_throughput: 32388447.876\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1306.863\n",
      "    sample_time_ms: 3060.764\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671813400\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:42 (running for 00:00:47.26)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         39.8612</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  377.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            377.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-46\n",
      "  done: false\n",
      "  episode_len_mean: 426.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 426.26\n",
      "  episode_reward_min: 145.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 423\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5119425654411316\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002727864310145378\n",
      "          model: {}\n",
      "          policy_loss: -0.0022586756385862827\n",
      "          total_loss: 531.2691650390625\n",
      "          vf_explained_var: -0.018852492794394493\n",
      "          vf_loss: 531.2714233398438\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.425\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04356888150762668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04300869335019948\n",
      "    mean_inference_ms: 0.4681950021281348\n",
      "    mean_raw_obs_processing_ms: 0.06084517978414137\n",
      "  time_since_restore: 45.945841789245605\n",
      "  time_this_iter_s: 3.0422747135162354\n",
      "  time_total_s: 45.945841789245605\n",
      "  timers:\n",
      "    learn_throughput: 2239.461\n",
      "    learn_time_ms: 1786.144\n",
      "    load_throughput: 32338504.241\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1309.229\n",
      "    sample_time_ms: 3055.234\n",
      "    update_time_ms: 1.369\n",
      "  timestamp: 1671813406\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:47 (running for 00:00:52.37)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         45.9458</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  426.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 145</td><td style=\"text-align: right;\">            426.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:52 (running for 00:00:57.44)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         49.0046</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  444.84</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 152</td><td style=\"text-align: right;\">            444.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 458.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 458.9\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 440\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.45621252059936523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0029501868411898613\n",
      "          model: {}\n",
      "          policy_loss: -0.0014081724220886827\n",
      "          total_loss: 563.0380249023438\n",
      "          vf_explained_var: -0.06301168352365494\n",
      "          vf_loss: 563.0394287109375\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.025\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04351516382494466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04296107168788171\n",
      "    mean_inference_ms: 0.4675370247388944\n",
      "    mean_raw_obs_processing_ms: 0.06028324745373462\n",
      "  time_since_restore: 52.02103805541992\n",
      "  time_this_iter_s: 3.016430377960205\n",
      "  time_total_s: 52.02103805541992\n",
      "  timers:\n",
      "    learn_throughput: 2238.559\n",
      "    learn_time_ms: 1786.864\n",
      "    load_throughput: 32232883.766\n",
      "    load_time_ms: 0.124\n",
      "    sample_throughput: 1307.784\n",
      "    sample_time_ms: 3058.608\n",
      "    update_time_ms: 1.353\n",
      "  timestamp: 1671813412\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:36:57 (running for 00:01:02.54)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         55.0431</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  472.96</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            472.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-36-58\n",
      "  done: false\n",
      "  episode_len_mean: 478.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 478.36\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 456\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4239750802516937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005886388476938009\n",
      "          model: {}\n",
      "          policy_loss: -0.0022193826735019684\n",
      "          total_loss: 529.1740112304688\n",
      "          vf_explained_var: -0.021180717274546623\n",
      "          vf_loss: 529.1762084960938\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.839999999999996\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04353004167542327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04292108372584675\n",
      "    mean_inference_ms: 0.46713100252918416\n",
      "    mean_raw_obs_processing_ms: 0.059906187369080065\n",
      "  time_since_restore: 58.090163469314575\n",
      "  time_this_iter_s: 3.0470263957977295\n",
      "  time_total_s: 58.090163469314575\n",
      "  timers:\n",
      "    learn_throughput: 2239.068\n",
      "    learn_time_ms: 1786.458\n",
      "    load_throughput: 31324152.353\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1309.644\n",
      "    sample_time_ms: 3054.265\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1671813418\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:02 (running for 00:01:07.61)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         61.1105</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  481.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            481.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-04\n",
      "  done: false\n",
      "  episode_len_mean: 488.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.19\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 472\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3980620205402374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037860707379877567\n",
      "          model: {}\n",
      "          policy_loss: -0.0029902078676968813\n",
      "          total_loss: 501.73126220703125\n",
      "          vf_explained_var: 0.03084975853562355\n",
      "          vf_loss: 501.73419189453125\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.25\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0434964826763005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04289641520668185\n",
      "    mean_inference_ms: 0.46675199210354307\n",
      "    mean_raw_obs_processing_ms: 0.05960073849467742\n",
      "  time_since_restore: 64.13966298103333\n",
      "  time_this_iter_s: 3.0291850566864014\n",
      "  time_total_s: 64.13966298103333\n",
      "  timers:\n",
      "    learn_throughput: 2239.589\n",
      "    learn_time_ms: 1786.042\n",
      "    load_throughput: 30251020.555\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1310.68\n",
      "    sample_time_ms: 3051.851\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1671813424\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:07 (running for 00:01:12.67)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         64.1397</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  488.19</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            488.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-10\n",
      "  done: false\n",
      "  episode_len_mean: 491.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 491.45\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 488\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.39177200198173523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006307192612439394\n",
      "          model: {}\n",
      "          policy_loss: -0.004075607750564814\n",
      "          total_loss: 513.6663818359375\n",
      "          vf_explained_var: 0.03715536370873451\n",
      "          vf_loss: 513.67041015625\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.85\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04348124121861787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0428735465670551\n",
      "    mean_inference_ms: 0.46646163840782523\n",
      "    mean_raw_obs_processing_ms: 0.05936134210496759\n",
      "  time_since_restore: 70.33358669281006\n",
      "  time_this_iter_s: 3.105516195297241\n",
      "  time_total_s: 70.33358669281006\n",
      "  timers:\n",
      "    learn_throughput: 2230.41\n",
      "    learn_time_ms: 1793.392\n",
      "    load_throughput: 30289250.767\n",
      "    load_time_ms: 0.132\n",
      "    sample_throughput: 1309.266\n",
      "    sample_time_ms: 3055.146\n",
      "    update_time_ms: 1.323\n",
      "  timestamp: 1671813430\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:12 (running for 00:01:17.87)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         70.3336</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  491.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            491.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 504\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38255375623703003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004798873793333769\n",
      "          model: {}\n",
      "          policy_loss: -0.002857438288629055\n",
      "          total_loss: 499.1186828613281\n",
      "          vf_explained_var: -0.08942034095525742\n",
      "          vf_loss: 499.1215515136719\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.275000000000006\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043456026695483006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04284503143551925\n",
      "    mean_inference_ms: 0.46611646032293147\n",
      "    mean_raw_obs_processing_ms: 0.05915616744860538\n",
      "  time_since_restore: 76.38708782196045\n",
      "  time_this_iter_s: 3.034912347793579\n",
      "  time_total_s: 76.38708782196045\n",
      "  timers:\n",
      "    learn_throughput: 2230.214\n",
      "    learn_time_ms: 1793.55\n",
      "    load_throughput: 30942855.035\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1307.879\n",
      "    sample_time_ms: 3058.386\n",
      "    update_time_ms: 1.345\n",
      "  timestamp: 1671813436\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:17 (running for 00:01:22.96)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         76.3871</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:22 (running for 00:01:28.02)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         79.4227</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 520\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3938816487789154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0015072141541168094\n",
      "          model: {}\n",
      "          policy_loss: 0.00038365216460078955\n",
      "          total_loss: 539.7266235351562\n",
      "          vf_explained_var: -0.0835423469543457\n",
      "          vf_loss: 539.7262573242188\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.0\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04342695082088102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04281428676654714\n",
      "    mean_inference_ms: 0.4657748444827307\n",
      "    mean_raw_obs_processing_ms: 0.058980607314950396\n",
      "  time_since_restore: 82.4312036037445\n",
      "  time_this_iter_s: 3.008547782897949\n",
      "  time_total_s: 82.4312036037445\n",
      "  timers:\n",
      "    learn_throughput: 2229.837\n",
      "    learn_time_ms: 1793.853\n",
      "    load_throughput: 31040177.613\n",
      "    load_time_ms: 0.129\n",
      "    sample_throughput: 1308.779\n",
      "    sample_time_ms: 3056.283\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1671813442\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:28 (running for 00:01:33.14)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         85.4967</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  495.85</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">            495.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 495.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.85\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 536\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38424333930015564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006487591657787561\n",
      "          model: {}\n",
      "          policy_loss: -0.0018671861616894603\n",
      "          total_loss: 507.647216796875\n",
      "          vf_explained_var: -0.022645175457000732\n",
      "          vf_loss: 507.6490478515625\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.475\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04340134687576476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04278862797043205\n",
      "    mean_inference_ms: 0.465469203657461\n",
      "    mean_raw_obs_processing_ms: 0.05884142547052737\n",
      "  time_since_restore: 88.5537428855896\n",
      "  time_this_iter_s: 3.0570731163024902\n",
      "  time_total_s: 88.5537428855896\n",
      "  timers:\n",
      "    learn_throughput: 2230.31\n",
      "    learn_time_ms: 1793.473\n",
      "    load_throughput: 31974873.261\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1305.674\n",
      "    sample_time_ms: 3063.551\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671813449\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:33 (running for 00:01:38.24)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         91.5883</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 552\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4100276827812195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002599272644147277\n",
      "          model: {}\n",
      "          policy_loss: -0.0005167347262613475\n",
      "          total_loss: 534.5759887695312\n",
      "          vf_explained_var: 0.02402246743440628\n",
      "          vf_loss: 534.5764770507812\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.225\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04339062856707493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04277819869065562\n",
      "    mean_inference_ms: 0.4653591828523603\n",
      "    mean_raw_obs_processing_ms: 0.05873912744601162\n",
      "  time_since_restore: 94.697509765625\n",
      "  time_this_iter_s: 3.109220027923584\n",
      "  time_total_s: 94.697509765625\n",
      "  timers:\n",
      "    learn_throughput: 2229.484\n",
      "    learn_time_ms: 1794.137\n",
      "    load_throughput: 34204313.965\n",
      "    load_time_ms: 0.117\n",
      "    sample_throughput: 1302.439\n",
      "    sample_time_ms: 3071.162\n",
      "    update_time_ms: 1.328\n",
      "  timestamp: 1671813455\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:38 (running for 00:01:43.40)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         94.6975</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 568\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4069482982158661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005019642412662506\n",
      "          model: {}\n",
      "          policy_loss: 8.927865565055981e-05\n",
      "          total_loss: 497.6102600097656\n",
      "          vf_explained_var: -0.0994281992316246\n",
      "          vf_loss: 497.6101379394531\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.05\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338835206418242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04277502355118303\n",
      "    mean_inference_ms: 0.4653988122359949\n",
      "    mean_raw_obs_processing_ms: 0.058662392389679886\n",
      "  time_since_restore: 100.76534819602966\n",
      "  time_this_iter_s: 3.012232542037964\n",
      "  time_total_s: 100.76534819602966\n",
      "  timers:\n",
      "    learn_throughput: 2238.176\n",
      "    learn_time_ms: 1787.169\n",
      "    load_throughput: 31980968.357\n",
      "    load_time_ms: 0.125\n",
      "    sample_throughput: 1304.348\n",
      "    sample_time_ms: 3066.667\n",
      "    update_time_ms: 1.315\n",
      "  timestamp: 1671813461\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:43 (running for 00:01:48.47)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         100.765</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 584\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3817431926727295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031131699215620756\n",
      "          model: {}\n",
      "          policy_loss: -0.0003612424770835787\n",
      "          total_loss: 545.5170288085938\n",
      "          vf_explained_var: -0.13232421875\n",
      "          vf_loss: 545.5173950195312\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.279999999999994\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338027509435271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042765930230909646\n",
      "    mean_inference_ms: 0.46533296087520787\n",
      "    mean_raw_obs_processing_ms: 0.058584593140984464\n",
      "  time_since_restore: 106.84234118461609\n",
      "  time_this_iter_s: 3.0423429012298584\n",
      "  time_total_s: 106.84234118461609\n",
      "  timers:\n",
      "    learn_throughput: 2237.05\n",
      "    learn_time_ms: 1788.069\n",
      "    load_throughput: 33427407.85\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1306.208\n",
      "    sample_time_ms: 3062.3\n",
      "    update_time_ms: 1.316\n",
      "  timestamp: 1671813467\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:48 (running for 00:01:53.58)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         106.842</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:53 (running for 00:01:58.63)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         109.881</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 600\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3397999703884125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002356970449909568\n",
      "          model: {}\n",
      "          policy_loss: 9.650844731368124e-05\n",
      "          total_loss: 500.5468444824219\n",
      "          vf_explained_var: -0.057527437806129456\n",
      "          vf_loss: 500.54669189453125\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.525\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337508963092878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042759413030790026\n",
      "    mean_inference_ms: 0.46531775045657353\n",
      "    mean_raw_obs_processing_ms: 0.05852268570035012\n",
      "  time_since_restore: 112.93902492523193\n",
      "  time_this_iter_s: 3.0579326152801514\n",
      "  time_total_s: 112.93902492523193\n",
      "  timers:\n",
      "    learn_throughput: 2235.84\n",
      "    learn_time_ms: 1789.037\n",
      "    load_throughput: 31371009.723\n",
      "    load_time_ms: 0.128\n",
      "    sample_throughput: 1304.808\n",
      "    sample_time_ms: 3065.585\n",
      "    update_time_ms: 1.294\n",
      "  timestamp: 1671813473\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:37:58 (running for 00:02:03.83)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         116.032</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 616\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3305110037326813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003707656404003501\n",
      "          model: {}\n",
      "          policy_loss: -0.0004925053799524903\n",
      "          total_loss: 542.630126953125\n",
      "          vf_explained_var: -0.0788637027144432\n",
      "          vf_loss: 542.630615234375\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.15\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433798125225252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276228873145193\n",
      "    mean_inference_ms: 0.46546237641781957\n",
      "    mean_raw_obs_processing_ms: 0.05848631114565947\n",
      "  time_since_restore: 119.0727334022522\n",
      "  time_this_iter_s: 3.040776014328003\n",
      "  time_total_s: 119.0727334022522\n",
      "  timers:\n",
      "    learn_throughput: 2235.346\n",
      "    learn_time_ms: 1789.432\n",
      "    load_throughput: 31441559.22\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1304.452\n",
      "    sample_time_ms: 3066.421\n",
      "    update_time_ms: 1.32\n",
      "  timestamp: 1671813479\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:03 (running for 00:02:08.90)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         122.091</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 632\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31372615694999695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007560974918305874\n",
      "          model: {}\n",
      "          policy_loss: -0.001454669632948935\n",
      "          total_loss: 440.6395263671875\n",
      "          vf_explained_var: -0.10135575383901596\n",
      "          vf_loss: 440.6409912109375\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.300000000000004\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338159494423964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276235310416606\n",
      "    mean_inference_ms: 0.4655717864048512\n",
      "    mean_raw_obs_processing_ms: 0.05845090618972977\n",
      "  time_since_restore: 125.10599422454834\n",
      "  time_this_iter_s: 3.01479172706604\n",
      "  time_total_s: 125.10599422454834\n",
      "  timers:\n",
      "    learn_throughput: 2235.87\n",
      "    learn_time_ms: 1789.013\n",
      "    load_throughput: 31453348.331\n",
      "    load_time_ms: 0.127\n",
      "    sample_throughput: 1308.565\n",
      "    sample_time_ms: 3056.783\n",
      "    update_time_ms: 1.315\n",
      "  timestamp: 1671813485\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:08 (running for 00:02:13.96)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         125.106</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 648\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31648343801498413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001370706595480442\n",
      "          model: {}\n",
      "          policy_loss: 0.002188962185755372\n",
      "          total_loss: 590.8506469726562\n",
      "          vf_explained_var: -0.33437660336494446\n",
      "          vf_loss: 590.8485107421875\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.480000000000004\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337976460803132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04275853246292382\n",
      "    mean_inference_ms: 0.46563096150647837\n",
      "    mean_raw_obs_processing_ms: 0.05841406440055217\n",
      "  time_since_restore: 131.22538375854492\n",
      "  time_this_iter_s: 3.042673110961914\n",
      "  time_total_s: 131.22538375854492\n",
      "  timers:\n",
      "    learn_throughput: 2236.824\n",
      "    learn_time_ms: 1788.249\n",
      "    load_throughput: 33601474.064\n",
      "    load_time_ms: 0.119\n",
      "    sample_throughput: 1306.641\n",
      "    sample_time_ms: 3061.285\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1671813492\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:14 (running for 00:02:19.08)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         131.225</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 664\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351742678949904e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30818021297454834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004482596181333065\n",
      "          model: {}\n",
      "          policy_loss: -0.0021901451982557774\n",
      "          total_loss: 534.6130981445312\n",
      "          vf_explained_var: -0.13855034112930298\n",
      "          vf_loss: 534.6152954101562\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.625\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04336968981322421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04274706912241399\n",
      "    mean_inference_ms: 0.4656025769762755\n",
      "    mean_raw_obs_processing_ms: 0.05836811215692574\n",
      "  time_since_restore: 137.3073651790619\n",
      "  time_this_iter_s: 3.0500004291534424\n",
      "  time_total_s: 137.3073651790619\n",
      "  timers:\n",
      "    learn_throughput: 2240.176\n",
      "    learn_time_ms: 1785.574\n",
      "    load_throughput: 33341049.285\n",
      "    load_time_ms: 0.12\n",
      "    sample_throughput: 1305.959\n",
      "    sample_time_ms: 3062.884\n",
      "    update_time_ms: 1.347\n",
      "  timestamp: 1671813498\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:19 (running for 00:02:24.19)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         137.307</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:24 (running for 00:02:29.27)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         140.364</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 680\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3042510747909546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00245378608815372\n",
      "          model: {}\n",
      "          policy_loss: -0.001417657476849854\n",
      "          total_loss: 499.1313781738281\n",
      "          vf_explained_var: -0.06330925971269608\n",
      "          vf_loss: 499.1327819824219\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.9\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337061683270438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04274897796074966\n",
      "    mean_inference_ms: 0.4656969482090039\n",
      "    mean_raw_obs_processing_ms: 0.05834609071398031\n",
      "  time_since_restore: 143.43842768669128\n",
      "  time_this_iter_s: 3.074427843093872\n",
      "  time_total_s: 143.43842768669128\n",
      "  timers:\n",
      "    learn_throughput: 2240.68\n",
      "    learn_time_ms: 1785.172\n",
      "    load_throughput: 35703800.809\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1304.756\n",
      "    sample_time_ms: 3065.707\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1671813504\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:29 (running for 00:02:34.46)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         146.509</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 696\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.396983917434369e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2733024060726166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002296850783750415\n",
      "          model: {}\n",
      "          policy_loss: -0.0009159276378341019\n",
      "          total_loss: 471.1797790527344\n",
      "          vf_explained_var: 0.13280321657657623\n",
      "          vf_loss: 471.18072509765625\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043378002237498985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04275719098005286\n",
      "    mean_inference_ms: 0.46583206841731534\n",
      "    mean_raw_obs_processing_ms: 0.058333711113715624\n",
      "  time_since_restore: 149.545015335083\n",
      "  time_this_iter_s: 3.0361175537109375\n",
      "  time_total_s: 149.545015335083\n",
      "  timers:\n",
      "    learn_throughput: 2240.355\n",
      "    learn_time_ms: 1785.432\n",
      "    load_throughput: 35734219.382\n",
      "    load_time_ms: 0.112\n",
      "    sample_throughput: 1306.21\n",
      "    sample_time_ms: 3062.294\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1671813510\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:34 (running for 00:02:39.53)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         152.577</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 712\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4924597935859225e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27746322751045227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006151467561721802\n",
      "          model: {}\n",
      "          policy_loss: -0.0030704454984515905\n",
      "          total_loss: 481.7795715332031\n",
      "          vf_explained_var: -0.29841360449790955\n",
      "          vf_loss: 481.78265380859375\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.8\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337844922411784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042757985244062026\n",
      "    mean_inference_ms: 0.4658473618278681\n",
      "    mean_raw_obs_processing_ms: 0.058309056758720966\n",
      "  time_since_restore: 155.60638856887817\n",
      "  time_this_iter_s: 3.029507637023926\n",
      "  time_total_s: 155.60638856887817\n",
      "  timers:\n",
      "    learn_throughput: 2238.026\n",
      "    learn_time_ms: 1787.289\n",
      "    load_throughput: 34613608.418\n",
      "    load_time_ms: 0.116\n",
      "    sample_throughput: 1305.567\n",
      "    sample_time_ms: 3063.803\n",
      "    update_time_ms: 1.363\n",
      "  timestamp: 1671813516\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:39 (running for 00:02:44.61)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         155.606</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 728\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7462298967929613e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2986508011817932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013733146479353309\n",
      "          model: {}\n",
      "          policy_loss: 0.000683144957292825\n",
      "          total_loss: 486.56427001953125\n",
      "          vf_explained_var: -0.2059945911169052\n",
      "          vf_loss: 486.5635986328125\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043381283263195944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04276231064570618\n",
      "    mean_inference_ms: 0.4658708441569494\n",
      "    mean_raw_obs_processing_ms: 0.058289807945067516\n",
      "  time_since_restore: 161.70151042938232\n",
      "  time_this_iter_s: 3.0203309059143066\n",
      "  time_total_s: 161.70151042938232\n",
      "  timers:\n",
      "    learn_throughput: 2237.062\n",
      "    learn_time_ms: 1788.059\n",
      "    load_throughput: 34670832.817\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1305.689\n",
      "    sample_time_ms: 3063.517\n",
      "    update_time_ms: 1.333\n",
      "  timestamp: 1671813522\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:44 (running for 00:02:49.71)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         161.702</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-48\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 744\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574741982403e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2781405746936798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003607149003073573\n",
      "          model: {}\n",
      "          policy_loss: -0.00031350748031400144\n",
      "          total_loss: 477.109619140625\n",
      "          vf_explained_var: -0.11387369781732559\n",
      "          vf_loss: 477.1099548339844\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.125\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338260187779113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042765959743893064\n",
      "    mean_inference_ms: 0.46586541982889423\n",
      "    mean_raw_obs_processing_ms: 0.05826954516918967\n",
      "  time_since_restore: 167.7967541217804\n",
      "  time_this_iter_s: 3.0590336322784424\n",
      "  time_total_s: 167.7967541217804\n",
      "  timers:\n",
      "    learn_throughput: 2234.602\n",
      "    learn_time_ms: 1790.028\n",
      "    load_throughput: 34937975.843\n",
      "    load_time_ms: 0.114\n",
      "    sample_throughput: 1305.696\n",
      "    sample_time_ms: 3063.5\n",
      "    update_time_ms: 1.324\n",
      "  timestamp: 1671813528\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:49 (running for 00:02:54.84)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         167.797</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:54 (running for 00:02:59.88)<br>Memory usage on this node: 2.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         170.827</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 760\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827873709912016e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2417421042919159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024954811669886112\n",
      "          model: {}\n",
      "          policy_loss: 0.00023529196914751083\n",
      "          total_loss: 499.0150451660156\n",
      "          vf_explained_var: -0.09930148720741272\n",
      "          vf_loss: 499.01483154296875\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.78\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433813347799769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042767207832479615\n",
      "    mean_inference_ms: 0.46582492527203967\n",
      "    mean_raw_obs_processing_ms: 0.05824659703123883\n",
      "  time_since_restore: 173.84184432029724\n",
      "  time_this_iter_s: 3.014643430709839\n",
      "  time_total_s: 173.84184432029724\n",
      "  timers:\n",
      "    learn_throughput: 2233.651\n",
      "    learn_time_ms: 1790.79\n",
      "    load_throughput: 34843646.937\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1309.024\n",
      "    sample_time_ms: 3055.711\n",
      "    update_time_ms: 1.319\n",
      "  timestamp: 1671813534\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:38:59 (running for 00:03:04.99)<br>Memory usage on this node: 2.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         176.883</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_de04c_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 776\n",
      "  experiment_id: 0edb2c66a3eb4d7484838f4f8775a744\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.456968427478004e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.23795464634895325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0027954927645623684\n",
      "          model: {}\n",
      "          policy_loss: 0.0004834141000173986\n",
      "          total_loss: 495.6991271972656\n",
      "          vf_explained_var: 0.05936768278479576\n",
      "          vf_loss: 495.69866943359375\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.875\n",
      "    ram_util_percent: 9.1\n",
      "  pid: 3658\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04337525023529494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042763011646329056\n",
      "    mean_inference_ms: 0.46574063565119983\n",
      "    mean_raw_obs_processing_ms: 0.05821613123418084\n",
      "  time_since_restore: 179.93333554267883\n",
      "  time_this_iter_s: 3.0500009059906006\n",
      "  time_total_s: 179.93333554267883\n",
      "  timers:\n",
      "    learn_throughput: 2232.875\n",
      "    learn_time_ms: 1791.413\n",
      "    load_throughput: 34850884.919\n",
      "    load_time_ms: 0.115\n",
      "    sample_throughput: 1310.448\n",
      "    sample_time_ms: 3052.39\n",
      "    update_time_ms: 1.33\n",
      "  timestamp: 1671813540\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: de04c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:05 (running for 00:03:10.10)<br>Memory usage on this node: 2.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         182.991</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 16:39:06,040\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:06 (running for 00:03:11.13)<br>Memory usage on this node: 2.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_de04c_00000</td><td>RUNNING </td><td>192.168.0.98:3658</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         182.991</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m 2022-12-23 16:39:06,085\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"python/ray/_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 648, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 965, in step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     step_attempt_results = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1044, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     step_results = self._exec_plan_or_training_iteration_fn()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 2032, in _exec_plan_or_training_iteration_fn\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     results = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     result = fn(item)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 329, in __call__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     results = policy.learn_on_loaded_batch(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 549, in learn_on_loaded_batch\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return self.learn_on_batch(sliced_batch)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py\", line 420, in learn_on_batch\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     stats = builder.get(fetches)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 92, in run_timeline\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1378, in _do_call\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1454, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/worker.py\", line 427, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3658)\u001b[0m SystemExit: 1\n",
      "2022-12-23 16:39:06,273\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_de04c_00000]\n",
      "2022-12-23 16:39:06,275\tINFO tune.py:639 -- Total run time: 193.02 seconds (191.10 seconds for the tuning loop).\n",
      "2022-12-23 16:39:06,277\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fd8dd018160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\n",
    "             \"env\": \"CartPole-v1\",\n",
    "                 # other configurations go here, if none provided, then default configurations will be used\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c39ff-9dca-476c-8452-272f495c7695",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "These configurations are applied in sequence\n",
    "\n",
    "1. [Common config](https://docs.ray.io/en/master/rllib-training.html#common-parameters)\n",
    "2. [Algorithm specific config (overrides common config)](https://docs.ray.io/en/master/rllib-algorithms.html#ppo)\n",
    "3. User defined config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a495bd-1aa6-41b8-8d63-548c77748d42",
   "metadata": {},
   "source": [
    "### Anatomy of an experiment\n",
    "\n",
    "<img src=\"images/ex/2.png\" width=\"750\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185adb02-d8b2-43c7-bed2-7efb3f89fe67",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:13 (running for 00:00:00.11)<br>Memory usage on this node: 2.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:14,870\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:14,871\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:14,871\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:14,871\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:20,658\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:22 (running for 00:00:09.40)<br>Memory usage on this node: 3.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:39:22,404\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:23 (running for 00:00:10.41)<br>Memory usage on this node: 3.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-28\n",
      "  done: false\n",
      "  episode_len_mean: 20.66839378238342\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.0\n",
      "  episode_reward_mean: 20.66839378238342\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 193\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6658182740211487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027578743174672127\n",
      "          model: {}\n",
      "          policy_loss: -0.03394927456974983\n",
      "          total_loss: 144.83274841308594\n",
      "          vf_explained_var: 0.02895354852080345\n",
      "          vf_loss: 144.86117553710938\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.099999999999998\n",
      "    ram_util_percent: 11.733333333333333\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04370196880261921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04307876331950958\n",
      "    mean_inference_ms: 0.4803198592966236\n",
      "    mean_raw_obs_processing_ms: 0.07193719710130439\n",
      "  time_since_restore: 5.693679332733154\n",
      "  time_this_iter_s: 5.693679332733154\n",
      "  time_total_s: 5.693679332733154\n",
      "  timers:\n",
      "    learn_throughput: 926.821\n",
      "    learn_time_ms: 4315.827\n",
      "    load_throughput: 103234.242\n",
      "    load_time_ms: 38.747\n",
      "    sample_throughput: 1305.777\n",
      "    sample_time_ms: 3063.309\n",
      "    update_time_ms: 1.788\n",
      "  timestamp: 1671813568\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:29 (running for 00:00:16.11)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.69368</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 20.6684</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           20.6684</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:34 (running for 00:00:21.14)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.69368</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 20.6684</td><td style=\"text-align: right;\">                  54</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           20.6684</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-35\n",
      "  done: false\n",
      "  episode_len_mean: 38.94174757281554\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 142.0\n",
      "  episode_reward_mean: 38.94174757281554\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 296\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 108.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 210.0\n",
      "    episode_reward_mean: 108.85\n",
      "    episode_reward_min: 26.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 104\n",
      "      - 142\n",
      "      - 187\n",
      "      - 142\n",
      "      - 26\n",
      "      - 92\n",
      "      - 162\n",
      "      - 40\n",
      "      - 55\n",
      "      - 177\n",
      "      - 138\n",
      "      - 121\n",
      "      - 95\n",
      "      - 210\n",
      "      - 32\n",
      "      - 79\n",
      "      - 127\n",
      "      - 70\n",
      "      - 97\n",
      "      - 81\n",
      "      episode_reward:\n",
      "      - 104.0\n",
      "      - 142.0\n",
      "      - 187.0\n",
      "      - 142.0\n",
      "      - 26.0\n",
      "      - 92.0\n",
      "      - 162.0\n",
      "      - 40.0\n",
      "      - 55.0\n",
      "      - 177.0\n",
      "      - 138.0\n",
      "      - 121.0\n",
      "      - 95.0\n",
      "      - 210.0\n",
      "      - 32.0\n",
      "      - 79.0\n",
      "      - 127.0\n",
      "      - 70.0\n",
      "      - 97.0\n",
      "      - 81.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.049628064214252796\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04883924682168812\n",
      "      mean_inference_ms: 0.9259066524803364\n",
      "      mean_raw_obs_processing_ms: 0.0692083815897792\n",
      "    timesteps_this_iter: 2177\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6065852046012878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020028863102197647\n",
      "          model: {}\n",
      "          policy_loss: -0.04172568395733833\n",
      "          total_loss: 243.6154022216797\n",
      "          vf_explained_var: 0.1625100076198578\n",
      "          vf_loss: 243.6531219482422\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.15\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04329507035726578\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04270620678823113\n",
      "    mean_inference_ms: 0.47024942138241466\n",
      "    mean_raw_obs_processing_ms: 0.06797313672829308\n",
      "  time_since_restore: 13.2352135181427\n",
      "  time_this_iter_s: 7.541534185409546\n",
      "  time_total_s: 13.2352135181427\n",
      "  timers:\n",
      "    learn_throughput: 978.407\n",
      "    learn_time_ms: 4088.279\n",
      "    load_throughput: 201546.277\n",
      "    load_time_ms: 19.847\n",
      "    sample_throughput: 918.404\n",
      "    sample_time_ms: 4355.381\n",
      "    update_time_ms: 1.761\n",
      "  timestamp: 1671813575\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:39 (running for 00:00:26.67)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.2352</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 38.9417</td><td style=\"text-align: right;\">                 142</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           38.9417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 60.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 60.35\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 334\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5733065009117126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014462259598076344\n",
      "          model: {}\n",
      "          policy_loss: -0.02470884658396244\n",
      "          total_loss: 605.0413818359375\n",
      "          vf_explained_var: 0.16552267968654633\n",
      "          vf_loss: 605.063232421875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.2875\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04340592961192516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04260075598207408\n",
      "    mean_inference_ms: 0.4695108165400259\n",
      "    mean_raw_obs_processing_ms: 0.06672173465851058\n",
      "  time_since_restore: 18.374347686767578\n",
      "  time_this_iter_s: 5.139134168624878\n",
      "  time_total_s: 18.374347686767578\n",
      "  timers:\n",
      "    learn_throughput: 994.819\n",
      "    learn_time_ms: 4020.83\n",
      "    load_throughput: 294831.958\n",
      "    load_time_ms: 13.567\n",
      "    sample_throughput: 737.66\n",
      "    sample_time_ms: 5422.55\n",
      "    update_time_ms: 1.786\n",
      "  timestamp: 1671813580\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:44 (running for 00:00:31.86)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.3743</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   60.35</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             60.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:49 (running for 00:00:36.86)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.3743</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   60.35</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             60.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 93.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 488.0\n",
      "  episode_reward_mean: 93.66\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 349\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 332.2\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 332.2\n",
      "    episode_reward_min: 55.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 375\n",
      "      - 342\n",
      "      - 182\n",
      "      - 233\n",
      "      - 500\n",
      "      - 238\n",
      "      - 500\n",
      "      - 289\n",
      "      - 391\n",
      "      - 413\n",
      "      - 449\n",
      "      - 62\n",
      "      - 289\n",
      "      - 456\n",
      "      - 196\n",
      "      - 445\n",
      "      - 299\n",
      "      - 455\n",
      "      - 475\n",
      "      - 55\n",
      "      episode_reward:\n",
      "      - 375.0\n",
      "      - 342.0\n",
      "      - 182.0\n",
      "      - 233.0\n",
      "      - 500.0\n",
      "      - 238.0\n",
      "      - 500.0\n",
      "      - 289.0\n",
      "      - 391.0\n",
      "      - 413.0\n",
      "      - 449.0\n",
      "      - 62.0\n",
      "      - 289.0\n",
      "      - 456.0\n",
      "      - 196.0\n",
      "      - 445.0\n",
      "      - 299.0\n",
      "      - 455.0\n",
      "      - 475.0\n",
      "      - 55.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.048960722700805245\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04826087147382564\n",
      "      mean_inference_ms: 0.9110565224463535\n",
      "      mean_raw_obs_processing_ms: 0.06629179675842298\n",
      "    timesteps_this_iter: 6644\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5556430816650391\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008279351517558098\n",
      "          model: {}\n",
      "          policy_loss: -0.015395468100905418\n",
      "          total_loss: 820.6736450195312\n",
      "          vf_explained_var: 0.18643075227737427\n",
      "          vf_loss: 820.6874389648438\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.11764705882353\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338712315543995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04254779284209498\n",
      "    mean_inference_ms: 0.4687605941275072\n",
      "    mean_raw_obs_processing_ms: 0.0660060296349924\n",
      "  time_since_restore: 30.599788427352905\n",
      "  time_this_iter_s: 12.225440740585327\n",
      "  time_total_s: 30.599788427352905\n",
      "  timers:\n",
      "    learn_throughput: 1004.801\n",
      "    learn_time_ms: 3980.887\n",
      "    load_throughput: 382351.831\n",
      "    load_time_ms: 10.462\n",
      "    sample_throughput: 747.173\n",
      "    sample_time_ms: 5353.513\n",
      "    update_time_ms: 1.771\n",
      "  timestamp: 1671813593\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:39:55 (running for 00:00:42.10)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         30.5998</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   93.66</td><td style=\"text-align: right;\">                 488</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             93.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 130.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 130.41\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 362\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5274227261543274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003228301415219903\n",
      "          model: {}\n",
      "          policy_loss: -0.014217283576726913\n",
      "          total_loss: 646.3196411132812\n",
      "          vf_explained_var: 0.12532773613929749\n",
      "          vf_loss: 646.333251953125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.15\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043293875528274926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04252930220242417\n",
      "    mean_inference_ms: 0.4680333138415231\n",
      "    mean_raw_obs_processing_ms: 0.06537694216950114\n",
      "  time_since_restore: 35.73881149291992\n",
      "  time_this_iter_s: 5.139023065567017\n",
      "  time_total_s: 35.73881149291992\n",
      "  timers:\n",
      "    learn_throughput: 1009.854\n",
      "    learn_time_ms: 3960.968\n",
      "    load_throughput: 466095.924\n",
      "    load_time_ms: 8.582\n",
      "    sample_throughput: 593.945\n",
      "    sample_time_ms: 6734.632\n",
      "    update_time_ms: 1.758\n",
      "  timestamp: 1671813598\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:00 (running for 00:00:47.23)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.7388</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            130.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:05 (running for 00:00:52.26)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.7388</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            130.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:10 (running for 00:00:57.26)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.7388</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.41</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            130.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 164.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 164.04\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 373\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 377.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 377.85\n",
      "    episode_reward_min: 163.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 249\n",
      "      - 361\n",
      "      - 282\n",
      "      - 163\n",
      "      - 384\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 398\n",
      "      - 380\n",
      "      - 337\n",
      "      - 352\n",
      "      - 419\n",
      "      - 447\n",
      "      - 354\n",
      "      - 421\n",
      "      - 284\n",
      "      - 368\n",
      "      - 500\n",
      "      - 358\n",
      "      episode_reward:\n",
      "      - 249.0\n",
      "      - 361.0\n",
      "      - 282.0\n",
      "      - 163.0\n",
      "      - 384.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 398.0\n",
      "      - 380.0\n",
      "      - 337.0\n",
      "      - 352.0\n",
      "      - 419.0\n",
      "      - 447.0\n",
      "      - 354.0\n",
      "      - 421.0\n",
      "      - 284.0\n",
      "      - 368.0\n",
      "      - 500.0\n",
      "      - 358.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04874078394387171\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04809989663691259\n",
      "      mean_inference_ms: 0.9065942236177579\n",
      "      mean_raw_obs_processing_ms: 0.06569985745408627\n",
      "    timesteps_this_iter: 7557\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5140173435211182\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002574685262516141\n",
      "          model: {}\n",
      "          policy_loss: -0.008090183138847351\n",
      "          total_loss: 424.1639709472656\n",
      "          vf_explained_var: 0.29855477809906006\n",
      "          vf_loss: 424.17156982421875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.75\n",
      "    ram_util_percent: 11.800000000000002\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04318293463305909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04250191508946351\n",
      "    mean_inference_ms: 0.4672887516339973\n",
      "    mean_raw_obs_processing_ms: 0.06476630804861637\n",
      "  time_since_restore: 48.9020791053772\n",
      "  time_this_iter_s: 13.163267612457275\n",
      "  time_total_s: 48.9020791053772\n",
      "  timers:\n",
      "    learn_throughput: 1014.132\n",
      "    learn_time_ms: 3944.261\n",
      "    load_throughput: 543247.918\n",
      "    load_time_ms: 7.363\n",
      "    sample_throughput: 618.319\n",
      "    sample_time_ms: 6469.158\n",
      "    update_time_ms: 1.752\n",
      "  timestamp: 1671813611\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:15 (running for 00:01:02.44)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         48.9021</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  164.04</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            164.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-40-16\n",
      "  done: false\n",
      "  episode_len_mean: 203.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 203.33\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 384\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5009874105453491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002488570287823677\n",
      "          model: {}\n",
      "          policy_loss: -0.009245635010302067\n",
      "          total_loss: 364.897705078125\n",
      "          vf_explained_var: 0.24103020131587982\n",
      "          vf_loss: 364.9064636230469\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.762500000000003\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043086843367341005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04247888440561825\n",
      "    mean_inference_ms: 0.46658721348223686\n",
      "    mean_raw_obs_processing_ms: 0.0640898777319044\n",
      "  time_since_restore: 54.0167510509491\n",
      "  time_this_iter_s: 5.114671945571899\n",
      "  time_total_s: 54.0167510509491\n",
      "  timers:\n",
      "    learn_throughput: 1017.496\n",
      "    learn_time_ms: 3931.219\n",
      "    load_throughput: 620144.643\n",
      "    load_time_ms: 6.45\n",
      "    sample_throughput: 538.271\n",
      "    sample_time_ms: 7431.208\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1671813616\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:20 (running for 00:01:07.55)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         54.0168</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  203.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            203.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:25 (running for 00:01:12.57)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         54.0168</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  203.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            203.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:30 (running for 00:01:17.58)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         54.0168</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  203.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            203.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 237.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 237.33\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 392\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 475.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 475.0\n",
      "    episode_reward_min: 368.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 483\n",
      "      - 500\n",
      "      - 500\n",
      "      - 453\n",
      "      - 500\n",
      "      - 432\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 442\n",
      "      - 500\n",
      "      - 433\n",
      "      - 368\n",
      "      - 500\n",
      "      - 495\n",
      "      - 500\n",
      "      - 394\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 483.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 453.0\n",
      "      - 500.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 442.0\n",
      "      - 500.0\n",
      "      - 433.0\n",
      "      - 368.0\n",
      "      - 500.0\n",
      "      - 495.0\n",
      "      - 500.0\n",
      "      - 394.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.048692639420650594\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04806857217965362\n",
      "      mean_inference_ms: 0.9046659352048904\n",
      "      mean_raw_obs_processing_ms: 0.06540300046519172\n",
      "    timesteps_this_iter: 9500\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.504379153251648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0039298986084759235\n",
      "          model: {}\n",
      "          policy_loss: -0.008224690333008766\n",
      "          total_loss: 295.3396301269531\n",
      "          vf_explained_var: 0.18345040082931519\n",
      "          vf_loss: 295.3470458984375\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.73333333333333\n",
      "    ram_util_percent: 11.800000000000002\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0430176037856946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04245600258199831\n",
      "    mean_inference_ms: 0.46600567572651597\n",
      "    mean_raw_obs_processing_ms: 0.0635503854403806\n",
      "  time_since_restore: 69.25154376029968\n",
      "  time_this_iter_s: 15.234792709350586\n",
      "  time_total_s: 69.25154376029968\n",
      "  timers:\n",
      "    learn_throughput: 1019.492\n",
      "    learn_time_ms: 3923.523\n",
      "    load_throughput: 688871.867\n",
      "    load_time_ms: 5.807\n",
      "    sample_throughput: 560.167\n",
      "    sample_time_ms: 7140.727\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1671813631\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:35 (running for 00:01:22.83)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         69.2515</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  237.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            237.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_55144_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_16-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 271.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 271.75\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 400\n",
      "  experiment_id: e5c692799fb2428096a6846fa76c1b24\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4935498833656311\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003270569257438183\n",
      "          model: {}\n",
      "          policy_loss: -0.00635184021666646\n",
      "          total_loss: 239.37002563476562\n",
      "          vf_explained_var: 0.26556897163391113\n",
      "          vf_loss: 239.375732421875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.525\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 3657\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04299136346593151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042427050395303756\n",
      "    mean_inference_ms: 0.4655129342036001\n",
      "    mean_raw_obs_processing_ms: 0.06302450516267091\n",
      "  time_since_restore: 74.38026714324951\n",
      "  time_this_iter_s: 5.128723382949829\n",
      "  time_total_s: 74.38026714324951\n",
      "  timers:\n",
      "    learn_throughput: 1020.488\n",
      "    learn_time_ms: 3919.695\n",
      "    load_throughput: 758181.829\n",
      "    load_time_ms: 5.276\n",
      "    sample_throughput: 497.355\n",
      "    sample_time_ms: 8042.55\n",
      "    update_time_ms: 1.728\n",
      "  timestamp: 1671813636\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '55144_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 16:40:40,954\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:40 (running for 00:01:27.95)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         74.3803</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  271.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            271.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 16:40:40 (running for 00:01:27.98)<br>Memory usage on this node: 3.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/18.16 GiB heap, 0.0/9.08 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_55144_00000</td><td>RUNNING </td><td>192.168.0.98:3657</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         74.3803</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  271.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            271.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m 2022-12-23 16:40:41,084\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"python/ray/_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 648, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 965, in step\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     step_attempt_results = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1049, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     step_results = self._exec_plan_or_training_iteration_fn()\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 2032, in _exec_plan_or_training_iteration_fn\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     results = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     result = fn(item)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 329, in __call__\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     results = policy.learn_on_loaded_batch(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 551, in learn_on_loaded_batch\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return self.multi_gpu_tower_stacks[buffer_index].optimize(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 1088, in optimize\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return sess.run(fetches, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 968, in run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1371, in _do_run\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1378, in _do_call\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1361, in _run_fn\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1454, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m   File \"/home/oscar/anaconda3/envs/fastdeeprl/lib/python3.9/site-packages/ray/worker.py\", line 427, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3657)\u001b[0m SystemExit: 1\n",
      "2022-12-23 16:40:41,187\tERROR tune.py:635 -- Trials did not complete: [PPO_CartPole-v1_55144_00000]\n",
      "2022-12-23 16:40:41,187\tINFO tune.py:639 -- Total run time: 88.19 seconds (87.95 seconds for the tuning loop).\n",
      "2022-12-23 16:40:41,188\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fd8d7fc1160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2,    # num of training iter between evaluations\n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 \"num_gpus\": 2\n",
    "                 }\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d547c2d-aa55-48f4-b586-f183b763a301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
