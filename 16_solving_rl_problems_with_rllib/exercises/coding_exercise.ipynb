{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b850cb2-9dcd-4c58-88c7-2536416817d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Teach the robot in the `BipedalWalker-v3` environment how to walk using `rllib`'s PPO implementation\n",
    "\n",
    "Finally, the time has come to teach the robot how to walk. \n",
    "\n",
    "Are you ready? Let's go!\n",
    "\n",
    "Before starting this exercise, first check your `ray` version using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340b3a1c-f83c-4a50-a1e9-b9273bc0f186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab45b0-7037-4662-bfb3-a688d644f169",
   "metadata": {},
   "source": [
    "If the version is greater than `1.11.0`, please downgrade the package to `1.11.0` first.\n",
    "\n",
    "You can do that by running the following code in the terminal.\n",
    "\n",
    "```\n",
    "pip uninstall ray[rllib]\n",
    "pip install ray[rllib]==1.11.0\n",
    "```\n",
    "\n",
    "We are doing this because version `1.12.0` has a bug which prevents `BipedalWalker-v3` from learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf60ce-2536-4d74-b9c7-a089543b7336",
   "metadata": {},
   "source": [
    "Next, import and initialize `ray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b732c65d-ddbf-4cd5-a27e-8826f4d439c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.98',\n",
       " 'raylet_ip_address': '192.168.0.98',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-12-23_17-24-36_251314_158663/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-12-23_17-24-36_251314_158663/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-12-23_17-24-36_251314_158663',\n",
       " 'metrics_export_port': 64706,\n",
       " 'gcs_address': '192.168.0.98:60517',\n",
       " 'address': '192.168.0.98:60517',\n",
       " 'node_id': '2179fa9e7c608b3605f79664922df67944fa2aeabd883093aa3cc4df'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and initialize ray in this cell\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735fb8c-cffe-451a-a994-32aac0810238",
   "metadata": {},
   "source": [
    "To teach the robot how to walk, you simply need to run an experiment (using `ray`'s experiment runner) with the following configurations\n",
    "\n",
    "- Set the algorithm to \"PPO\"\n",
    "- Set the environment to \"BipedalWalker-v3\"\n",
    "\n",
    "Those are the required configurations. In addition to those, also set the following optional configurations.\n",
    "\n",
    "- Set the number of training iterations between evaluations to 100\n",
    "- Set the number of episodes used for each evaluation to 100\n",
    "\n",
    "**WARNING: The `BipedalWalker-v3` environment is much harder than `CartPole-v1`. It's going to take many timesteps (~ 5 million timesteps) for the robot to reach acceptable walking performance (~ 250 cumulative rewards per episode). You may have to keep your computer running for ~ 4 hours, when using tensorflow on CPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84c0361-3189-40bf-b6c5-3605de24c5b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:00 (running for 00:00:00.12)<br>Memory usage on this node: 5.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:02,025\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:02,026\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:02,026\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:02,026\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:07,805\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:09 (running for 00:00:09.82)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=158805)\u001b[0m 2022-12-23 17:28:09,834\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:10 (running for 00:00:10.82)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:15 (running for 00:00:15.83)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 678.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.33964626871236\n",
      "  episode_reward_mean: -109.75189014949474\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 5\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.645051956176758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0156696829944849\n",
      "          model: {}\n",
      "          policy_loss: -0.022245073691010475\n",
      "          total_loss: 274.5101623535156\n",
      "          vf_explained_var: -0.021566230803728104\n",
      "          vf_loss: 274.5292663574219\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.380000000000003\n",
      "    ram_util_percent: 21.509999999999998\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09046051277034822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3263741359300818\n",
      "    mean_inference_ms: 0.5338038997850318\n",
      "    mean_raw_obs_processing_ms: 0.0724974779532231\n",
      "  time_since_restore: 6.840594530105591\n",
      "  time_this_iter_s: 6.840594530105591\n",
      "  time_total_s: 6.840594530105591\n",
      "  timers:\n",
      "    learn_throughput: 848.261\n",
      "    learn_time_ms: 4715.529\n",
      "    load_throughput: 106834.03\n",
      "    load_time_ms: 37.441\n",
      "    sample_throughput: 971.806\n",
      "    sample_time_ms: 4116.047\n",
      "    update_time_ms: 1.815\n",
      "  timestamp: 1671816496\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:21 (running for 00:00:21.70)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.84059</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-109.752</td><td style=\"text-align: right;\">             -101.34</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">             678.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 422.2352941176471\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.2622276331869\n",
      "  episode_reward_mean: -110.29294449331815\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 17\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.692352771759033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01690756529569626\n",
      "          model: {}\n",
      "          policy_loss: -0.026634719222784042\n",
      "          total_loss: 691.4613647460938\n",
      "          vf_explained_var: -0.3008078336715698\n",
      "          vf_loss: 691.484619140625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.577777777777776\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0894252714002733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32318917101454814\n",
      "    mean_inference_ms: 0.5242658765980911\n",
      "    mean_raw_obs_processing_ms: 0.07364177257176586\n",
      "  time_since_restore: 13.082947015762329\n",
      "  time_this_iter_s: 6.242352485656738\n",
      "  time_total_s: 13.082947015762329\n",
      "  timers:\n",
      "    learn_throughput: 895.494\n",
      "    learn_time_ms: 4466.811\n",
      "    load_throughput: 208353.092\n",
      "    load_time_ms: 19.198\n",
      "    sample_throughput: 733.267\n",
      "    sample_time_ms: 5455.039\n",
      "    update_time_ms: 1.75\n",
      "  timestamp: 1671816502\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:26 (running for 00:00:26.94)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.0829</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-110.293</td><td style=\"text-align: right;\">            -101.262</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           422.235</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 523.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01919514975386\n",
      "  episode_reward_mean: -109.73047982333135\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 20\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.704278945922852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010567907243967056\n",
      "          model: {}\n",
      "          policy_loss: -0.0235713180154562\n",
      "          total_loss: 75.7781753540039\n",
      "          vf_explained_var: -0.41361796855926514\n",
      "          vf_loss: 75.79962921142578\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.43333333333333\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08936064062892823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3230784737365422\n",
      "    mean_inference_ms: 0.5231103885023252\n",
      "    mean_raw_obs_processing_ms: 0.07349957901066767\n",
      "  time_since_restore: 19.346436262130737\n",
      "  time_this_iter_s: 6.263489246368408\n",
      "  time_total_s: 19.346436262130737\n",
      "  timers:\n",
      "    learn_throughput: 911.23\n",
      "    learn_time_ms: 4389.671\n",
      "    load_throughput: 304648.863\n",
      "    load_time_ms: 13.13\n",
      "    sample_throughput: 698.667\n",
      "    sample_time_ms: 5725.19\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1671816509\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:32 (running for 00:00:32.24)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         19.3464</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -109.73</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">            523.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 461.96774193548384\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01919514975386\n",
      "  episode_reward_mean: -110.43834433051995\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 31\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.697986602783203\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020429696887731552\n",
      "          model: {}\n",
      "          policy_loss: -0.03610014542937279\n",
      "          total_loss: 578.8330078125\n",
      "          vf_explained_var: -0.6338520646095276\n",
      "          vf_loss: 578.8650512695312\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.51111111111111\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0892411051921174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32220420649519876\n",
      "    mean_inference_ms: 0.5199934854972303\n",
      "    mean_raw_obs_processing_ms: 0.07386015457512038\n",
      "  time_since_restore: 25.635694980621338\n",
      "  time_this_iter_s: 6.289258718490601\n",
      "  time_total_s: 25.635694980621338\n",
      "  timers:\n",
      "    learn_throughput: 919.399\n",
      "    learn_time_ms: 4350.668\n",
      "    load_throughput: 395558.448\n",
      "    load_time_ms: 10.112\n",
      "    sample_throughput: 682.03\n",
      "    sample_time_ms: 5864.844\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1671816515\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:37 (running for 00:00:37.52)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         25.6357</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-110.438</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           461.968</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 517.3823529411765\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01919514975386\n",
      "  episode_reward_mean: -110.4887255121062\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 34\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.538061618804932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010549559257924557\n",
      "          model: {}\n",
      "          policy_loss: -0.02856048196554184\n",
      "          total_loss: 52.932525634765625\n",
      "          vf_explained_var: -0.5739486813545227\n",
      "          vf_loss: 52.95897674560547\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.37777777777778\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08912204058915457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3219144495873468\n",
      "    mean_inference_ms: 0.5193510861946445\n",
      "    mean_raw_obs_processing_ms: 0.0736530346807649\n",
      "  time_since_restore: 31.832728147506714\n",
      "  time_this_iter_s: 6.197033166885376\n",
      "  time_total_s: 31.832728147506714\n",
      "  timers:\n",
      "    learn_throughput: 924.519\n",
      "    learn_time_ms: 4326.572\n",
      "    load_throughput: 479779.459\n",
      "    load_time_ms: 8.337\n",
      "    sample_throughput: 673.357\n",
      "    sample_time_ms: 5940.381\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1671816521\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:42 (running for 00:00:42.76)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         31.8327</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-110.489</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           517.382</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:47 (running for 00:00:47.76)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         31.8327</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-110.489</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           517.382</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 590.3421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01919514975386\n",
      "  episode_reward_mean: -110.57435748232952\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 38\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.5306525230407715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009493827819824219\n",
      "          model: {}\n",
      "          policy_loss: -0.023390917107462883\n",
      "          total_loss: 52.365264892578125\n",
      "          vf_explained_var: -0.23677678406238556\n",
      "          vf_loss: 52.38675308227539\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.57777777777778\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.089017247665187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3216648901140061\n",
      "    mean_inference_ms: 0.5185501535671819\n",
      "    mean_raw_obs_processing_ms: 0.07345690774279406\n",
      "  time_since_restore: 38.08493399620056\n",
      "  time_this_iter_s: 6.252205848693848\n",
      "  time_total_s: 38.08493399620056\n",
      "  timers:\n",
      "    learn_throughput: 928.202\n",
      "    learn_time_ms: 4309.405\n",
      "    load_throughput: 559881.286\n",
      "    load_time_ms: 7.144\n",
      "    sample_throughput: 666.999\n",
      "    sample_time_ms: 5997.014\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1671816528\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:53 (running for 00:00:53.04)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         38.0849</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-110.574</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           590.342</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 664.219512195122\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01919514975386\n",
      "  episode_reward_mean: -110.24733988385232\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 41\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.502223491668701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009272604249417782\n",
      "          model: {}\n",
      "          policy_loss: 0.002136416034772992\n",
      "          total_loss: 3.188262462615967\n",
      "          vf_explained_var: -0.04488189518451691\n",
      "          vf_loss: 3.1842713356018066\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.97777777777778\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08895392845838458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3214482347879195\n",
      "    mean_inference_ms: 0.5179146032059067\n",
      "    mean_raw_obs_processing_ms: 0.07333446919846495\n",
      "  time_since_restore: 44.31987762451172\n",
      "  time_this_iter_s: 6.234943628311157\n",
      "  time_total_s: 44.31987762451172\n",
      "  timers:\n",
      "    learn_throughput: 930.26\n",
      "    learn_time_ms: 4299.875\n",
      "    load_throughput: 636813.517\n",
      "    load_time_ms: 6.281\n",
      "    sample_throughput: 663.235\n",
      "    sample_time_ms: 6031.042\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1671816534\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:28:58 (running for 00:00:58.27)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         44.3199</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-110.247</td><td style=\"text-align: right;\">            -101.019</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">            664.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-00\n",
      "  done: false\n",
      "  episode_len_mean: 707.7441860465116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101.01330546350157\n",
      "  episode_reward_mean: -109.8995033452653\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 43\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.505726337432861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011723184026777744\n",
      "          model: {}\n",
      "          policy_loss: -0.019915057346224785\n",
      "          total_loss: 2.3570139408111572\n",
      "          vf_explained_var: 0.04875466972589493\n",
      "          vf_loss: 2.374584197998047\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.733333333333334\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08890070574703186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3213108999764229\n",
      "    mean_inference_ms: 0.5175277319301486\n",
      "    mean_raw_obs_processing_ms: 0.07321396964809958\n",
      "  time_since_restore: 50.536664724349976\n",
      "  time_this_iter_s: 6.216787099838257\n",
      "  time_total_s: 50.536664724349976\n",
      "  timers:\n",
      "    learn_throughput: 932.129\n",
      "    learn_time_ms: 4291.251\n",
      "    load_throughput: 709534.782\n",
      "    load_time_ms: 5.637\n",
      "    sample_throughput: 660.251\n",
      "    sample_time_ms: 6058.299\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1671816540\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:03 (running for 00:01:03.53)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         50.5367</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  -109.9</td><td style=\"text-align: right;\">            -101.013</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           707.744</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 747.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -98.12393183841229\n",
      "  episode_reward_mean: -109.57519787152438\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 45\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.39167594909668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015462150797247887\n",
      "          model: {}\n",
      "          policy_loss: -0.02336364984512329\n",
      "          total_loss: 1.1338802576065063\n",
      "          vf_explained_var: -0.009230677969753742\n",
      "          vf_loss: 1.154151439666748\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.3\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08885254438494537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3211999656478133\n",
      "    mean_inference_ms: 0.5171561724371055\n",
      "    mean_raw_obs_processing_ms: 0.07308672671383011\n",
      "  time_since_restore: 56.74006533622742\n",
      "  time_this_iter_s: 6.203400611877441\n",
      "  time_total_s: 56.74006533622742\n",
      "  timers:\n",
      "    learn_throughput: 934.453\n",
      "    learn_time_ms: 4280.577\n",
      "    load_throughput: 782734.395\n",
      "    load_time_ms: 5.11\n",
      "    sample_throughput: 657.835\n",
      "    sample_time_ms: 6080.554\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1671816546\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:08 (running for 00:01:08.73)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         56.7401</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-109.575</td><td style=\"text-align: right;\">            -98.1239</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">             747.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 785.5510204081633\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -98.12393183841229\n",
      "  episode_reward_mean: -109.10391027058873\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 49\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.346787929534912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010472910478711128\n",
      "          model: {}\n",
      "          policy_loss: -0.021779850125312805\n",
      "          total_loss: 65.93119049072266\n",
      "          vf_explained_var: -0.21763400733470917\n",
      "          vf_loss: 65.95087432861328\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.977777777777774\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08874286703533454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3210586245819444\n",
      "    mean_inference_ms: 0.5165789155588874\n",
      "    mean_raw_obs_processing_ms: 0.07282608420462833\n",
      "  time_since_restore: 62.99511432647705\n",
      "  time_this_iter_s: 6.255048990249634\n",
      "  time_total_s: 62.99511432647705\n",
      "  timers:\n",
      "    learn_throughput: 935.502\n",
      "    learn_time_ms: 4275.779\n",
      "    load_throughput: 850681.013\n",
      "    load_time_ms: 4.702\n",
      "    sample_throughput: 656.146\n",
      "    sample_time_ms: 6096.201\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1671816553\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:14 (running for 00:01:14.03)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         62.9951</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-109.104</td><td style=\"text-align: right;\">            -98.1239</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           785.551</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:19 (running for 00:01:19.04)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         62.9951</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-109.104</td><td style=\"text-align: right;\">            -98.1239</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           785.551</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 832.5384615384615\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -92.20223257929587\n",
      "  episode_reward_mean: -108.33758692707475\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 52\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.207039833068848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013340535573661327\n",
      "          model: {}\n",
      "          policy_loss: -0.012544560246169567\n",
      "          total_loss: 2.334543228149414\n",
      "          vf_explained_var: -0.01019146665930748\n",
      "          vf_loss: 2.3444197177886963\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.06666666666667\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08869705007976668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32093707308398656\n",
      "    mean_inference_ms: 0.5160991433494719\n",
      "    mean_raw_obs_processing_ms: 0.07268400382704024\n",
      "  time_since_restore: 69.25296568870544\n",
      "  time_this_iter_s: 6.2578513622283936\n",
      "  time_total_s: 69.25296568870544\n",
      "  timers:\n",
      "    learn_throughput: 946.114\n",
      "    learn_time_ms: 4227.822\n",
      "    load_throughput: 3770754.051\n",
      "    load_time_ms: 1.061\n",
      "    sample_throughput: 633.715\n",
      "    sample_time_ms: 6311.991\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1671816559\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:24 (running for 00:01:24.31)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          69.253</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-108.338</td><td style=\"text-align: right;\">            -92.2022</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           832.538</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 832.7857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -92.20223257929587\n",
      "  episode_reward_mean: -108.05404639794207\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 56\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.260312557220459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01325264759361744\n",
      "          model: {}\n",
      "          policy_loss: -0.03027886338531971\n",
      "          total_loss: 106.6024169921875\n",
      "          vf_explained_var: -0.32580262422561646\n",
      "          vf_loss: 106.63005065917969\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.0125\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08862628604505261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3208013605996066\n",
      "    mean_inference_ms: 0.5155445998669949\n",
      "    mean_raw_obs_processing_ms: 0.0724957789594139\n",
      "  time_since_restore: 75.46610641479492\n",
      "  time_this_iter_s: 6.2131407260894775\n",
      "  time_total_s: 75.46610641479492\n",
      "  timers:\n",
      "    learn_throughput: 945.991\n",
      "    learn_time_ms: 4228.369\n",
      "    load_throughput: 3746754.21\n",
      "    load_time_ms: 1.068\n",
      "    sample_throughput: 639.293\n",
      "    sample_time_ms: 6256.912\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1671816565\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:29 (running for 00:01:29.51)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         75.4661</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-108.054</td><td style=\"text-align: right;\">            -92.2022</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           832.786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 845.8474576271186\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -92.20223257929587\n",
      "  episode_reward_mean: -107.6359293292368\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 59\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.196269989013672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007698612287640572\n",
      "          model: {}\n",
      "          policy_loss: -0.027825862169265747\n",
      "          total_loss: 47.55764389038086\n",
      "          vf_explained_var: -0.36368098855018616\n",
      "          vf_loss: 47.58393096923828\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.32222222222222\n",
      "    ram_util_percent: 21.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08856510840487325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32069401743859977\n",
      "    mean_inference_ms: 0.5151683435528214\n",
      "    mean_raw_obs_processing_ms: 0.07235079722563446\n",
      "  time_since_restore: 81.66256356239319\n",
      "  time_this_iter_s: 6.196457147598267\n",
      "  time_total_s: 81.66256356239319\n",
      "  timers:\n",
      "    learn_throughput: 946.129\n",
      "    learn_time_ms: 4227.755\n",
      "    load_throughput: 3735991.271\n",
      "    load_time_ms: 1.071\n",
      "    sample_throughput: 639.875\n",
      "    sample_time_ms: 6251.225\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1671816571\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:34 (running for 00:01:34.75)<br>Memory usage on this node: 6.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         81.6626</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-107.636</td><td style=\"text-align: right;\">            -92.2022</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           845.847</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-38\n",
      "  done: false\n",
      "  episode_len_mean: 882.3387096774194\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -92.20223257929587\n",
      "  episode_reward_mean: -107.2313999005172\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 62\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.153186798095703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011024028062820435\n",
      "          model: {}\n",
      "          policy_loss: -0.028049172833561897\n",
      "          total_loss: 4.7348809242248535\n",
      "          vf_explained_var: -0.14691314101219177\n",
      "          vf_loss: 4.760725498199463\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.17777777777778\n",
      "    ram_util_percent: 21.555555555555557\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08850808260768221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32059295724229664\n",
      "    mean_inference_ms: 0.5148147975752518\n",
      "    mean_raw_obs_processing_ms: 0.0722160932879067\n",
      "  time_since_restore: 87.92753148078918\n",
      "  time_this_iter_s: 6.264967918395996\n",
      "  time_total_s: 87.92753148078918\n",
      "  timers:\n",
      "    learn_throughput: 945.369\n",
      "    learn_time_ms: 4231.152\n",
      "    load_throughput: 3737572.625\n",
      "    load_time_ms: 1.07\n",
      "    sample_throughput: 640.269\n",
      "    sample_time_ms: 6247.37\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1671816578\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:40 (running for 00:01:40.01)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         87.9275</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-107.231</td><td style=\"text-align: right;\">            -92.2022</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           882.339</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 904.765625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -83.99104351180475\n",
      "  episode_reward_mean: -106.71932501567602\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 64\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.10328483581543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012507213279604912\n",
      "          model: {}\n",
      "          policy_loss: -0.025799917057156563\n",
      "          total_loss: 1.2651374340057373\n",
      "          vf_explained_var: 0.11706218123435974\n",
      "          vf_loss: 1.2884358167648315\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.72222222222222\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847668197117445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.320535059505316\n",
      "    mean_inference_ms: 0.5145660348973253\n",
      "    mean_raw_obs_processing_ms: 0.0721323263800806\n",
      "  time_since_restore: 94.19981908798218\n",
      "  time_this_iter_s: 6.272287607192993\n",
      "  time_total_s: 94.19981908798218\n",
      "  timers:\n",
      "    learn_throughput: 944.553\n",
      "    learn_time_ms: 4234.809\n",
      "    load_throughput: 3804529.911\n",
      "    load_time_ms: 1.051\n",
      "    sample_throughput: 639.756\n",
      "    sample_time_ms: 6252.384\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1671816584\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:45 (running for 00:01:45.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         94.1998</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-106.719</td><td style=\"text-align: right;\">             -83.991</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           904.766</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:50 (running for 00:01:50.32)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         94.1998</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-106.719</td><td style=\"text-align: right;\">             -83.991</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           904.766</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 923.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -83.99104351180475\n",
      "  episode_reward_mean: -106.11382712370833\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 68\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.118871212005615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009027808904647827\n",
      "          model: {}\n",
      "          policy_loss: -0.024687567725777626\n",
      "          total_loss: 53.5495719909668\n",
      "          vf_explained_var: -0.126837357878685\n",
      "          vf_loss: 53.57245635986328\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.28888888888889\n",
      "    ram_util_percent: 21.5\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841370951359243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3204089429314531\n",
      "    mean_inference_ms: 0.5140818467192685\n",
      "    mean_raw_obs_processing_ms: 0.07197629966004652\n",
      "  time_since_restore: 100.41538619995117\n",
      "  time_this_iter_s: 6.215567111968994\n",
      "  time_total_s: 100.41538619995117\n",
      "  timers:\n",
      "    learn_throughput: 944.477\n",
      "    learn_time_ms: 4235.149\n",
      "    load_throughput: 3854084.675\n",
      "    load_time_ms: 1.038\n",
      "    sample_throughput: 639.815\n",
      "    sample_time_ms: 6251.807\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1671816590\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:29:55 (running for 00:01:55.56)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         100.415</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-106.114</td><td style=\"text-align: right;\">             -83.991</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">               923</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 942.3428571428572\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -83.99104351180475\n",
      "  episode_reward_mean: -105.55590158544807\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 70\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.122050762176514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008303681388497353\n",
      "          model: {}\n",
      "          policy_loss: -0.022225620225071907\n",
      "          total_loss: 3.5172359943389893\n",
      "          vf_explained_var: 0.018919847905635834\n",
      "          vf_loss: 3.5378007888793945\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.011111111111106\n",
      "    ram_util_percent: 21.433333333333334\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08838435744349435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32035789976313095\n",
      "    mean_inference_ms: 0.5138564264228859\n",
      "    mean_raw_obs_processing_ms: 0.07190079606634064\n",
      "  time_since_restore: 106.64849352836609\n",
      "  time_this_iter_s: 6.233107328414917\n",
      "  time_total_s: 106.64849352836609\n",
      "  timers:\n",
      "    learn_throughput: 944.943\n",
      "    learn_time_ms: 4233.058\n",
      "    load_throughput: 3843841.73\n",
      "    load_time_ms: 1.041\n",
      "    sample_throughput: 639.586\n",
      "    sample_time_ms: 6254.048\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1671816596\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:00 (running for 00:02:00.79)<br>Memory usage on this node: 6.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         106.648</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-105.556</td><td style=\"text-align: right;\">             -83.991</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           942.343</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 969.3698630136986\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -78.04261612102256\n",
      "  episode_reward_mean: -104.63253795179156\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.056233882904053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012547197751700878\n",
      "          model: {}\n",
      "          policy_loss: -0.026586515828967094\n",
      "          total_loss: 1.4096653461456299\n",
      "          vf_explained_var: 0.09894546866416931\n",
      "          vf_loss: 1.433742642402649\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07000000000001\n",
      "    ram_util_percent: 21.56\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08833952842594225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203071558974575\n",
      "    mean_inference_ms: 0.5135835485338769\n",
      "    mean_raw_obs_processing_ms: 0.07179279683366986\n",
      "  time_since_restore: 113.32752537727356\n",
      "  time_this_iter_s: 6.679031848907471\n",
      "  time_total_s: 113.32752537727356\n",
      "  timers:\n",
      "    learn_throughput: 935.954\n",
      "    learn_time_ms: 4273.716\n",
      "    load_throughput: 3789149.22\n",
      "    load_time_ms: 1.056\n",
      "    sample_throughput: 639.244\n",
      "    sample_time_ms: 6257.39\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1671816603\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:06 (running for 00:02:06.52)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         113.328</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-104.633</td><td style=\"text-align: right;\">            -78.0426</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">            969.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 986.1866666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -78.04261612102256\n",
      "  episode_reward_mean: -104.18562738686231\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 75\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.963725566864014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015596229583024979\n",
      "          model: {}\n",
      "          policy_loss: -0.02193654328584671\n",
      "          total_loss: 0.8278713226318359\n",
      "          vf_explained_var: 0.13307681679725647\n",
      "          vf_loss: 0.8466886281967163\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.76666666666667\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883197828937449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202930766547736\n",
      "    mean_inference_ms: 0.5134139835389715\n",
      "    mean_raw_obs_processing_ms: 0.07172854829979203\n",
      "  time_since_restore: 119.6411440372467\n",
      "  time_this_iter_s: 6.3136186599731445\n",
      "  time_total_s: 119.6411440372467\n",
      "  timers:\n",
      "    learn_throughput: 935.439\n",
      "    learn_time_ms: 4276.067\n",
      "    load_throughput: 3778482.05\n",
      "    load_time_ms: 1.059\n",
      "    sample_throughput: 634.229\n",
      "    sample_time_ms: 6306.874\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1671816609\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:11 (running for 00:02:11.82)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         119.641</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-104.186</td><td style=\"text-align: right;\">            -78.0426</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           986.187</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 1009.7948717948718\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -78.04261612102256\n",
      "  episode_reward_mean: -103.44984294275586\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 78\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.865558624267578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01628044806420803\n",
      "          model: {}\n",
      "          policy_loss: -0.02020290493965149\n",
      "          total_loss: 1.0671919584274292\n",
      "          vf_explained_var: 0.1866530478000641\n",
      "          vf_loss: 1.0841387510299683\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.144444444444446\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08830203135033145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32028245689097035\n",
      "    mean_inference_ms: 0.5131696587288693\n",
      "    mean_raw_obs_processing_ms: 0.07164540869988056\n",
      "  time_since_restore: 125.94289374351501\n",
      "  time_this_iter_s: 6.3017497062683105\n",
      "  time_total_s: 125.94289374351501\n",
      "  timers:\n",
      "    learn_throughput: 935.144\n",
      "    learn_time_ms: 4277.415\n",
      "    load_throughput: 3792232.544\n",
      "    load_time_ms: 1.055\n",
      "    sample_throughput: 633.669\n",
      "    sample_time_ms: 6312.442\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1671816616\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:17 (running for 00:02:17.16)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         125.943</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -103.45</td><td style=\"text-align: right;\">            -78.0426</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           1009.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:22 (running for 00:02:22.16)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         125.943</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -103.45</td><td style=\"text-align: right;\">            -78.0426</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           1009.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 1024.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -78.04261612102256\n",
      "  episode_reward_mean: -102.91670263285671\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 80\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.826045036315918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011342574842274189\n",
      "          model: {}\n",
      "          policy_loss: -0.01837044209241867\n",
      "          total_loss: 1.095554232597351\n",
      "          vf_explained_var: -0.04764169827103615\n",
      "          vf_loss: 1.1116561889648438\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.35555555555555\n",
      "    ram_util_percent: 22.01111111111111\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08829026517346043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32030260612401207\n",
      "    mean_inference_ms: 0.5130572923032233\n",
      "    mean_raw_obs_processing_ms: 0.07159253223426616\n",
      "  time_since_restore: 132.26675271987915\n",
      "  time_this_iter_s: 6.323858976364136\n",
      "  time_total_s: 132.26675271987915\n",
      "  timers:\n",
      "    learn_throughput: 935.326\n",
      "    learn_time_ms: 4276.583\n",
      "    load_throughput: 3643814.695\n",
      "    load_time_ms: 1.098\n",
      "    sample_throughput: 632.828\n",
      "    sample_time_ms: 6320.835\n",
      "    update_time_ms: 1.708\n",
      "  timestamp: 1671816622\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:27 (running for 00:02:27.51)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         132.267</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-102.917</td><td style=\"text-align: right;\">            -78.0426</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           1024.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 1045.3493975903614\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -73.73051724980522\n",
      "  episode_reward_mean: -101.9606771555184\n",
      "  episode_reward_min: -125.64550395528546\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 83\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.729432106018066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016451548784971237\n",
      "          model: {}\n",
      "          policy_loss: -0.01875431463122368\n",
      "          total_loss: 1.1680885553359985\n",
      "          vf_explained_var: 0.118070088326931\n",
      "          vf_loss: 1.183552622795105\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.611111111111114\n",
      "    ram_util_percent: 22.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08826669243239942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203315307185182\n",
      "    mean_inference_ms: 0.5129100477479921\n",
      "    mean_raw_obs_processing_ms: 0.07151152435855895\n",
      "  time_since_restore: 138.49744582176208\n",
      "  time_this_iter_s: 6.230693101882935\n",
      "  time_total_s: 138.49744582176208\n",
      "  timers:\n",
      "    learn_throughput: 935.015\n",
      "    learn_time_ms: 4278.008\n",
      "    load_throughput: 3682120.973\n",
      "    load_time_ms: 1.086\n",
      "    sample_throughput: 632.838\n",
      "    sample_time_ms: 6320.733\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1671816628\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:32 (running for 00:02:32.73)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         138.497</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-101.961</td><td style=\"text-align: right;\">            -73.7305</td><td style=\"text-align: right;\">            -125.646</td><td style=\"text-align: right;\">           1045.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 1047.2674418604652\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -71.31383190269953\n",
      "  episode_reward_mean: -101.64580852941666\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 86\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.80612850189209\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00841265358030796\n",
      "          model: {}\n",
      "          policy_loss: -0.01834314875304699\n",
      "          total_loss: 21.60879898071289\n",
      "          vf_explained_var: 0.25819385051727295\n",
      "          vf_loss: 21.62546157836914\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.35555555555556\n",
      "    ram_util_percent: 22.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08824240580159518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32035062289770677\n",
      "    mean_inference_ms: 0.5127561508364704\n",
      "    mean_raw_obs_processing_ms: 0.07143302621445842\n",
      "  time_since_restore: 144.70869970321655\n",
      "  time_this_iter_s: 6.211253881454468\n",
      "  time_total_s: 144.70869970321655\n",
      "  timers:\n",
      "    learn_throughput: 934.939\n",
      "    learn_time_ms: 4278.355\n",
      "    load_throughput: 3546305.354\n",
      "    load_time_ms: 1.128\n",
      "    sample_throughput: 632.584\n",
      "    sample_time_ms: 6323.267\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1671816634\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:38 (running for 00:02:37.99)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         144.709</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-101.646</td><td style=\"text-align: right;\">            -71.3138</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1047.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 1055.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -68.67654774620566\n",
      "  episode_reward_mean: -100.80301981724539\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 90\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.759394645690918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011985072866082191\n",
      "          model: {}\n",
      "          policy_loss: -0.030410969629883766\n",
      "          total_loss: 46.23482894897461\n",
      "          vf_explained_var: 0.22080311179161072\n",
      "          vf_loss: 46.262840270996094\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.1\n",
      "    ram_util_percent: 22.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08821783431074776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32037086105639107\n",
      "    mean_inference_ms: 0.512527192966432\n",
      "    mean_raw_obs_processing_ms: 0.07133963996994958\n",
      "  time_since_restore: 150.9377794265747\n",
      "  time_this_iter_s: 6.229079723358154\n",
      "  time_total_s: 150.9377794265747\n",
      "  timers:\n",
      "    learn_throughput: 935.947\n",
      "    learn_time_ms: 4273.746\n",
      "    load_throughput: 3590552.583\n",
      "    load_time_ms: 1.114\n",
      "    sample_throughput: 632.413\n",
      "    sample_time_ms: 6324.983\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1671816641\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:43 (running for 00:02:43.21)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         150.938</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-100.803</td><td style=\"text-align: right;\">            -68.6765</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">              1055</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 1066.8478260869565\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -68.67654774620566\n",
      "  episode_reward_mean: -100.25642361851125\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 92\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.6793293952941895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010267348028719425\n",
      "          model: {}\n",
      "          policy_loss: -0.019951317459344864\n",
      "          total_loss: 1.1288803815841675\n",
      "          vf_explained_var: 0.30974724888801575\n",
      "          vf_loss: 1.1467781066894531\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.022222222222226\n",
      "    ram_util_percent: 22.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08820636180930612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32038682763620596\n",
      "    mean_inference_ms: 0.5124198223247471\n",
      "    mean_raw_obs_processing_ms: 0.07129425789231403\n",
      "  time_since_restore: 157.182767868042\n",
      "  time_this_iter_s: 6.244988441467285\n",
      "  time_total_s: 157.182767868042\n",
      "  timers:\n",
      "    learn_throughput: 937.013\n",
      "    learn_time_ms: 4268.886\n",
      "    load_throughput: 3623980.127\n",
      "    load_time_ms: 1.104\n",
      "    sample_throughput: 632.681\n",
      "    sample_time_ms: 6322.301\n",
      "    update_time_ms: 1.723\n",
      "  timestamp: 1671816647\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:48 (running for 00:02:48.49)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         157.183</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-100.256</td><td style=\"text-align: right;\">            -68.6765</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1066.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:53 (running for 00:02:53.50)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         157.183</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-100.256</td><td style=\"text-align: right;\">            -68.6765</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1066.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-30-53\n",
      "  done: false\n",
      "  episode_len_mean: 1078.1914893617022\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -65.7238838826139\n",
      "  episode_reward_mean: -99.56250970400357\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 94\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.5209760665893555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014213238842785358\n",
      "          model: {}\n",
      "          policy_loss: -0.028331246227025986\n",
      "          total_loss: 0.5754063725471497\n",
      "          vf_explained_var: 0.47236308455467224\n",
      "          vf_loss: 0.6008949875831604\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.51111111111111\n",
      "    ram_util_percent: 22.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08819530888534627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32040639134421406\n",
      "    mean_inference_ms: 0.5123179220679168\n",
      "    mean_raw_obs_processing_ms: 0.07125002734911552\n",
      "  time_since_restore: 163.43409180641174\n",
      "  time_this_iter_s: 6.251323938369751\n",
      "  time_total_s: 163.43409180641174\n",
      "  timers:\n",
      "    learn_throughput: 936.972\n",
      "    learn_time_ms: 4269.069\n",
      "    load_throughput: 3655485.445\n",
      "    load_time_ms: 1.094\n",
      "    sample_throughput: 632.819\n",
      "    sample_time_ms: 6320.927\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1671816653\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:30:58 (running for 00:02:58.78)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         163.434</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">-99.5625</td><td style=\"text-align: right;\">            -65.7239</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1078.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 1094.3298969072166\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -62.79402244450651\n",
      "  episode_reward_mean: -98.46022111033729\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 97\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.403848648071289\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01345518883317709\n",
      "          model: {}\n",
      "          policy_loss: -0.020581062883138657\n",
      "          total_loss: 0.795676589012146\n",
      "          vf_explained_var: 0.3905438780784607\n",
      "          vf_loss: 0.8135666251182556\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64444444444444\n",
      "    ram_util_percent: 22.022222222222222\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08817431116450257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32044448209529636\n",
      "    mean_inference_ms: 0.5121918947732738\n",
      "    mean_raw_obs_processing_ms: 0.0711838423375049\n",
      "  time_since_restore: 169.69644975662231\n",
      "  time_this_iter_s: 6.262357950210571\n",
      "  time_total_s: 169.69644975662231\n",
      "  timers:\n",
      "    learn_throughput: 936.577\n",
      "    learn_time_ms: 4270.871\n",
      "    load_throughput: 3700557.161\n",
      "    load_time_ms: 1.081\n",
      "    sample_throughput: 632.666\n",
      "    sample_time_ms: 6322.447\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1671816660\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:04 (running for 00:03:04.03)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         169.696</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-98.4602</td><td style=\"text-align: right;\">             -62.794</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1094.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-06\n",
      "  done: false\n",
      "  episode_len_mean: 1109.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -59.09859828475125\n",
      "  episode_reward_mean: -97.38045160573704\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 100\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.326533317565918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01756836287677288\n",
      "          model: {}\n",
      "          policy_loss: -0.023189380764961243\n",
      "          total_loss: 0.5684767365455627\n",
      "          vf_explained_var: 0.35760727524757385\n",
      "          vf_loss: 0.5881524085998535\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.611111111111114\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08816286238190232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3204683751955201\n",
      "    mean_inference_ms: 0.5120281659069003\n",
      "    mean_raw_obs_processing_ms: 0.0711260419339167\n",
      "  time_since_restore: 175.92558574676514\n",
      "  time_this_iter_s: 6.229135990142822\n",
      "  time_total_s: 175.92558574676514\n",
      "  timers:\n",
      "    learn_throughput: 945.726\n",
      "    learn_time_ms: 4229.555\n",
      "    load_throughput: 3797038.814\n",
      "    load_time_ms: 1.053\n",
      "    sample_throughput: 632.875\n",
      "    sample_time_ms: 6320.364\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1671816666\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:09 (running for 00:03:09.30)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         175.926</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-97.3805</td><td style=\"text-align: right;\">            -59.0986</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">            1109.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 1124.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -59.09859828475125\n",
      "  episode_reward_mean: -96.5257652550617\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 102\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.149298667907715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012953488156199455\n",
      "          model: {}\n",
      "          policy_loss: -0.013378625735640526\n",
      "          total_loss: 0.5389046669006348\n",
      "          vf_explained_var: 0.4351658225059509\n",
      "          vf_loss: 0.5496925115585327\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.022222222222226\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08812353860760719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3204351089745604\n",
      "    mean_inference_ms: 0.5114321649686635\n",
      "    mean_raw_obs_processing_ms: 0.07109358718015676\n",
      "  time_since_restore: 182.19892477989197\n",
      "  time_this_iter_s: 6.273339033126831\n",
      "  time_total_s: 182.19892477989197\n",
      "  timers:\n",
      "    learn_throughput: 945.677\n",
      "    learn_time_ms: 4229.772\n",
      "    load_throughput: 3744412.802\n",
      "    load_time_ms: 1.068\n",
      "    sample_throughput: 637.491\n",
      "    sample_time_ms: 6274.601\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1671816672\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:14 (running for 00:03:14.57)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         182.199</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">-96.5258</td><td style=\"text-align: right;\">            -59.0986</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1124.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 1140.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.66297147874337\n",
      "  episode_reward_mean: -95.19517193084558\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 104\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.139858245849609\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015605791471898556\n",
      "          model: {}\n",
      "          policy_loss: -0.02138504385948181\n",
      "          total_loss: 0.5984724164009094\n",
      "          vf_explained_var: 0.19251106679439545\n",
      "          vf_loss: 0.6167363524436951\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.08888888888889\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08805731524916162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203112754886135\n",
      "    mean_inference_ms: 0.5109588569017712\n",
      "    mean_raw_obs_processing_ms: 0.07100388532781567\n",
      "  time_since_restore: 188.47352838516235\n",
      "  time_this_iter_s: 6.274603605270386\n",
      "  time_total_s: 188.47352838516235\n",
      "  timers:\n",
      "    learn_throughput: 945.802\n",
      "    learn_time_ms: 4229.216\n",
      "    load_throughput: 3764408.544\n",
      "    load_time_ms: 1.063\n",
      "    sample_throughput: 637.669\n",
      "    sample_time_ms: 6272.848\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1671816678\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:19 (running for 00:03:19.88)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         188.474</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-95.1952</td><td style=\"text-align: right;\">             -46.663</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1140.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:24 (running for 00:03:24.89)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         188.474</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-95.1952</td><td style=\"text-align: right;\">             -46.663</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1140.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 1171.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46.66297147874337\n",
      "  episode_reward_mean: -93.59568333889723\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 107\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.134120941162109\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0146876759827137\n",
      "          model: {}\n",
      "          policy_loss: -0.02018466405570507\n",
      "          total_loss: 0.5085868835449219\n",
      "          vf_explained_var: 0.3224993348121643\n",
      "          vf_loss: 0.5258340239524841\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.900000000000006\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08800348715830492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202555593306409\n",
      "    mean_inference_ms: 0.5104318135936307\n",
      "    mean_raw_obs_processing_ms: 0.07087415034491841\n",
      "  time_since_restore: 194.72789549827576\n",
      "  time_this_iter_s: 6.254367113113403\n",
      "  time_total_s: 194.72789549827576\n",
      "  timers:\n",
      "    learn_throughput: 945.676\n",
      "    learn_time_ms: 4229.78\n",
      "    load_throughput: 3891181.0\n",
      "    load_time_ms: 1.028\n",
      "    sample_throughput: 638.438\n",
      "    sample_time_ms: 6265.294\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1671816685\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:30 (running for 00:03:30.17)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         194.728</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-93.5957</td><td style=\"text-align: right;\">             -46.663</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">              1171</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-31\n",
      "  done: false\n",
      "  episode_len_mean: 1217.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -41.52519579624876\n",
      "  episode_reward_mean: -91.81309835573323\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 110\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.143176078796387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017285246402025223\n",
      "          model: {}\n",
      "          policy_loss: -0.02535649761557579\n",
      "          total_loss: 0.7481164336204529\n",
      "          vf_explained_var: 0.12397495657205582\n",
      "          vf_loss: 0.7700158357620239\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.95555555555556\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08798377523952217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32024651781118135\n",
      "    mean_inference_ms: 0.5099439393747492\n",
      "    mean_raw_obs_processing_ms: 0.07075168109865632\n",
      "  time_since_restore: 200.97560501098633\n",
      "  time_this_iter_s: 6.247709512710571\n",
      "  time_total_s: 200.97560501098633\n",
      "  timers:\n",
      "    learn_throughput: 945.758\n",
      "    learn_time_ms: 4229.411\n",
      "    load_throughput: 3863314.528\n",
      "    load_time_ms: 1.035\n",
      "    sample_throughput: 638.215\n",
      "    sample_time_ms: 6267.477\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1671816691\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:35 (running for 00:03:35.41)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         200.976</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">-91.8131</td><td style=\"text-align: right;\">            -41.5252</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1217.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-37\n",
      "  done: false\n",
      "  episode_len_mean: 1232.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -41.52519579624876\n",
      "  episode_reward_mean: -90.545817950646\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 112\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.050921440124512\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017605522647500038\n",
      "          model: {}\n",
      "          policy_loss: -0.02758100815117359\n",
      "          total_loss: 0.41643407940864563\n",
      "          vf_explained_var: 0.19712138175964355\n",
      "          vf_loss: 0.44049400091171265\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.900000000000006\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08795711747668325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202462244384075\n",
      "    mean_inference_ms: 0.5096875028232518\n",
      "    mean_raw_obs_processing_ms: 0.07065073180005839\n",
      "  time_since_restore: 207.18859958648682\n",
      "  time_this_iter_s: 6.212994575500488\n",
      "  time_total_s: 207.18859958648682\n",
      "  timers:\n",
      "    learn_throughput: 945.832\n",
      "    learn_time_ms: 4229.082\n",
      "    load_throughput: 4020131.79\n",
      "    load_time_ms: 0.995\n",
      "    sample_throughput: 638.173\n",
      "    sample_time_ms: 6267.89\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1671816697\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:40 (running for 00:03:40.66)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         207.189</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-90.5458</td><td style=\"text-align: right;\">            -41.5252</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1232.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-43\n",
      "  done: false\n",
      "  episode_len_mean: 1263.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37.0856413499644\n",
      "  episode_reward_mean: -88.96229410310286\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 114\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.075223445892334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014315873384475708\n",
      "          model: {}\n",
      "          policy_loss: -0.02392864041030407\n",
      "          total_loss: 0.5152563452720642\n",
      "          vf_explained_var: 0.1593097746372223\n",
      "          vf_loss: 0.5363218188285828\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.56666666666667\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08792075135121065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32024898740687746\n",
      "    mean_inference_ms: 0.5094907991687765\n",
      "    mean_raw_obs_processing_ms: 0.07053251322291425\n",
      "  time_since_restore: 213.4222514629364\n",
      "  time_this_iter_s: 6.233651876449585\n",
      "  time_total_s: 213.4222514629364\n",
      "  timers:\n",
      "    learn_throughput: 945.801\n",
      "    learn_time_ms: 4229.221\n",
      "    load_throughput: 3850104.645\n",
      "    load_time_ms: 1.039\n",
      "    sample_throughput: 638.204\n",
      "    sample_time_ms: 6267.591\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1671816703\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:45 (running for 00:03:45.89)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         213.422</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-88.9623</td><td style=\"text-align: right;\">            -37.0856</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1263.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 1309.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.032033176709\n",
      "  episode_reward_mean: -86.7508708556427\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 117\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9507052898406982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01702706143260002\n",
      "          model: {}\n",
      "          policy_loss: -0.029691515490412712\n",
      "          total_loss: 0.4654120206832886\n",
      "          vf_explained_var: 0.2319677323102951\n",
      "          vf_loss: 0.49169814586639404\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.111111111111114\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08786328102129215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32026076512045853\n",
      "    mean_inference_ms: 0.5092251116332794\n",
      "    mean_raw_obs_processing_ms: 0.07035431280857511\n",
      "  time_since_restore: 219.72104477882385\n",
      "  time_this_iter_s: 6.298793315887451\n",
      "  time_total_s: 219.72104477882385\n",
      "  timers:\n",
      "    learn_throughput: 945.182\n",
      "    learn_time_ms: 4231.987\n",
      "    load_throughput: 3627819.92\n",
      "    load_time_ms: 1.103\n",
      "    sample_throughput: 637.884\n",
      "    sample_time_ms: 6270.735\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1671816710\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:51 (running for 00:03:51.23)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         219.721</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-86.7509</td><td style=\"text-align: right;\">             -36.032</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1309.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:31:56 (running for 00:03:56.24)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         219.721</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-86.7509</td><td style=\"text-align: right;\">             -36.032</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1309.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 1324.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -25.983555634621492\n",
      "  episode_reward_mean: -84.55787905372136\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.8789021968841553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017974689602851868\n",
      "          model: {}\n",
      "          policy_loss: -0.027225183323025703\n",
      "          total_loss: 0.5114206671714783\n",
      "          vf_explained_var: 0.17187879979610443\n",
      "          vf_loss: 0.5350509285926819\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.988888888888894\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0878292055488039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.320227398146245\n",
      "    mean_inference_ms: 0.5089417021223231\n",
      "    mean_raw_obs_processing_ms: 0.07024756580622697\n",
      "  time_since_restore: 225.94761991500854\n",
      "  time_this_iter_s: 6.226575136184692\n",
      "  time_total_s: 225.94761991500854\n",
      "  timers:\n",
      "    learn_throughput: 945.29\n",
      "    learn_time_ms: 4231.505\n",
      "    load_throughput: 3610643.481\n",
      "    load_time_ms: 1.108\n",
      "    sample_throughput: 637.824\n",
      "    sample_time_ms: 6271.323\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1671816716\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:01 (running for 00:04:01.48)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         225.948</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-84.5579</td><td style=\"text-align: right;\">            -25.9836</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1324.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 1340.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22.01111738647646\n",
      "  episode_reward_mean: -82.85340996584515\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 122\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.7430531978607178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01624983362853527\n",
      "          model: {}\n",
      "          policy_loss: -0.032297149300575256\n",
      "          total_loss: 0.6162409782409668\n",
      "          vf_explained_var: 0.0824027955532074\n",
      "          vf_loss: 0.6452882289886475\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.91111111111112\n",
      "    ram_util_percent: 22.166666666666664\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0878247320980988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202159633214\n",
      "    mean_inference_ms: 0.5087622980451163\n",
      "    mean_raw_obs_processing_ms: 0.07021035667568577\n",
      "  time_since_restore: 232.18227696418762\n",
      "  time_this_iter_s: 6.234657049179077\n",
      "  time_total_s: 232.18227696418762\n",
      "  timers:\n",
      "    learn_throughput: 945.288\n",
      "    learn_time_ms: 4231.516\n",
      "    load_throughput: 3495034.894\n",
      "    load_time_ms: 1.144\n",
      "    sample_throughput: 638.166\n",
      "    sample_time_ms: 6267.958\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1671816722\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:06 (running for 00:04:06.71)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         232.182</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-82.8534</td><td style=\"text-align: right;\">            -22.0111</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1340.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 1355.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -17.207103320685523\n",
      "  episode_reward_mean: -80.94483107403941\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 124\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.6454689502716064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01648136042058468\n",
      "          model: {}\n",
      "          policy_loss: -0.025614218786358833\n",
      "          total_loss: 0.875985860824585\n",
      "          vf_explained_var: 0.18415215611457825\n",
      "          vf_loss: 0.898303747177124\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.733333333333334\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08779340637576125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202311277895699\n",
      "    mean_inference_ms: 0.5086418816153153\n",
      "    mean_raw_obs_processing_ms: 0.07008382607654096\n",
      "  time_since_restore: 238.41523933410645\n",
      "  time_this_iter_s: 6.232962369918823\n",
      "  time_total_s: 238.41523933410645\n",
      "  timers:\n",
      "    learn_throughput: 944.862\n",
      "    learn_time_ms: 4233.422\n",
      "    load_throughput: 3502769.693\n",
      "    load_time_ms: 1.142\n",
      "    sample_throughput: 638.314\n",
      "    sample_time_ms: 6266.511\n",
      "    update_time_ms: 1.668\n",
      "  timestamp: 1671816728\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:12 (running for 00:04:11.99)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         238.415</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">-80.9448</td><td style=\"text-align: right;\">            -17.2071</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">            1355.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 1401.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.365108984111494\n",
      "  episode_reward_mean: -77.92002780272776\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 127\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.539142370223999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019679401069879532\n",
      "          model: {}\n",
      "          policy_loss: -0.02874567173421383\n",
      "          total_loss: 0.6950201988220215\n",
      "          vf_explained_var: 0.3659529685974121\n",
      "          vf_loss: 0.7198299169540405\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.355555555555554\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08774171733652464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32025103855024967\n",
      "    mean_inference_ms: 0.5084793282140976\n",
      "    mean_raw_obs_processing_ms: 0.06989133865090279\n",
      "  time_since_restore: 244.6124348640442\n",
      "  time_this_iter_s: 6.197195529937744\n",
      "  time_total_s: 244.6124348640442\n",
      "  timers:\n",
      "    learn_throughput: 944.899\n",
      "    learn_time_ms: 4233.257\n",
      "    load_throughput: 3481689.252\n",
      "    load_time_ms: 1.149\n",
      "    sample_throughput: 638.844\n",
      "    sample_time_ms: 6261.308\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1671816735\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:17 (running for 00:04:17.18)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         244.612</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  -77.92</td><td style=\"text-align: right;\">             8.36511</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1401.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 1446.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.654014368278256\n",
      "  episode_reward_mean: -74.33992823301487\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 130\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.44712233543396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020502088591456413\n",
      "          model: {}\n",
      "          policy_loss: -0.025623278692364693\n",
      "          total_loss: 0.9350083470344543\n",
      "          vf_explained_var: 0.38602742552757263\n",
      "          vf_loss: 0.9565311670303345\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.388888888888886\n",
      "    ram_util_percent: 22.188888888888886\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0877022623840124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202609093267419\n",
      "    mean_inference_ms: 0.508295753482276\n",
      "    mean_raw_obs_processing_ms: 0.06970586457316715\n",
      "  time_since_restore: 250.95565104484558\n",
      "  time_this_iter_s: 6.343216180801392\n",
      "  time_total_s: 250.95565104484558\n",
      "  timers:\n",
      "    learn_throughput: 945.26\n",
      "    learn_time_ms: 4231.642\n",
      "    load_throughput: 3432327.332\n",
      "    load_time_ms: 1.165\n",
      "    sample_throughput: 638.013\n",
      "    sample_time_ms: 6269.461\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1671816741\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:22 (running for 00:04:22.56)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         250.956</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-74.3399</td><td style=\"text-align: right;\">              12.654</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1446.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:27 (running for 00:04:27.57)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         250.956</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-74.3399</td><td style=\"text-align: right;\">              12.654</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1446.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 1462.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.35069172622824\n",
      "  episode_reward_mean: -71.68991880876985\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 132\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.355738401412964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615133136510849\n",
      "          model: {}\n",
      "          policy_loss: -0.023596681654453278\n",
      "          total_loss: 1.5008500814437866\n",
      "          vf_explained_var: 0.3882867693901062\n",
      "          vf_loss: 1.5212165117263794\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.03333333333333\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08769147071980501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202810188090428\n",
      "    mean_inference_ms: 0.5081883348066301\n",
      "    mean_raw_obs_processing_ms: 0.0696298020477918\n",
      "  time_since_restore: 257.20536708831787\n",
      "  time_this_iter_s: 6.24971604347229\n",
      "  time_total_s: 257.20536708831787\n",
      "  timers:\n",
      "    learn_throughput: 944.88\n",
      "    learn_time_ms: 4233.341\n",
      "    load_throughput: 3441833.214\n",
      "    load_time_ms: 1.162\n",
      "    sample_throughput: 638.389\n",
      "    sample_time_ms: 6265.774\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1671816747\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:32 (running for 00:04:32.85)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         257.205</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-71.6899</td><td style=\"text-align: right;\">             24.3507</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1462.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 1477.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 27.88887789489643\n",
      "  episode_reward_mean: -68.99343499980982\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 134\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.2590012550354004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019517716020345688\n",
      "          model: {}\n",
      "          policy_loss: -0.027942674234509468\n",
      "          total_loss: 0.7930856943130493\n",
      "          vf_explained_var: 0.3477388918399811\n",
      "          vf_loss: 0.8171248435974121\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.35\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768470379300496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203131271808091\n",
      "    mean_inference_ms: 0.5081097364663376\n",
      "    mean_raw_obs_processing_ms: 0.06956891298571803\n",
      "  time_since_restore: 263.4225137233734\n",
      "  time_this_iter_s: 6.217146635055542\n",
      "  time_total_s: 263.4225137233734\n",
      "  timers:\n",
      "    learn_throughput: 944.901\n",
      "    learn_time_ms: 4233.248\n",
      "    load_throughput: 3404812.988\n",
      "    load_time_ms: 1.175\n",
      "    sample_throughput: 638.456\n",
      "    sample_time_ms: 6265.116\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1671816754\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:38 (running for 00:04:38.06)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         263.423</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-68.9934</td><td style=\"text-align: right;\">             27.8889</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1477.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-40\n",
      "  done: false\n",
      "  episode_len_mean: 1477.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 47.31160857421595\n",
      "  episode_reward_mean: -64.70396911711491\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 137\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.178323984146118\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017454734072089195\n",
      "          model: {}\n",
      "          policy_loss: -0.020908204838633537\n",
      "          total_loss: 1.1090140342712402\n",
      "          vf_explained_var: 0.5400403141975403\n",
      "          vf_loss: 1.1264312267303467\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.42222222222223\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767575633690841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203331254848946\n",
      "    mean_inference_ms: 0.5080071162933473\n",
      "    mean_raw_obs_processing_ms: 0.06949732602472103\n",
      "  time_since_restore: 269.63870429992676\n",
      "  time_this_iter_s: 6.216190576553345\n",
      "  time_total_s: 269.63870429992676\n",
      "  timers:\n",
      "    learn_throughput: 944.659\n",
      "    learn_time_ms: 4234.33\n",
      "    load_throughput: 3373660.969\n",
      "    load_time_ms: 1.186\n",
      "    sample_throughput: 638.552\n",
      "    sample_time_ms: 6264.169\n",
      "    update_time_ms: 1.619\n",
      "  timestamp: 1671816760\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:43 (running for 00:04:43.32)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         269.639</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -64.704</td><td style=\"text-align: right;\">             47.3116</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1477.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 1493.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.33984423151367\n",
      "  episode_reward_mean: -60.26966257576341\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 140\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.009298324584961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018369564786553383\n",
      "          model: {}\n",
      "          policy_loss: -0.02908375859260559\n",
      "          total_loss: 1.3322762250900269\n",
      "          vf_explained_var: 0.36092931032180786\n",
      "          vf_loss: 1.357685923576355\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.81111111111111\n",
      "    ram_util_percent: 22.11111111111111\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766671595048109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203539165939677\n",
      "    mean_inference_ms: 0.5079233342879774\n",
      "    mean_raw_obs_processing_ms: 0.06941086915536653\n",
      "  time_since_restore: 275.8755042552948\n",
      "  time_this_iter_s: 6.236799955368042\n",
      "  time_total_s: 275.8755042552948\n",
      "  timers:\n",
      "    learn_throughput: 944.299\n",
      "    learn_time_ms: 4235.947\n",
      "    load_throughput: 3456440.388\n",
      "    load_time_ms: 1.157\n",
      "    sample_throughput: 638.557\n",
      "    sample_time_ms: 6264.119\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1671816766\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:48 (running for 00:04:48.55)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         275.876</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">-60.2697</td><td style=\"text-align: right;\">             49.3398</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 1493.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 49.33984423151367\n",
      "  episode_reward_mean: -57.25636387808469\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 142\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.016550064086914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020736612379550934\n",
      "          model: {}\n",
      "          policy_loss: -0.023000719025731087\n",
      "          total_loss: 1.176267385482788\n",
      "          vf_explained_var: 0.4983614683151245\n",
      "          vf_loss: 1.1951208114624023\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.46666666666667\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0876641275466196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203730839924432\n",
      "    mean_inference_ms: 0.5078860518752277\n",
      "    mean_raw_obs_processing_ms: 0.06936765713722845\n",
      "  time_since_restore: 282.1159725189209\n",
      "  time_this_iter_s: 6.240468263626099\n",
      "  time_total_s: 282.1159725189209\n",
      "  timers:\n",
      "    learn_throughput: 944.399\n",
      "    learn_time_ms: 4235.499\n",
      "    load_throughput: 3614299.317\n",
      "    load_time_ms: 1.107\n",
      "    sample_throughput: 638.952\n",
      "    sample_time_ms: 6260.249\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1671816772\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:53 (running for 00:04:53.83)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         282.116</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-57.2564</td><td style=\"text-align: right;\">             49.3398</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:32:58 (running for 00:04:58.83)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         282.116</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-57.2564</td><td style=\"text-align: right;\">             49.3398</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-32-59\n",
      "  done: false\n",
      "  episode_len_mean: 1493.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 54.763589788761664\n",
      "  episode_reward_mean: -54.33802675470178\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 144\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.8649802207946777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01465429738163948\n",
      "          model: {}\n",
      "          policy_loss: -0.02600852958858013\n",
      "          total_loss: 1.1912256479263306\n",
      "          vf_explained_var: 0.5964446067810059\n",
      "          vf_loss: 1.2143032550811768\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.72222222222222\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766329070839694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203930181200992\n",
      "    mean_inference_ms: 0.5078619431879905\n",
      "    mean_raw_obs_processing_ms: 0.0693346458255005\n",
      "  time_since_restore: 288.354674577713\n",
      "  time_this_iter_s: 6.238702058792114\n",
      "  time_total_s: 288.354674577713\n",
      "  timers:\n",
      "    learn_throughput: 943.915\n",
      "    learn_time_ms: 4237.668\n",
      "    load_throughput: 3518859.013\n",
      "    load_time_ms: 1.137\n",
      "    sample_throughput: 639.066\n",
      "    sample_time_ms: 6259.137\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1671816779\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:04 (running for 00:05:04.10)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         288.355</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -54.338</td><td style=\"text-align: right;\">             54.7636</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-05\n",
      "  done: false\n",
      "  episode_len_mean: 1493.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.196726671077236\n",
      "  episode_reward_mean: -49.62837386651622\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 148\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.9135756492614746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015298234298825264\n",
      "          model: {}\n",
      "          policy_loss: -0.02517312951385975\n",
      "          total_loss: 76.9236831665039\n",
      "          vf_explained_var: 0.24996121227741241\n",
      "          vf_loss: 76.94579315185547\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.711111111111116\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767222659745066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203848518411781\n",
      "    mean_inference_ms: 0.507772538346357\n",
      "    mean_raw_obs_processing_ms: 0.0692920353553197\n",
      "  time_since_restore: 294.5397367477417\n",
      "  time_this_iter_s: 6.1850621700286865\n",
      "  time_total_s: 294.5397367477417\n",
      "  timers:\n",
      "    learn_throughput: 943.971\n",
      "    learn_time_ms: 4237.419\n",
      "    load_throughput: 3645873.482\n",
      "    load_time_ms: 1.097\n",
      "    sample_throughput: 639.334\n",
      "    sample_time_ms: 6256.513\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1671816785\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:09 (running for 00:05:09.27)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          294.54</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-49.6284</td><td style=\"text-align: right;\">             58.1967</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 1493.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 58.196726671077236\n",
      "  episode_reward_mean: -44.95274335252751\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 151\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.807393789291382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014634872786700726\n",
      "          model: {}\n",
      "          policy_loss: -0.02270382270216942\n",
      "          total_loss: 1.9731619358062744\n",
      "          vf_explained_var: 0.48563703894615173\n",
      "          vf_loss: 1.9929389953613281\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.03333333333333\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766782530780322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.320376898590971\n",
      "    mean_inference_ms: 0.5077304431402104\n",
      "    mean_raw_obs_processing_ms: 0.06924734592685701\n",
      "  time_since_restore: 300.73238015174866\n",
      "  time_this_iter_s: 6.192643404006958\n",
      "  time_total_s: 300.73238015174866\n",
      "  timers:\n",
      "    learn_throughput: 943.796\n",
      "    learn_time_ms: 4238.206\n",
      "    load_throughput: 3632139.594\n",
      "    load_time_ms: 1.101\n",
      "    sample_throughput: 639.822\n",
      "    sample_time_ms: 6251.737\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1671816791\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:14 (running for 00:05:14.52)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         300.732</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-44.9527</td><td style=\"text-align: right;\">             58.1967</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-17\n",
      "  done: false\n",
      "  episode_len_mean: 1493.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 61.41466772591825\n",
      "  episode_reward_mean: -41.86654138215867\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 153\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7678608894348145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017175907269120216\n",
      "          model: {}\n",
      "          policy_loss: -0.02367447316646576\n",
      "          total_loss: 1.036445140838623\n",
      "          vf_explained_var: 0.6352463960647583\n",
      "          vf_loss: 1.0566843748092651\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.988888888888894\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766701641241166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32036952284605136\n",
      "    mean_inference_ms: 0.5077089148774061\n",
      "    mean_raw_obs_processing_ms: 0.06922147346688384\n",
      "  time_since_restore: 306.9439218044281\n",
      "  time_this_iter_s: 6.211541652679443\n",
      "  time_total_s: 306.9439218044281\n",
      "  timers:\n",
      "    learn_throughput: 943.706\n",
      "    learn_time_ms: 4238.606\n",
      "    load_throughput: 3690707.026\n",
      "    load_time_ms: 1.084\n",
      "    sample_throughput: 639.641\n",
      "    sample_time_ms: 6253.507\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1671816797\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:19 (running for 00:05:19.72)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         306.944</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-41.8665</td><td style=\"text-align: right;\">             61.4147</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1493.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 1524.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 61.41466772591825\n",
      "  episode_reward_mean: -38.737129195633536\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 155\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7678558826446533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014752203598618507\n",
      "          model: {}\n",
      "          policy_loss: -0.02203713357448578\n",
      "          total_loss: 1.317419171333313\n",
      "          vf_explained_var: 0.5553975701332092\n",
      "          vf_loss: 1.3365058898925781\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.455555555555556\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766626321571783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32035825830575293\n",
      "    mean_inference_ms: 0.507692471874003\n",
      "    mean_raw_obs_processing_ms: 0.06919680838819273\n",
      "  time_since_restore: 313.1534478664398\n",
      "  time_this_iter_s: 6.209526062011719\n",
      "  time_total_s: 313.1534478664398\n",
      "  timers:\n",
      "    learn_throughput: 943.539\n",
      "    learn_time_ms: 4239.358\n",
      "    load_throughput: 3713005.643\n",
      "    load_time_ms: 1.077\n",
      "    sample_throughput: 641.011\n",
      "    sample_time_ms: 6240.141\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1671816803\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:24 (running for 00:05:24.97)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         313.153</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-38.7371</td><td style=\"text-align: right;\">             61.4147</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">            1524.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:30 (running for 00:05:29.98)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         313.153</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-38.7371</td><td style=\"text-align: right;\">             61.4147</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">            1524.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 1539.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 61.551451826769856\n",
      "  episode_reward_mean: -33.84935219887304\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 158\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7201650142669678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01868390664458275\n",
      "          model: {}\n",
      "          policy_loss: -0.031237497925758362\n",
      "          total_loss: 1.2586733102798462\n",
      "          vf_explained_var: 0.5899474620819092\n",
      "          vf_loss: 1.286173939704895\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84444444444444\n",
      "    ram_util_percent: 22.1\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766884828629701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203540019561293\n",
      "    mean_inference_ms: 0.5076896129340753\n",
      "    mean_raw_obs_processing_ms: 0.06916772547176232\n",
      "  time_since_restore: 319.405503988266\n",
      "  time_this_iter_s: 6.252056121826172\n",
      "  time_total_s: 319.405503988266\n",
      "  timers:\n",
      "    learn_throughput: 944.111\n",
      "    learn_time_ms: 4236.79\n",
      "    load_throughput: 3698436.17\n",
      "    load_time_ms: 1.082\n",
      "    sample_throughput: 640.665\n",
      "    sample_time_ms: 6243.511\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1671816810\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:35 (running for 00:05:35.26)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         319.406</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-33.8494</td><td style=\"text-align: right;\">             61.5515</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1539.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 1539.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 65.32448174328037\n",
      "  episode_reward_mean: -29.161712624717193\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 161\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.670811891555786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017008623108267784\n",
      "          model: {}\n",
      "          policy_loss: -0.027855489403009415\n",
      "          total_loss: 0.8931689262390137\n",
      "          vf_explained_var: 0.6478950381278992\n",
      "          vf_loss: 0.9176226854324341\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.611111111111114\n",
      "    ram_util_percent: 22.166666666666664\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768071111968245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32033817256671776\n",
      "    mean_inference_ms: 0.5076589012589396\n",
      "    mean_raw_obs_processing_ms: 0.06914606514113307\n",
      "  time_since_restore: 325.6378688812256\n",
      "  time_this_iter_s: 6.232364892959595\n",
      "  time_total_s: 325.6378688812256\n",
      "  timers:\n",
      "    learn_throughput: 944.104\n",
      "    learn_time_ms: 4236.823\n",
      "    load_throughput: 3695259.24\n",
      "    load_time_ms: 1.082\n",
      "    sample_throughput: 640.756\n",
      "    sample_time_ms: 6242.625\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1671816816\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:40 (running for 00:05:40.48)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         325.638</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-29.1617</td><td style=\"text-align: right;\">             65.3245</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1539.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 1539.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 80.13907978402577\n",
      "  episode_reward_mean: -25.84346662223099\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 163\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.569742202758789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01608564890921116\n",
      "          model: {}\n",
      "          policy_loss: -0.023057473823428154\n",
      "          total_loss: 1.176178216934204\n",
      "          vf_explained_var: 0.6723731160163879\n",
      "          vf_loss: 1.1960185766220093\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.737500000000004\n",
      "    ram_util_percent: 22.1125\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768292911255827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32032596796640994\n",
      "    mean_inference_ms: 0.5076683975316061\n",
      "    mean_raw_obs_processing_ms: 0.06912972582368648\n",
      "  time_since_restore: 331.79292583465576\n",
      "  time_this_iter_s: 6.155056953430176\n",
      "  time_total_s: 331.79292583465576\n",
      "  timers:\n",
      "    learn_throughput: 944.652\n",
      "    learn_time_ms: 4234.364\n",
      "    load_throughput: 3726862.296\n",
      "    load_time_ms: 1.073\n",
      "    sample_throughput: 641.128\n",
      "    sample_time_ms: 6239.001\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1671816822\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:45 (running for 00:05:45.69)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         331.793</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-25.8435</td><td style=\"text-align: right;\">             80.1391</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1539.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 1539.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 80.13907978402577\n",
      "  episode_reward_mean: -22.486864970563094\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 165\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.667537212371826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018682045862078667\n",
      "          model: {}\n",
      "          policy_loss: -0.027474420145154\n",
      "          total_loss: 1.4568471908569336\n",
      "          vf_explained_var: 0.5869460701942444\n",
      "          vf_loss: 1.4805853366851807\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.93333333333333\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768682104142814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3203122318406226\n",
      "    mean_inference_ms: 0.5076840415169755\n",
      "    mean_raw_obs_processing_ms: 0.06911488986409799\n",
      "  time_since_restore: 338.03460597991943\n",
      "  time_this_iter_s: 6.241680145263672\n",
      "  time_total_s: 338.03460597991943\n",
      "  timers:\n",
      "    learn_throughput: 944.899\n",
      "    learn_time_ms: 4233.256\n",
      "    load_throughput: 3732749.522\n",
      "    load_time_ms: 1.072\n",
      "    sample_throughput: 641.198\n",
      "    sample_time_ms: 6238.32\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1671816828\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:50 (running for 00:05:50.92)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         338.035</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-22.4869</td><td style=\"text-align: right;\">             80.1391</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1539.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-33-55\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 93.46700640210497\n",
      "  episode_reward_mean: -17.173550528344176\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.519136667251587\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015298551879823208\n",
      "          model: {}\n",
      "          policy_loss: -0.022592667490243912\n",
      "          total_loss: 1.8697651624679565\n",
      "          vf_explained_var: 0.565716028213501\n",
      "          vf_loss: 1.8892982006072998\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.711111111111116\n",
      "    ram_util_percent: 22.144444444444442\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768488660193448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32029943550611806\n",
      "    mean_inference_ms: 0.5077556634349392\n",
      "    mean_raw_obs_processing_ms: 0.06908810099957777\n",
      "  time_since_restore: 344.21069145202637\n",
      "  time_this_iter_s: 6.176085472106934\n",
      "  time_total_s: 344.21069145202637\n",
      "  timers:\n",
      "    learn_throughput: 944.858\n",
      "    learn_time_ms: 4233.441\n",
      "    load_throughput: 3762720.014\n",
      "    load_time_ms: 1.063\n",
      "    sample_throughput: 641.987\n",
      "    sample_time_ms: 6230.652\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1671816835\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:33:56 (running for 00:05:56.14)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         344.211</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-17.1736</td><td style=\"text-align: right;\">              93.467</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:01 (running for 00:06:01.14)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         344.211</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-17.1736</td><td style=\"text-align: right;\">              93.467</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-01\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 93.46700640210497\n",
      "  episode_reward_mean: -12.18085206060269\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 171\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.5125021934509277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017615333199501038\n",
      "          model: {}\n",
      "          policy_loss: -0.024079903960227966\n",
      "          total_loss: 2.126746892929077\n",
      "          vf_explained_var: 0.6378647685050964\n",
      "          vf_loss: 2.147303581237793\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.55555555555555\n",
      "    ram_util_percent: 22.188888888888886\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08769982540317182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202457664900531\n",
      "    mean_inference_ms: 0.5077376479539432\n",
      "    mean_raw_obs_processing_ms: 0.06907506787871791\n",
      "  time_since_restore: 350.5281779766083\n",
      "  time_this_iter_s: 6.317486524581909\n",
      "  time_total_s: 350.5281779766083\n",
      "  timers:\n",
      "    learn_throughput: 943.102\n",
      "    learn_time_ms: 4241.323\n",
      "    load_throughput: 3869730.366\n",
      "    load_time_ms: 1.034\n",
      "    sample_throughput: 641.976\n",
      "    sample_time_ms: 6230.758\n",
      "    update_time_ms: 1.691\n",
      "  timestamp: 1671816841\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:06 (running for 00:06:06.49)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         350.528</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-12.1809</td><td style=\"text-align: right;\">              93.467</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-07\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.18910911667994\n",
      "  episode_reward_mean: -8.497616550158453\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 173\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.4561827182769775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016429683193564415\n",
      "          model: {}\n",
      "          policy_loss: -0.024742910638451576\n",
      "          total_loss: 2.0931057929992676\n",
      "          vf_explained_var: 0.6077880263328552\n",
      "          vf_loss: 2.114562749862671\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.577777777777776\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08770226814463557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3202075908822906\n",
      "    mean_inference_ms: 0.5077470396565532\n",
      "    mean_raw_obs_processing_ms: 0.06906223184653731\n",
      "  time_since_restore: 356.72740602493286\n",
      "  time_this_iter_s: 6.199228048324585\n",
      "  time_total_s: 356.72740602493286\n",
      "  timers:\n",
      "    learn_throughput: 942.846\n",
      "    learn_time_ms: 4242.474\n",
      "    load_throughput: 3870712.44\n",
      "    load_time_ms: 1.033\n",
      "    sample_throughput: 641.124\n",
      "    sample_time_ms: 6239.047\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1671816847\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:11 (running for 00:06:11.68)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         356.727</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-8.49762</td><td style=\"text-align: right;\">             102.189</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 102.18910911667994\n",
      "  episode_reward_mean: -4.814706611194834\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 175\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.47151255607605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01663992553949356\n",
      "          model: {}\n",
      "          policy_loss: -0.0220294501632452\n",
      "          total_loss: 1.5958036184310913\n",
      "          vf_explained_var: 0.6473569273948669\n",
      "          vf_loss: 1.6145050525665283\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.95555555555555\n",
      "    ram_util_percent: 22.144444444444442\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08770187122497354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32015007298959175\n",
      "    mean_inference_ms: 0.5077402760215548\n",
      "    mean_raw_obs_processing_ms: 0.06904843582268183\n",
      "  time_since_restore: 362.9343605041504\n",
      "  time_this_iter_s: 6.206954479217529\n",
      "  time_total_s: 362.9343605041504\n",
      "  timers:\n",
      "    learn_throughput: 943.158\n",
      "    learn_time_ms: 4241.073\n",
      "    load_throughput: 3826570.568\n",
      "    load_time_ms: 1.045\n",
      "    sample_throughput: 640.71\n",
      "    sample_time_ms: 6243.07\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1671816853\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:16 (running for 00:06:16.93)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         362.934</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-4.81471</td><td style=\"text-align: right;\">             102.189</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.38122016854042\n",
      "  episode_reward_mean: 0.550987442791384\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 178\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3414409160614014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01842312514781952\n",
      "          model: {}\n",
      "          policy_loss: -0.018493831157684326\n",
      "          total_loss: 2.510244131088257\n",
      "          vf_explained_var: 0.656004011631012\n",
      "          vf_loss: 2.5250532627105713\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.12222222222222\n",
      "    ram_util_percent: 22.188888888888886\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768934831642201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32005954385939894\n",
      "    mean_inference_ms: 0.507755522377717\n",
      "    mean_raw_obs_processing_ms: 0.06901977416271947\n",
      "  time_since_restore: 369.15849590301514\n",
      "  time_this_iter_s: 6.224135398864746\n",
      "  time_total_s: 369.15849590301514\n",
      "  timers:\n",
      "    learn_throughput: 942.653\n",
      "    learn_time_ms: 4243.344\n",
      "    load_throughput: 3794033.469\n",
      "    load_time_ms: 1.054\n",
      "    sample_throughput: 640.989\n",
      "    sample_time_ms: 6240.359\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1671816860\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:22 (running for 00:06:22.15)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         369.158</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">0.550987</td><td style=\"text-align: right;\">             106.381</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-26\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.38122016854042\n",
      "  episode_reward_mean: 5.8052545762566705\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 181\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.3234264850616455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01764369010925293\n",
      "          model: {}\n",
      "          policy_loss: -0.025295550003647804\n",
      "          total_loss: 1.9944006204605103\n",
      "          vf_explained_var: 0.6982313990592957\n",
      "          vf_loss: 2.016167402267456\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.29999999999999\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768932356318261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31990971578895755\n",
      "    mean_inference_ms: 0.5076547724592944\n",
      "    mean_raw_obs_processing_ms: 0.06899892980673927\n",
      "  time_since_restore: 375.33921337127686\n",
      "  time_this_iter_s: 6.180717468261719\n",
      "  time_total_s: 375.33921337127686\n",
      "  timers:\n",
      "    learn_throughput: 942.675\n",
      "    learn_time_ms: 4243.245\n",
      "    load_throughput: 3751110.316\n",
      "    load_time_ms: 1.066\n",
      "    sample_throughput: 641.067\n",
      "    sample_time_ms: 6239.594\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1671816866\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:27 (running for 00:06:27.37)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         375.339</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> 5.80525</td><td style=\"text-align: right;\">             106.381</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:32 (running for 00:06:32.37)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         375.339</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> 5.80525</td><td style=\"text-align: right;\">             106.381</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 1555.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.38122016854042\n",
      "  episode_reward_mean: 9.291125895702551\n",
      "  episode_reward_min: -127.98035087673117\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 183\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.167717218399048\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018386106938123703\n",
      "          model: {}\n",
      "          policy_loss: -0.02489362098276615\n",
      "          total_loss: 2.3401167392730713\n",
      "          vf_explained_var: 0.6952233910560608\n",
      "          vf_loss: 2.3613333702087402\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.833333333333336\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768400661704387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31981753599915663\n",
      "    mean_inference_ms: 0.5076218679160795\n",
      "    mean_raw_obs_processing_ms: 0.0689824966294892\n",
      "  time_since_restore: 381.56228852272034\n",
      "  time_this_iter_s: 6.2230751514434814\n",
      "  time_total_s: 381.56228852272034\n",
      "  timers:\n",
      "    learn_throughput: 942.067\n",
      "    learn_time_ms: 4245.982\n",
      "    load_throughput: 3761117.313\n",
      "    load_time_ms: 1.064\n",
      "    sample_throughput: 641.66\n",
      "    sample_time_ms: 6233.834\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1671816872\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:37 (running for 00:06:37.62)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         381.562</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> 9.29113</td><td style=\"text-align: right;\">             106.381</td><td style=\"text-align: right;\">             -127.98</td><td style=\"text-align: right;\">           1555.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 1570.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 115.63895268679263\n",
      "  episode_reward_mean: 13.677251114916457\n",
      "  episode_reward_min: -108.29767615103164\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 185\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0794057846069336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018527600914239883\n",
      "          model: {}\n",
      "          policy_loss: -0.026610208675265312\n",
      "          total_loss: 1.7842766046524048\n",
      "          vf_explained_var: 0.7412393093109131\n",
      "          vf_loss: 1.8071812391281128\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.7875\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0876887874564124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3197128318351712\n",
      "    mean_inference_ms: 0.5075503868744586\n",
      "    mean_raw_obs_processing_ms: 0.06897192072471657\n",
      "  time_since_restore: 387.89091658592224\n",
      "  time_this_iter_s: 6.328628063201904\n",
      "  time_total_s: 387.89091658592224\n",
      "  timers:\n",
      "    learn_throughput: 938.574\n",
      "    learn_time_ms: 4261.782\n",
      "    load_throughput: 3716871.815\n",
      "    load_time_ms: 1.076\n",
      "    sample_throughput: 642.037\n",
      "    sample_time_ms: 6230.169\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1671816878\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:42 (running for 00:06:42.94)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         387.891</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 13.6773</td><td style=\"text-align: right;\">             115.639</td><td style=\"text-align: right;\">            -108.298</td><td style=\"text-align: right;\">           1570.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-45\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 116.53638200174446\n",
      "  episode_reward_mean: 19.552030946431344\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 188\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0519678592681885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018491655588150024\n",
      "          model: {}\n",
      "          policy_loss: -0.02184155210852623\n",
      "          total_loss: 2.3210480213165283\n",
      "          vf_explained_var: 0.7104918360710144\n",
      "          vf_loss: 2.3391916751861572\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.55555555555556\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768383919152291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3195665619491758\n",
      "    mean_inference_ms: 0.5075108763691066\n",
      "    mean_raw_obs_processing_ms: 0.0689502327855424\n",
      "  time_since_restore: 394.09382700920105\n",
      "  time_this_iter_s: 6.202910423278809\n",
      "  time_total_s: 394.09382700920105\n",
      "  timers:\n",
      "    learn_throughput: 938.188\n",
      "    learn_time_ms: 4263.538\n",
      "    load_throughput: 3730093.824\n",
      "    load_time_ms: 1.072\n",
      "    sample_throughput: 640.072\n",
      "    sample_time_ms: 6249.292\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1671816885\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:48 (running for 00:06:48.19)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         394.094</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  19.552</td><td style=\"text-align: right;\">             116.536</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-51\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 123.75832014023563\n",
      "  episode_reward_mean: 25.1721345908049\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 191\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0434775352478027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017398664727807045\n",
      "          model: {}\n",
      "          policy_loss: -0.017797963693737984\n",
      "          total_loss: 1.721159815788269\n",
      "          vf_explained_var: 0.7598265409469604\n",
      "          vf_loss: 1.7354779243469238\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.800000000000004\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767685048406675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31940592443301824\n",
      "    mean_inference_ms: 0.507471245896323\n",
      "    mean_raw_obs_processing_ms: 0.06892755225195482\n",
      "  time_since_restore: 400.3141417503357\n",
      "  time_this_iter_s: 6.2203147411346436\n",
      "  time_total_s: 400.3141417503357\n",
      "  timers:\n",
      "    learn_throughput: 937.912\n",
      "    learn_time_ms: 4264.795\n",
      "    load_throughput: 3762720.014\n",
      "    load_time_ms: 1.063\n",
      "    sample_throughput: 640.253\n",
      "    sample_time_ms: 6247.531\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1671816891\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:53 (running for 00:06:53.41)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         400.314</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> 25.1721</td><td style=\"text-align: right;\">             123.758</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-34-57\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 123.75832014023563\n",
      "  episode_reward_mean: 28.90521730354901\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 193\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.0044479370117188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0197888370603323\n",
      "          model: {}\n",
      "          policy_loss: -0.022660456597805023\n",
      "          total_loss: 2.114398717880249\n",
      "          vf_explained_var: 0.7790555953979492\n",
      "          vf_loss: 2.13310170173645\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.18888888888889\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767334588491454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3192945511902581\n",
      "    mean_inference_ms: 0.5074455876124945\n",
      "    mean_raw_obs_processing_ms: 0.06891413541936312\n",
      "  time_since_restore: 406.5505497455597\n",
      "  time_this_iter_s: 6.236407995223999\n",
      "  time_total_s: 406.5505497455597\n",
      "  timers:\n",
      "    learn_throughput: 938.089\n",
      "    learn_time_ms: 4263.989\n",
      "    load_throughput: 3689002.836\n",
      "    load_time_ms: 1.084\n",
      "    sample_throughput: 639.396\n",
      "    sample_time_ms: 6255.906\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1671816897\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:34:58 (running for 00:06:58.69)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         406.551</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> 28.9052</td><td style=\"text-align: right;\">             123.758</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:03 (running for 00:07:03.70)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         406.551</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> 28.9052</td><td style=\"text-align: right;\">             123.758</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-03\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 123.75832014023563\n",
      "  episode_reward_mean: 32.40169491703493\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 195\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8838040828704834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018966548144817352\n",
      "          model: {}\n",
      "          policy_loss: -0.024374568834900856\n",
      "          total_loss: 2.2215092182159424\n",
      "          vf_explained_var: 0.6909325122833252\n",
      "          vf_loss: 2.242090940475464\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.144444444444446\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766999408851556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31917828960126077\n",
      "    mean_inference_ms: 0.5074208808822565\n",
      "    mean_raw_obs_processing_ms: 0.06890107671912077\n",
      "  time_since_restore: 412.8138029575348\n",
      "  time_this_iter_s: 6.263253211975098\n",
      "  time_total_s: 412.8138029575348\n",
      "  timers:\n",
      "    learn_throughput: 939.529\n",
      "    learn_time_ms: 4257.454\n",
      "    load_throughput: 3692087.762\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 639.318\n",
      "    sample_time_ms: 6256.671\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1671816903\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:09 (running for 00:07:08.99)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         412.814</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> 32.4017</td><td style=\"text-align: right;\">             123.758</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 128.25306119199035\n",
      "  episode_reward_mean: 38.050663699798655\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 198\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8604434728622437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018509436398744583\n",
      "          model: {}\n",
      "          policy_loss: -0.025587067008018494\n",
      "          total_loss: 2.0098073482513428\n",
      "          vf_explained_var: 0.7242030501365662\n",
      "          vf_loss: 2.0316925048828125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.8\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766714357296067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3190021453467532\n",
      "    mean_inference_ms: 0.5073896053162995\n",
      "    mean_raw_obs_processing_ms: 0.06888353535815163\n",
      "  time_since_restore: 419.0521357059479\n",
      "  time_this_iter_s: 6.238332748413086\n",
      "  time_total_s: 419.0521357059479\n",
      "  timers:\n",
      "    learn_throughput: 939.789\n",
      "    learn_time_ms: 4256.276\n",
      "    load_throughput: 3658275.223\n",
      "    load_time_ms: 1.093\n",
      "    sample_throughput: 639.472\n",
      "    sample_time_ms: 6255.162\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1671816910\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:14 (running for 00:07:14.22)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         419.052</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> 38.0507</td><td style=\"text-align: right;\">             128.253</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-16\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 139.90959892308334\n",
      "  episode_reward_mean: 43.67661011173016\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 201\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8749794960021973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021059125661849976\n",
      "          model: {}\n",
      "          policy_loss: -0.02348347008228302\n",
      "          total_loss: 2.4576053619384766\n",
      "          vf_explained_var: 0.6108849048614502\n",
      "          vf_loss: 2.476876974105835\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.24444444444445\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766308893055282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3188186817897416\n",
      "    mean_inference_ms: 0.5073579565630424\n",
      "    mean_raw_obs_processing_ms: 0.06886505147938192\n",
      "  time_since_restore: 425.3240602016449\n",
      "  time_this_iter_s: 6.2719244956970215\n",
      "  time_total_s: 425.3240602016449\n",
      "  timers:\n",
      "    learn_throughput: 939.61\n",
      "    learn_time_ms: 4257.086\n",
      "    load_throughput: 3696969.216\n",
      "    load_time_ms: 1.082\n",
      "    sample_throughput: 638.979\n",
      "    sample_time_ms: 6259.985\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1671816916\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:19 (running for 00:07:19.54)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         425.324</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> 43.6766</td><td style=\"text-align: right;\">              139.91</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 139.90959892308334\n",
      "  episode_reward_mean: 47.165209839357345\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 203\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8213542699813843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020204197615385056\n",
      "          model: {}\n",
      "          policy_loss: -0.024396749213337898\n",
      "          total_loss: 2.1872122287750244\n",
      "          vf_explained_var: 0.734703540802002\n",
      "          vf_loss: 2.2075681686401367\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.06666666666666\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765995080905889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31868689171979336\n",
      "    mean_inference_ms: 0.5073327975630055\n",
      "    mean_raw_obs_processing_ms: 0.06885285748100788\n",
      "  time_since_restore: 431.53829169273376\n",
      "  time_this_iter_s: 6.214231491088867\n",
      "  time_total_s: 431.53829169273376\n",
      "  timers:\n",
      "    learn_throughput: 939.661\n",
      "    learn_time_ms: 4256.856\n",
      "    load_throughput: 3703579.691\n",
      "    load_time_ms: 1.08\n",
      "    sample_throughput: 638.917\n",
      "    sample_time_ms: 6260.592\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1671816922\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:24 (running for 00:07:24.75)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         431.538</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> 47.1652</td><td style=\"text-align: right;\">              139.91</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-28\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 139.90959892308334\n",
      "  episode_reward_mean: 50.88908243934396\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 205\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7097615003585815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023235542699694633\n",
      "          model: {}\n",
      "          policy_loss: -0.028683258220553398\n",
      "          total_loss: 1.8819326162338257\n",
      "          vf_explained_var: 0.7688300013542175\n",
      "          vf_loss: 1.9059687852859497\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.333333333333336\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765628655953268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3185472334561219\n",
      "    mean_inference_ms: 0.5073051793214853\n",
      "    mean_raw_obs_processing_ms: 0.06884018746589915\n",
      "  time_since_restore: 437.72668385505676\n",
      "  time_this_iter_s: 6.188392162322998\n",
      "  time_total_s: 437.72668385505676\n",
      "  timers:\n",
      "    learn_throughput: 939.324\n",
      "    learn_time_ms: 4258.383\n",
      "    load_throughput: 3705461.051\n",
      "    load_time_ms: 1.079\n",
      "    sample_throughput: 639.002\n",
      "    sample_time_ms: 6259.764\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1671816928\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:30 (running for 00:07:29.99)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         437.727</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> 50.8891</td><td style=\"text-align: right;\">              139.91</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:35 (running for 00:07:34.99)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         437.727</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> 50.8891</td><td style=\"text-align: right;\">              139.91</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 139.90959892308334\n",
      "  episode_reward_mean: 56.29127113622545\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 208\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7135632038116455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019921600818634033\n",
      "          model: {}\n",
      "          policy_loss: -0.022566331550478935\n",
      "          total_loss: 3.0583314895629883\n",
      "          vf_explained_var: 0.7637818455696106\n",
      "          vf_loss: 3.076913595199585\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.06666666666666\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765214499112467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31833732122666075\n",
      "    mean_inference_ms: 0.5072684735104732\n",
      "    mean_raw_obs_processing_ms: 0.06882271877062339\n",
      "  time_since_restore: 443.9957630634308\n",
      "  time_this_iter_s: 6.269079208374023\n",
      "  time_total_s: 443.9957630634308\n",
      "  timers:\n",
      "    learn_throughput: 938.903\n",
      "    learn_time_ms: 4260.292\n",
      "    load_throughput: 3731836.199\n",
      "    load_time_ms: 1.072\n",
      "    sample_throughput: 638.551\n",
      "    sample_time_ms: 6264.18\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1671816935\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:40 (running for 00:07:40.28)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         443.996</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> 56.2913</td><td style=\"text-align: right;\">              139.91</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-41\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 146.44794694091448\n",
      "  episode_reward_mean: 61.902993809823435\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 211\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6942790746688843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018128881230950356\n",
      "          model: {}\n",
      "          policy_loss: -0.0234367772936821\n",
      "          total_loss: 3.5720713138580322\n",
      "          vf_explained_var: 0.6732159852981567\n",
      "          vf_loss: 3.5918824672698975\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.58888888888889\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764655614117407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3181261813229456\n",
      "    mean_inference_ms: 0.5072304453573175\n",
      "    mean_raw_obs_processing_ms: 0.06880515565024829\n",
      "  time_since_restore: 450.25321102142334\n",
      "  time_this_iter_s: 6.257447957992554\n",
      "  time_total_s: 450.25321102142334\n",
      "  timers:\n",
      "    learn_throughput: 941.987\n",
      "    learn_time_ms: 4246.343\n",
      "    load_throughput: 3818036.503\n",
      "    load_time_ms: 1.048\n",
      "    sample_throughput: 637.679\n",
      "    sample_time_ms: 6272.751\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1671816941\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:45 (running for 00:07:45.54)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         450.253</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  61.903</td><td style=\"text-align: right;\">             146.448</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 146.44794694091448\n",
      "  episode_reward_mean: 65.30519018404311\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 213\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5715563297271729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021096108481287956\n",
      "          model: {}\n",
      "          policy_loss: -0.024431124329566956\n",
      "          total_loss: 2.1531825065612793\n",
      "          vf_explained_var: 0.7597988843917847\n",
      "          vf_loss: 2.173394203186035\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.27777777777778\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764425824389566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31798974639855443\n",
      "    mean_inference_ms: 0.5072106717832157\n",
      "    mean_raw_obs_processing_ms: 0.06879462100433888\n",
      "  time_since_restore: 456.513459444046\n",
      "  time_this_iter_s: 6.260248422622681\n",
      "  time_total_s: 456.513459444046\n",
      "  timers:\n",
      "    learn_throughput: 941.784\n",
      "    learn_time_ms: 4247.257\n",
      "    load_throughput: 3738655.376\n",
      "    load_time_ms: 1.07\n",
      "    sample_throughput: 638.566\n",
      "    sample_time_ms: 6264.036\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1671816947\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:50 (running for 00:07:50.84)<br>Memory usage on this node: 6.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         456.513</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> 65.3052</td><td style=\"text-align: right;\">             146.448</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 146.44794694091448\n",
      "  episode_reward_mean: 68.90268660191263\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 215\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6189743280410767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01882951706647873\n",
      "          model: {}\n",
      "          policy_loss: -0.019374078139662743\n",
      "          total_loss: 2.8728411197662354\n",
      "          vf_explained_var: 0.7222643494606018\n",
      "          vf_loss: 2.88844895362854\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.67777777777778\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764135785023995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31785118538766116\n",
      "    mean_inference_ms: 0.5071859744367967\n",
      "    mean_raw_obs_processing_ms: 0.06878351698696208\n",
      "  time_since_restore: 462.786495923996\n",
      "  time_this_iter_s: 6.273036479949951\n",
      "  time_total_s: 462.786495923996\n",
      "  timers:\n",
      "    learn_throughput: 940.805\n",
      "    learn_time_ms: 4251.677\n",
      "    load_throughput: 3754888.208\n",
      "    load_time_ms: 1.065\n",
      "    sample_throughput: 638.396\n",
      "    sample_time_ms: 6265.706\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1671816954\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:35:56 (running for 00:07:56.11)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         462.786</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> 68.9027</td><td style=\"text-align: right;\">             146.448</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.5775085584526\n",
      "  episode_reward_mean: 74.31297412108539\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 218\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4674354791641235\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02410992980003357\n",
      "          model: {}\n",
      "          policy_loss: -0.02380327135324478\n",
      "          total_loss: 3.2609033584594727\n",
      "          vf_explained_var: 0.7715415358543396\n",
      "          vf_loss: 3.2798848152160645\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.86666666666667\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08763744711036106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31764447153059144\n",
      "    mean_inference_ms: 0.5071487715728232\n",
      "    mean_raw_obs_processing_ms: 0.06876793566930202\n",
      "  time_since_restore: 469.04782700538635\n",
      "  time_this_iter_s: 6.261331081390381\n",
      "  time_total_s: 469.04782700538635\n",
      "  timers:\n",
      "    learn_throughput: 940.604\n",
      "    learn_time_ms: 4252.587\n",
      "    load_throughput: 3835055.204\n",
      "    load_time_ms: 1.043\n",
      "    sample_throughput: 637.801\n",
      "    sample_time_ms: 6271.551\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1671816960\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:01 (running for 00:08:01.42)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         469.048</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  74.313</td><td style=\"text-align: right;\">             147.578</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:06 (running for 00:08:06.42)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         469.048</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  74.313</td><td style=\"text-align: right;\">             147.578</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-06\n",
      "  done: false\n",
      "  episode_len_mean: 1585.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.5775085584526\n",
      "  episode_reward_mean: 79.51966217386547\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 221\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4236180782318115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01943369396030903\n",
      "          model: {}\n",
      "          policy_loss: -0.020422637462615967\n",
      "          total_loss: 2.4171853065490723\n",
      "          vf_explained_var: 0.7138649821281433\n",
      "          vf_loss: 2.4337210655212402\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.7\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08763262369541203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31744239683251935\n",
      "    mean_inference_ms: 0.5071100898634259\n",
      "    mean_raw_obs_processing_ms: 0.06875242628707758\n",
      "  time_since_restore: 475.3202621936798\n",
      "  time_this_iter_s: 6.272435188293457\n",
      "  time_total_s: 475.3202621936798\n",
      "  timers:\n",
      "    learn_throughput: 940.626\n",
      "    learn_time_ms: 4252.486\n",
      "    load_throughput: 3829627.702\n",
      "    load_time_ms: 1.044\n",
      "    sample_throughput: 637.663\n",
      "    sample_time_ms: 6272.905\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1671816966\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:11 (running for 00:08:11.71)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">          475.32</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> 79.5197</td><td style=\"text-align: right;\">             147.578</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1585.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-12\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.5775085584526\n",
      "  episode_reward_mean: 82.56886887198323\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 224\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4040957689285278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019727036356925964\n",
      "          model: {}\n",
      "          policy_loss: -0.03082900680601597\n",
      "          total_loss: 99.31715393066406\n",
      "          vf_explained_var: 0.6811762452125549\n",
      "          vf_loss: 99.34403228759766\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.855555555555554\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08762067069973932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172615742094086\n",
      "    mean_inference_ms: 0.5071177733221875\n",
      "    mean_raw_obs_processing_ms: 0.06873580907344323\n",
      "  time_since_restore: 481.5133001804352\n",
      "  time_this_iter_s: 6.193037986755371\n",
      "  time_total_s: 481.5133001804352\n",
      "  timers:\n",
      "    learn_throughput: 940.902\n",
      "    learn_time_ms: 4251.242\n",
      "    load_throughput: 3805220.231\n",
      "    load_time_ms: 1.051\n",
      "    sample_throughput: 638.024\n",
      "    sample_time_ms: 6269.36\n",
      "    update_time_ms: 1.613\n",
      "  timestamp: 1671816972\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:16 (running for 00:08:16.90)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         481.513</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> 82.5689</td><td style=\"text-align: right;\">             147.578</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 85.62501210035089\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 226\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4444215297698975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022354597225785255\n",
      "          model: {}\n",
      "          policy_loss: -0.025012420490384102\n",
      "          total_loss: 2.617321491241455\n",
      "          vf_explained_var: 0.6568194031715393\n",
      "          vf_loss: 2.6378626823425293\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.711111111111116\n",
      "    ram_util_percent: 22.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08762756492557598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31712652736318697\n",
      "    mean_inference_ms: 0.5070552296768029\n",
      "    mean_raw_obs_processing_ms: 0.06873110591787049\n",
      "  time_since_restore: 487.80007338523865\n",
      "  time_this_iter_s: 6.286773204803467\n",
      "  time_total_s: 487.80007338523865\n",
      "  timers:\n",
      "    learn_throughput: 941.032\n",
      "    learn_time_ms: 4250.652\n",
      "    load_throughput: 3655166.885\n",
      "    load_time_ms: 1.094\n",
      "    sample_throughput: 637.987\n",
      "    sample_time_ms: 6269.724\n",
      "    update_time_ms: 1.591\n",
      "  timestamp: 1671816979\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:22 (running for 00:08:22.23)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">           487.8</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  85.625</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 88.4342381807944\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 228\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.350753903388977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021944966167211533\n",
      "          model: {}\n",
      "          policy_loss: -0.02694406732916832\n",
      "          total_loss: 1.8033301830291748\n",
      "          vf_explained_var: 0.7911669611930847\n",
      "          vf_loss: 1.8258851766586304\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.362500000000004\n",
      "    ram_util_percent: 22.262500000000003\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0876248210478629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3170043455804532\n",
      "    mean_inference_ms: 0.5070287245130166\n",
      "    mean_raw_obs_processing_ms: 0.06872223048071241\n",
      "  time_since_restore: 494.0622193813324\n",
      "  time_this_iter_s: 6.26214599609375\n",
      "  time_total_s: 494.0622193813324\n",
      "  timers:\n",
      "    learn_throughput: 940.703\n",
      "    learn_time_ms: 4252.14\n",
      "    load_throughput: 3649680.437\n",
      "    load_time_ms: 1.096\n",
      "    sample_throughput: 637.761\n",
      "    sample_time_ms: 6271.938\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1671816985\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:27 (running for 00:08:27.49)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         494.062</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> 88.4342</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-31\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 92.52258279448994\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 231\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3991602659225464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018160449340939522\n",
      "          model: {}\n",
      "          policy_loss: -0.024305401369929314\n",
      "          total_loss: 3.5538933277130127\n",
      "          vf_explained_var: 0.7065319418907166\n",
      "          vf_loss: 3.574566602706909\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.33\n",
      "    ram_util_percent: 22.32\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08761872401504067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.316821024625834\n",
      "    mean_inference_ms: 0.5069800977034666\n",
      "    mean_raw_obs_processing_ms: 0.06870723037511535\n",
      "  time_since_restore: 500.3812017440796\n",
      "  time_this_iter_s: 6.318982362747192\n",
      "  time_total_s: 500.3812017440796\n",
      "  timers:\n",
      "    learn_throughput: 939.354\n",
      "    learn_time_ms: 4258.246\n",
      "    load_throughput: 3655405.8\n",
      "    load_time_ms: 1.094\n",
      "    sample_throughput: 636.884\n",
      "    sample_time_ms: 6280.581\n",
      "    update_time_ms: 1.616\n",
      "  timestamp: 1671816991\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:32 (running for 00:08:32.85)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         500.381</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 92.5226</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:37 (running for 00:08:37.85)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         500.381</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 92.5226</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 96.17501986360239\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 234\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3791464567184448\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02081056497991085\n",
      "          model: {}\n",
      "          policy_loss: -0.024672552943229675\n",
      "          total_loss: 4.321838855743408\n",
      "          vf_explained_var: 0.7611939907073975\n",
      "          vf_loss: 4.342349052429199\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.15\n",
      "    ram_util_percent: 22.35\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0876062315993882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3166612377505309\n",
      "    mean_inference_ms: 0.5069801100879762\n",
      "    mean_raw_obs_processing_ms: 0.06869058490687208\n",
      "  time_since_restore: 506.64833784103394\n",
      "  time_this_iter_s: 6.267136096954346\n",
      "  time_total_s: 506.64833784103394\n",
      "  timers:\n",
      "    learn_throughput: 939.765\n",
      "    learn_time_ms: 4256.382\n",
      "    load_throughput: 3670038.938\n",
      "    load_time_ms: 1.09\n",
      "    sample_throughput: 636.117\n",
      "    sample_time_ms: 6288.152\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1671816998\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:43 (running for 00:08:43.15)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         506.648</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  96.175</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 98.16308212386956\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 236\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3699365854263306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01970175839960575\n",
      "          model: {}\n",
      "          policy_loss: -0.020673198625445366\n",
      "          total_loss: 2.919490337371826\n",
      "          vf_explained_var: 0.730987012386322\n",
      "          vf_loss: 2.936223268508911\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.65\n",
      "    ram_util_percent: 22.160000000000004\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08761249775530212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3165391434040411\n",
      "    mean_inference_ms: 0.5069120398845558\n",
      "    mean_raw_obs_processing_ms: 0.0686855733157665\n",
      "  time_since_restore: 513.5517470836639\n",
      "  time_this_iter_s: 6.903409242630005\n",
      "  time_total_s: 513.5517470836639\n",
      "  timers:\n",
      "    learn_throughput: 926.822\n",
      "    learn_time_ms: 4315.823\n",
      "    load_throughput: 3607615.525\n",
      "    load_time_ms: 1.109\n",
      "    sample_throughput: 635.785\n",
      "    sample_time_ms: 6291.431\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1671817005\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:49 (running for 00:08:49.05)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         513.552</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> 98.1631</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 100.39432461367156\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 238\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3472306728363037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020647086203098297\n",
      "          model: {}\n",
      "          policy_loss: -0.02393530309200287\n",
      "          total_loss: 2.4643235206604004\n",
      "          vf_explained_var: 0.7962733507156372\n",
      "          vf_loss: 2.4841291904449463\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.26363636363636\n",
      "    ram_util_percent: 22.37272727272727\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08761609371593954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164491230481061\n",
      "    mean_inference_ms: 0.5069345193808388\n",
      "    mean_raw_obs_processing_ms: 0.0686819885533609\n",
      "  time_since_restore: 520.7168605327606\n",
      "  time_this_iter_s: 7.16511344909668\n",
      "  time_total_s: 520.7168605327606\n",
      "  timers:\n",
      "    learn_throughput: 919.828\n",
      "    learn_time_ms: 4348.639\n",
      "    load_throughput: 3639151.447\n",
      "    load_time_ms: 1.099\n",
      "    sample_throughput: 624.1\n",
      "    sample_time_ms: 6409.232\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1671817012\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:54 (running for 00:08:54.27)<br>Memory usage on this node: 7.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         520.717</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> 100.394</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:36:59 (running for 00:08:59.27)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         520.717</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> 100.394</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 103.6683621019167\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 241\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3977605104446411\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019256271421909332\n",
      "          model: {}\n",
      "          policy_loss: -0.027345949783921242\n",
      "          total_loss: 2.164727210998535\n",
      "          vf_explained_var: 0.7950323820114136\n",
      "          vf_loss: 2.1882216930389404\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.52\n",
      "    ram_util_percent: 24.2\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08762996109715664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163437227377748\n",
      "    mean_inference_ms: 0.507015474158405\n",
      "    mean_raw_obs_processing_ms: 0.06868494692376283\n",
      "  time_since_restore: 528.0874907970428\n",
      "  time_this_iter_s: 7.370630264282227\n",
      "  time_total_s: 528.0874907970428\n",
      "  timers:\n",
      "    learn_throughput: 909.14\n",
      "    learn_time_ms: 4399.761\n",
      "    load_throughput: 3598562.052\n",
      "    load_time_ms: 1.112\n",
      "    sample_throughput: 615.312\n",
      "    sample_time_ms: 6500.768\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1671817019\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:04 (running for 00:09:04.67)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         528.087</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 103.668</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 106.6904737763409\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 244\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3866848945617676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022523801773786545\n",
      "          model: {}\n",
      "          policy_loss: -0.025492116808891296\n",
      "          total_loss: 3.292675733566284\n",
      "          vf_explained_var: 0.7744581699371338\n",
      "          vf_loss: 3.3136630058288574\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.008333333333326\n",
      "    ram_util_percent: 24.849999999999998\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765235644257309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31631106147393334\n",
      "    mean_inference_ms: 0.5072289637111538\n",
      "    mean_raw_obs_processing_ms: 0.06870168344050978\n",
      "  time_since_restore: 536.3726885318756\n",
      "  time_this_iter_s: 8.285197734832764\n",
      "  time_total_s: 536.3726885318756\n",
      "  timers:\n",
      "    learn_throughput: 888.646\n",
      "    learn_time_ms: 4501.231\n",
      "    load_throughput: 3460575.483\n",
      "    load_time_ms: 1.156\n",
      "    sample_throughput: 601.171\n",
      "    sample_time_ms: 6653.685\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1671817027\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:09 (running for 00:09:09.95)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         536.373</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  106.69</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:15 (running for 00:09:14.99)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         536.373</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  106.69</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 1577.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 108.29812727187385\n",
      "  episode_reward_min: -96.71755117153315\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 246\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4367494583129883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01683288998901844\n",
      "          model: {}\n",
      "          policy_loss: -0.021808264777064323\n",
      "          total_loss: 2.326220750808716\n",
      "          vf_explained_var: 0.7359540462493896\n",
      "          vf_loss: 2.3446621894836426\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.96666666666666\n",
      "    ram_util_percent: 24.899999999999995\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768859293521997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31630478189994204\n",
      "    mean_inference_ms: 0.5073468793939132\n",
      "    mean_raw_obs_processing_ms: 0.06872694131491905\n",
      "  time_since_restore: 544.564977645874\n",
      "  time_this_iter_s: 8.192289113998413\n",
      "  time_total_s: 544.564977645874\n",
      "  timers:\n",
      "    learn_throughput: 866.643\n",
      "    learn_time_ms: 4615.513\n",
      "    load_throughput: 3372033.605\n",
      "    load_time_ms: 1.186\n",
      "    sample_throughput: 585.362\n",
      "    sample_time_ms: 6833.382\n",
      "    update_time_ms: 1.797\n",
      "  timestamp: 1671817036\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:20 (running for 00:09:20.18)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         544.565</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 108.298</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -96.7176</td><td style=\"text-align: right;\">           1577.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-23\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.36129082783216\n",
      "  episode_reward_mean: 111.57426377302149\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 248\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3233288526535034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019827691838145256\n",
      "          model: {}\n",
      "          policy_loss: -0.027133790776133537\n",
      "          total_loss: 2.4900898933410645\n",
      "          vf_explained_var: 0.7889662384986877\n",
      "          vf_loss: 2.513258218765259\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.436363636363645\n",
      "    ram_util_percent: 24.9\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08771670034361093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3163552489088569\n",
      "    mean_inference_ms: 0.5075904074220501\n",
      "    mean_raw_obs_processing_ms: 0.06875248334363047\n",
      "  time_since_restore: 552.138808965683\n",
      "  time_this_iter_s: 7.57383131980896\n",
      "  time_total_s: 552.138808965683\n",
      "  timers:\n",
      "    learn_throughput: 855.691\n",
      "    learn_time_ms: 4674.583\n",
      "    load_throughput: 3385164.948\n",
      "    load_time_ms: 1.182\n",
      "    sample_throughput: 569.121\n",
      "    sample_time_ms: 7028.382\n",
      "    update_time_ms: 1.853\n",
      "  timestamp: 1671817043\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:25 (running for 00:09:25.81)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         552.139</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> 111.574</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:30 (running for 00:09:30.82)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         552.139</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> 111.574</td><td style=\"text-align: right;\">             153.361</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.7515850224378\n",
      "  episode_reward_mean: 114.24704078841587\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 251\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3695552349090576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021332886070013046\n",
      "          model: {}\n",
      "          policy_loss: -0.024914046749472618\n",
      "          total_loss: 4.321210861206055\n",
      "          vf_explained_var: 0.7350334525108337\n",
      "          vf_loss: 4.341858386993408\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.85\n",
      "    ram_util_percent: 24.941666666666663\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08778885863775109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3164792343052875\n",
      "    mean_inference_ms: 0.5079931219489082\n",
      "    mean_raw_obs_processing_ms: 0.06881333259838875\n",
      "  time_since_restore: 560.4431729316711\n",
      "  time_this_iter_s: 8.30436396598816\n",
      "  time_total_s: 560.4431729316711\n",
      "  timers:\n",
      "    learn_throughput: 839.166\n",
      "    learn_time_ms: 4766.638\n",
      "    load_throughput: 3429380.647\n",
      "    load_time_ms: 1.166\n",
      "    sample_throughput: 555.681\n",
      "    sample_time_ms: 7198.369\n",
      "    update_time_ms: 1.862\n",
      "  timestamp: 1671817052\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:36 (running for 00:09:36.14)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         560.443</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> 114.247</td><td style=\"text-align: right;\">             153.752</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.7515850224378\n",
      "  episode_reward_mean: 117.08818344228996\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 254\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3729214668273926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018078170716762543\n",
      "          model: {}\n",
      "          policy_loss: -0.023826247081160545\n",
      "          total_loss: 3.563232898712158\n",
      "          vf_explained_var: 0.7560592293739319\n",
      "          vf_loss: 3.5834436416625977\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.25\n",
      "    ram_util_percent: 25.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08786912240609965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3166423639264029\n",
      "    mean_inference_ms: 0.5084335887010369\n",
      "    mean_raw_obs_processing_ms: 0.06888420504390277\n",
      "  time_since_restore: 568.0201427936554\n",
      "  time_this_iter_s: 7.576969861984253\n",
      "  time_total_s: 568.0201427936554\n",
      "  timers:\n",
      "    learn_throughput: 826.853\n",
      "    learn_time_ms: 4837.621\n",
      "    load_throughput: 3393861.715\n",
      "    load_time_ms: 1.179\n",
      "    sample_throughput: 544.101\n",
      "    sample_time_ms: 7351.573\n",
      "    update_time_ms: 1.877\n",
      "  timestamp: 1671817059\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:41 (running for 00:09:41.73)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">          568.02</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 117.088</td><td style=\"text-align: right;\">             153.752</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:46 (running for 00:09:46.76)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">          568.02</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 117.088</td><td style=\"text-align: right;\">             153.752</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 118.99582648413642\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 256\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3756389617919922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021005798131227493\n",
      "          model: {}\n",
      "          policy_loss: -0.023990830406546593\n",
      "          total_loss: 3.2325079441070557\n",
      "          vf_explained_var: 0.7960026860237122\n",
      "          vf_loss: 3.2522976398468018\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.675000000000004\n",
      "    ram_util_percent: 24.958333333333332\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08793266026549892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31678859959624966\n",
      "    mean_inference_ms: 0.5087844824394989\n",
      "    mean_raw_obs_processing_ms: 0.06894081103156827\n",
      "  time_since_restore: 576.1967997550964\n",
      "  time_this_iter_s: 8.17665696144104\n",
      "  time_total_s: 576.1967997550964\n",
      "  timers:\n",
      "    learn_throughput: 813.185\n",
      "    learn_time_ms: 4918.928\n",
      "    load_throughput: 3303837.262\n",
      "    load_time_ms: 1.211\n",
      "    sample_throughput: 531.327\n",
      "    sample_time_ms: 7528.325\n",
      "    update_time_ms: 1.911\n",
      "  timestamp: 1671817067\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:51 (running for 00:09:51.93)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         576.197</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 118.996</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 120.68293566715529\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 258\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3116084337234497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022327661514282227\n",
      "          model: {}\n",
      "          policy_loss: -0.022050932049751282\n",
      "          total_loss: 2.945211410522461\n",
      "          vf_explained_var: 0.8262830376625061\n",
      "          vf_loss: 2.962797164916992\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.63636363636363\n",
      "    ram_util_percent: 24.936363636363634\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08800058261264061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31695590586761413\n",
      "    mean_inference_ms: 0.5091590587357063\n",
      "    mean_raw_obs_processing_ms: 0.06900209476346272\n",
      "  time_since_restore: 583.7870049476624\n",
      "  time_this_iter_s: 7.590205192565918\n",
      "  time_total_s: 583.7870049476624\n",
      "  timers:\n",
      "    learn_throughput: 801.008\n",
      "    learn_time_ms: 4993.709\n",
      "    load_throughput: 3287136.503\n",
      "    load_time_ms: 1.217\n",
      "    sample_throughput: 521.662\n",
      "    sample_time_ms: 7667.802\n",
      "    update_time_ms: 1.938\n",
      "  timestamp: 1671817075\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:37:57 (running for 00:09:57.59)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         583.787</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> 120.683</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:02 (running for 00:10:02.59)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         583.787</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> 120.683</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-03\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 123.35554261483732\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 261\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.270633339881897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02010677382349968\n",
      "          model: {}\n",
      "          policy_loss: -0.023874491453170776\n",
      "          total_loss: 3.1358304023742676\n",
      "          vf_explained_var: 0.7926644682884216\n",
      "          vf_loss: 3.1556835174560547\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.625\n",
      "    ram_util_percent: 24.95\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08811636724427689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3172580250377674\n",
      "    mean_inference_ms: 0.509800355168436\n",
      "    mean_raw_obs_processing_ms: 0.06910699238521512\n",
      "  time_since_restore: 591.9184761047363\n",
      "  time_this_iter_s: 8.131471157073975\n",
      "  time_total_s: 591.9184761047363\n",
      "  timers:\n",
      "    learn_throughput: 797.108\n",
      "    learn_time_ms: 5018.138\n",
      "    load_throughput: 3229617.31\n",
      "    load_time_ms: 1.239\n",
      "    sample_throughput: 510.034\n",
      "    sample_time_ms: 7842.615\n",
      "    update_time_ms: 1.937\n",
      "  timestamp: 1671817083\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:07 (running for 00:10:07.75)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         591.918</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> 123.356</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 125.67389370726707\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 264\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2318545579910278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02058662474155426\n",
      "          model: {}\n",
      "          policy_loss: -0.023503977805376053\n",
      "          total_loss: 4.637133598327637\n",
      "          vf_explained_var: 0.7566186189651489\n",
      "          vf_loss: 4.656520843505859\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.5\n",
      "    ram_util_percent: 24.99090909090909\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08824080519692913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3176020591703748\n",
      "    mean_inference_ms: 0.5104805107993777\n",
      "    mean_raw_obs_processing_ms: 0.06922136210544293\n",
      "  time_since_restore: 599.5620028972626\n",
      "  time_this_iter_s: 7.643526792526245\n",
      "  time_total_s: 599.5620028972626\n",
      "  timers:\n",
      "    learn_throughput: 789.535\n",
      "    learn_time_ms: 5066.276\n",
      "    load_throughput: 3207574.037\n",
      "    load_time_ms: 1.247\n",
      "    sample_throughput: 508.511\n",
      "    sample_time_ms: 7866.104\n",
      "    update_time_ms: 1.94\n",
      "  timestamp: 1671817091\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:13 (running for 00:10:13.40)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         599.562</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 125.674</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:18 (running for 00:10:18.43)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         599.562</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 125.674</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-19\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 127.31254201456981\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 266\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2615786790847778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019166110083460808\n",
      "          model: {}\n",
      "          policy_loss: -0.02306693233549595\n",
      "          total_loss: 3.7450907230377197\n",
      "          vf_explained_var: 0.7761012315750122\n",
      "          vf_loss: 3.764324426651001\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.5090909090909\n",
      "    ram_util_percent: 25.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08833249133782856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3178645210217293\n",
      "    mean_inference_ms: 0.5109882825382424\n",
      "    mean_raw_obs_processing_ms: 0.06930578944241529\n",
      "  time_since_restore: 607.6311521530151\n",
      "  time_this_iter_s: 8.069149255752563\n",
      "  time_total_s: 607.6311521530151\n",
      "  timers:\n",
      "    learn_throughput: 785.658\n",
      "    learn_time_ms: 5091.274\n",
      "    load_throughput: 3132064.369\n",
      "    load_time_ms: 1.277\n",
      "    sample_throughput: 502.46\n",
      "    sample_time_ms: 7960.826\n",
      "    update_time_ms: 1.935\n",
      "  timestamp: 1671817099\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:23 (running for 00:10:23.50)<br>Memory usage on this node: 7.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         607.631</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> 127.313</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-27\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62657769973515\n",
      "  episode_reward_mean: 128.36415823091195\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 268\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.168962001800537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019138526171445847\n",
      "          model: {}\n",
      "          policy_loss: -0.020162787288427353\n",
      "          total_loss: 2.5402209758758545\n",
      "          vf_explained_var: 0.7894414067268372\n",
      "          vf_loss: 2.556555986404419\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.72727272727273\n",
      "    ram_util_percent: 25.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08842859350298213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.318149979850632\n",
      "    mean_inference_ms: 0.5115204219104563\n",
      "    mean_raw_obs_processing_ms: 0.06939516623178207\n",
      "  time_since_restore: 615.3291630744934\n",
      "  time_this_iter_s: 7.6980109214782715\n",
      "  time_total_s: 615.3291630744934\n",
      "  timers:\n",
      "    learn_throughput: 787.728\n",
      "    learn_time_ms: 5077.893\n",
      "    load_throughput: 3215750.978\n",
      "    load_time_ms: 1.244\n",
      "    sample_throughput: 503.768\n",
      "    sample_time_ms: 7940.156\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1671817107\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:29 (running for 00:10:29.26)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         615.329</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 128.364</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:34 (running for 00:10:34.27)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         615.329</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 128.364</td><td style=\"text-align: right;\">             155.627</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 165.31675665614569\n",
      "  episode_reward_mean: 130.7380520477591\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 271\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1845260858535767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021208051592111588\n",
      "          model: {}\n",
      "          policy_loss: -0.023954784497618675\n",
      "          total_loss: 3.0957534313201904\n",
      "          vf_explained_var: 0.7849385142326355\n",
      "          vf_loss: 3.115467071533203\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.49999999999999\n",
      "    ram_util_percent: 25.483333333333334\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0885891810723872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3186336298789741\n",
      "    mean_inference_ms: 0.5124151136480597\n",
      "    mean_raw_obs_processing_ms: 0.06954409163762984\n",
      "  time_since_restore: 623.7275745868683\n",
      "  time_this_iter_s: 8.398411512374878\n",
      "  time_total_s: 623.7275745868683\n",
      "  timers:\n",
      "    learn_throughput: 791.736\n",
      "    learn_time_ms: 5052.189\n",
      "    load_throughput: 3186071.632\n",
      "    load_time_ms: 1.255\n",
      "    sample_throughput: 501.603\n",
      "    sample_time_ms: 7974.438\n",
      "    update_time_ms: 2.037\n",
      "  timestamp: 1671817115\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:39 (running for 00:10:39.69)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         623.728</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 130.738</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-43\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 165.31675665614569\n",
      "  episode_reward_mean: 132.62327257312504\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 274\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2083194255828857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021694425493478775\n",
      "          model: {}\n",
      "          policy_loss: -0.01960534043610096\n",
      "          total_loss: 4.451814651489258\n",
      "          vf_explained_var: 0.7487953901290894\n",
      "          vf_loss: 4.467080593109131\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.463636363636354\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08875883893591208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31915810450812\n",
      "    mean_inference_ms: 0.5133457141769294\n",
      "    mean_raw_obs_processing_ms: 0.06970256829558007\n",
      "  time_since_restore: 631.3208141326904\n",
      "  time_this_iter_s: 7.5932395458221436\n",
      "  time_total_s: 631.3208141326904\n",
      "  timers:\n",
      "    learn_throughput: 789.759\n",
      "    learn_time_ms: 5064.838\n",
      "    load_throughput: 3204878.03\n",
      "    load_time_ms: 1.248\n",
      "    sample_throughput: 503.944\n",
      "    sample_time_ms: 7937.386\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1671817123\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:45 (running for 00:10:45.29)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         631.321</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 132.623</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:50 (running for 00:10:50.32)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         631.321</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 132.623</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-51\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 165.31675665614569\n",
      "  episode_reward_mean: 134.03289484391777\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 276\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2029165029525757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02198474109172821\n",
      "          model: {}\n",
      "          policy_loss: -0.02438454143702984\n",
      "          total_loss: 2.8891921043395996\n",
      "          vf_explained_var: 0.7424609065055847\n",
      "          vf_loss: 2.9091796875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.70000000000001\n",
      "    ram_util_percent: 25.600000000000005\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08888037136600083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3195388149017438\n",
      "    mean_inference_ms: 0.5140182165623335\n",
      "    mean_raw_obs_processing_ms: 0.0698162614729403\n",
      "  time_since_restore: 639.4606173038483\n",
      "  time_this_iter_s: 8.139803171157837\n",
      "  time_total_s: 639.4606173038483\n",
      "  timers:\n",
      "    learn_throughput: 790.653\n",
      "    learn_time_ms: 5059.11\n",
      "    load_throughput: 3186918.927\n",
      "    load_time_ms: 1.255\n",
      "    sample_throughput: 503.807\n",
      "    sample_time_ms: 7939.552\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1671817131\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:38:55 (running for 00:10:55.46)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         639.461</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> 134.033</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-38-58\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 165.31675665614569\n",
      "  episode_reward_mean: 135.12990210404448\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 278\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.194416880607605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023148750886321068\n",
      "          model: {}\n",
      "          policy_loss: -0.02270408906042576\n",
      "          total_loss: 3.00585675239563\n",
      "          vf_explained_var: 0.7824265360832214\n",
      "          vf_loss: 3.0239310264587402\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.57000000000001\n",
      "    ram_util_percent: 25.6\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08900574168049935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.31993846311749397\n",
      "    mean_inference_ms: 0.514711512655407\n",
      "    mean_raw_obs_processing_ms: 0.06993418629791709\n",
      "  time_since_restore: 646.9637386798859\n",
      "  time_this_iter_s: 7.503121376037598\n",
      "  time_total_s: 646.9637386798859\n",
      "  timers:\n",
      "    learn_throughput: 791.386\n",
      "    learn_time_ms: 5054.427\n",
      "    load_throughput: 3223721.922\n",
      "    load_time_ms: 1.241\n",
      "    sample_throughput: 504.325\n",
      "    sample_time_ms: 7931.392\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1671817138\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:01 (running for 00:11:01.02)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:06 (running for 00:11:06.03)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:11 (running for 00:11:11.03)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:16 (running for 00:11:16.04)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:21 (running for 00:11:21.04)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:26 (running for 00:11:26.05)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:31 (running for 00:11:31.05)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:36 (running for 00:11:36.06)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:41 (running for 00:11:41.06)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:46 (running for 00:11:46.06)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:51 (running for 00:11:51.07)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:39:56 (running for 00:11:56.07)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:01 (running for 00:12:01.08)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:06 (running for 00:12:06.08)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:11 (running for 00:12:11.09)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:16 (running for 00:12:16.09)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:21 (running for 00:12:21.10)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:26 (running for 00:12:26.10)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:31 (running for 00:12:31.11)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:36 (running for 00:12:36.11)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:41 (running for 00:12:41.11)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:46 (running for 00:12:46.12)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:51 (running for 00:12:51.12)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:40:56 (running for 00:12:56.13)<br>Memory usage on this node: 8.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:01 (running for 00:13:01.13)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:06 (running for 00:13:06.14)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:11 (running for 00:13:11.14)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:16 (running for 00:13:16.14)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:21 (running for 00:13:21.15)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:26 (running for 00:13:26.15)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:31 (running for 00:13:31.16)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:36 (running for 00:13:36.16)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:41 (running for 00:13:41.17)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:46 (running for 00:13:46.17)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:51 (running for 00:13:51.17)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:41:56 (running for 00:13:56.18)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:01 (running for 00:14:01.18)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:06 (running for 00:14:06.19)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:11 (running for 00:14:11.19)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:16 (running for 00:14:16.19)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:21 (running for 00:14:21.20)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:26 (running for 00:14:26.20)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:31 (running for 00:14:31.21)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:36 (running for 00:14:36.21)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:41 (running for 00:14:41.22)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:46 (running for 00:14:46.22)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:51 (running for 00:14:51.22)<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:42:56 (running for 00:14:56.23)<br>Memory usage on this node: 7.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:01 (running for 00:15:01.23)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:06 (running for 00:15:06.23)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:11 (running for 00:15:11.24)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:16 (running for 00:15:16.24)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:21 (running for 00:15:21.25)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:26 (running for 00:15:26.25)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:31 (running for 00:15:31.26)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         646.964</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  135.13</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 165.31675665614569\n",
      "  episode_reward_mean: 136.99515304034477\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 281\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 1564.59\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 177.08494279565102\n",
      "    episode_reward_mean: 151.6827130599735\n",
      "    episode_reward_min: -81.63790952512372\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 652\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1312\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 806\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 317\n",
      "      - 1372\n",
      "      episode_reward:\n",
      "      - 161.5416428532624\n",
      "      - 163.66500592470086\n",
      "      - 161.22600511546455\n",
      "      - 157.46222967737344\n",
      "      - 167.78122852948036\n",
      "      - 143.71945338261176\n",
      "      - 169.0069603699054\n",
      "      - 169.78067035613418\n",
      "      - 148.2697052408902\n",
      "      - 166.90629588889595\n",
      "      - 156.34785939852637\n",
      "      - 169.90976764193542\n",
      "      - 161.37047751209178\n",
      "      - 161.9934413428995\n",
      "      - 169.89790704612318\n",
      "      - 151.00850935589702\n",
      "      - 154.62009070648372\n",
      "      - 163.4546303326398\n",
      "      - 152.77040469033219\n",
      "      - 156.39766148164483\n",
      "      - 164.49265607196836\n",
      "      - 164.40481652765237\n",
      "      - 158.22903744694756\n",
      "      - 165.71312136704736\n",
      "      - 149.01664088757593\n",
      "      - 177.08494279565102\n",
      "      - 163.69307520228142\n",
      "      - 157.37978430552252\n",
      "      - 158.42910894503757\n",
      "      - 155.1167409745104\n",
      "      - 160.52267684924792\n",
      "      - 165.8410729423236\n",
      "      - 170.37750250461082\n",
      "      - 158.4990907263136\n",
      "      - 151.51815291988527\n",
      "      - 155.957589617379\n",
      "      - 158.64058231017202\n",
      "      - 160.19375898625572\n",
      "      - 161.18875971287468\n",
      "      - 160.87500930191717\n",
      "      - 154.35117391511253\n",
      "      - 161.4736894668075\n",
      "      - 155.0769287939811\n",
      "      - 161.93493544177574\n",
      "      - 160.31452222863732\n",
      "      - 163.01883072723976\n",
      "      - 159.94937350728665\n",
      "      - 166.1954511897731\n",
      "      - 157.8169627166843\n",
      "      - 164.98089247006672\n",
      "      - 164.34917630353743\n",
      "      - 165.48410227605086\n",
      "      - 154.32867487756025\n",
      "      - 158.7732273843523\n",
      "      - 164.86297946932206\n",
      "      - -38.75144315798207\n",
      "      - 169.9940176825719\n",
      "      - 172.10252247346008\n",
      "      - 162.09572141903016\n",
      "      - 154.61834869378544\n",
      "      - 153.5256629739007\n",
      "      - 167.54339893965718\n",
      "      - 148.97538218572467\n",
      "      - 152.87971927513192\n",
      "      - 167.76553360174896\n",
      "      - 160.0989804017918\n",
      "      - 163.88551743231778\n",
      "      - 176.115391223849\n",
      "      - 168.6021356174614\n",
      "      - 162.45106892809446\n",
      "      - 158.683886414723\n",
      "      - 158.19520939581218\n",
      "      - 162.85209393710377\n",
      "      - 156.48387984972854\n",
      "      - 157.7419110889312\n",
      "      - 169.48857084620795\n",
      "      - 150.75775249522667\n",
      "      - 158.39242762871893\n",
      "      - 160.78255497332302\n",
      "      - 168.7854468324506\n",
      "      - 158.72894569765606\n",
      "      - 22.101593846583413\n",
      "      - 161.19009257441496\n",
      "      - 165.3335285384683\n",
      "      - 171.32950860837735\n",
      "      - 175.8459424289024\n",
      "      - 150.23386741324057\n",
      "      - 152.74428377749086\n",
      "      - 157.8392933594868\n",
      "      - 163.72666339190198\n",
      "      - 149.79955643038946\n",
      "      - -33.64219742917328\n",
      "      - 164.08341256028592\n",
      "      - 153.50775256619792\n",
      "      - 153.86723630932016\n",
      "      - 158.72766235397782\n",
      "      - 159.47251675844495\n",
      "      - 149.996289780105\n",
      "      - -81.63790952512372\n",
      "      - 33.74258739297704\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11138330407418762\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.4120503790951836\n",
      "      mean_inference_ms: 1.102346795211435\n",
      "      mean_raw_obs_processing_ms: 0.08020625004115564\n",
      "    timesteps_this_iter: 156459\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.239462971687317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01966150663793087\n",
      "          model: {}\n",
      "          policy_loss: -0.023171398788690567\n",
      "          total_loss: 3.958226442337036\n",
      "          vf_explained_var: 0.781898021697998\n",
      "          vf_loss: 3.9774656295776367\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25786802030457\n",
      "    ram_util_percent: 23.95126903553299\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08920638155491947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32058504225024287\n",
      "    mean_inference_ms: 0.5158270621803854\n",
      "    mean_raw_obs_processing_ms: 0.07012311090277722\n",
      "  time_since_restore: 922.6298339366913\n",
      "  time_this_iter_s: 275.6660952568054\n",
      "  time_total_s: 922.6298339366913\n",
      "  timers:\n",
      "    learn_throughput: 792.021\n",
      "    learn_time_ms: 5050.372\n",
      "    load_throughput: 3264621.432\n",
      "    load_time_ms: 1.225\n",
      "    sample_throughput: 504.762\n",
      "    sample_time_ms: 7924.525\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1671817414\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:36 (running for 00:15:36.69)<br>Memory usage on this node: 7.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">          922.63</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 136.995</td><td style=\"text-align: right;\">             165.317</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 167.1156446566322\n",
      "  episode_reward_mean: 138.7016051483566\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 284\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2115111351013184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022446125745773315\n",
      "          model: {}\n",
      "          policy_loss: -0.027156470343470573\n",
      "          total_loss: 3.898442506790161\n",
      "          vf_explained_var: 0.747413694858551\n",
      "          vf_loss: 3.921109676361084\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.48888888888889\n",
      "    ram_util_percent: 23.155555555555555\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08940726873398196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32123219507709694\n",
      "    mean_inference_ms: 0.516929346656184\n",
      "    mean_raw_obs_processing_ms: 0.07031261388332619\n",
      "  time_since_restore: 928.9211182594299\n",
      "  time_this_iter_s: 6.2912843227386475\n",
      "  time_total_s: 928.9211182594299\n",
      "  timers:\n",
      "    learn_throughput: 803.52\n",
      "    learn_time_ms: 4978.099\n",
      "    load_throughput: 1494576.229\n",
      "    load_time_ms: 2.676\n",
      "    sample_throughput: 115.551\n",
      "    sample_time_ms: 34616.821\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1671817421\n",
      "  timesteps_since_restore: 404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:42 (running for 00:15:42.00)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         928.921</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> 138.702</td><td style=\"text-align: right;\">             167.116</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:47 (running for 00:15:47.00)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         928.921</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> 138.702</td><td style=\"text-align: right;\">             167.116</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 167.1156446566322\n",
      "  episode_reward_mean: 139.38449779667155\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 286\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1165621280670166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020652014762163162\n",
      "          model: {}\n",
      "          policy_loss: -0.026598898693919182\n",
      "          total_loss: 4.07981014251709\n",
      "          vf_explained_var: 0.8019489049911499\n",
      "          vf_loss: 4.102278709411621\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.03333333333333\n",
      "    ram_util_percent: 23.08888888888889\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08954178415432244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32166859635634526\n",
      "    mean_inference_ms: 0.5176747773573246\n",
      "    mean_raw_obs_processing_ms: 0.07043922357775143\n",
      "  time_since_restore: 935.5664055347443\n",
      "  time_this_iter_s: 6.645287275314331\n",
      "  time_total_s: 935.5664055347443\n",
      "  timers:\n",
      "    learn_throughput: 814.064\n",
      "    learn_time_ms: 4913.616\n",
      "    load_throughput: 1520281.272\n",
      "    load_time_ms: 2.631\n",
      "    sample_throughput: 116.076\n",
      "    sample_time_ms: 34460.197\n",
      "    update_time_ms: 2.021\n",
      "  timestamp: 1671817427\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:52 (running for 00:15:52.70)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         935.566</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> 139.384</td><td style=\"text-align: right;\">             167.116</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 167.85608853499915\n",
      "  episode_reward_mean: 140.36763652350692\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 288\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1163411140441895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01949864812195301\n",
      "          model: {}\n",
      "          policy_loss: -0.02145601622760296\n",
      "          total_loss: 2.530954122543335\n",
      "          vf_explained_var: 0.8255624175071716\n",
      "          vf_loss: 2.5485105514526367\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.644444444444446\n",
      "    ram_util_percent: 23.08888888888889\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0896755658313473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32210201282349615\n",
      "    mean_inference_ms: 0.5184168028868978\n",
      "    mean_raw_obs_processing_ms: 0.07056499947008693\n",
      "  time_since_restore: 941.9143471717834\n",
      "  time_this_iter_s: 6.347941637039185\n",
      "  time_total_s: 941.9143471717834\n",
      "  timers:\n",
      "    learn_throughput: 826.474\n",
      "    learn_time_ms: 4839.836\n",
      "    load_throughput: 1525562.042\n",
      "    load_time_ms: 2.622\n",
      "    sample_throughput: 116.485\n",
      "    sample_time_ms: 34339.23\n",
      "    update_time_ms: 1.987\n",
      "  timestamp: 1671817434\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:43:58 (running for 00:15:58.03)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         941.914</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> 140.368</td><td style=\"text-align: right;\">             167.856</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 169.6027647026398\n",
      "  episode_reward_mean: 141.91821046443857\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 291\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1963956356048584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020615125074982643\n",
      "          model: {}\n",
      "          policy_loss: -0.025517892092466354\n",
      "          total_loss: 3.535935878753662\n",
      "          vf_explained_var: 0.7365742921829224\n",
      "          vf_loss: 3.55733060836792\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.67777777777778\n",
      "    ram_util_percent: 23.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08987348879038615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32274657706105786\n",
      "    mean_inference_ms: 0.5195219819374812\n",
      "    mean_raw_obs_processing_ms: 0.0707515643062273\n",
      "  time_since_restore: 948.12966132164\n",
      "  time_this_iter_s: 6.215314149856567\n",
      "  time_total_s: 948.12966132164\n",
      "  timers:\n",
      "    learn_throughput: 840.26\n",
      "    learn_time_ms: 4760.429\n",
      "    load_throughput: 1542420.476\n",
      "    load_time_ms: 2.593\n",
      "    sample_throughput: 117.105\n",
      "    sample_time_ms: 34157.506\n",
      "    update_time_ms: 1.989\n",
      "  timestamp: 1671817440\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:03 (running for 00:16:03.30)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">          948.13</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> 141.918</td><td style=\"text-align: right;\">             169.603</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-06\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 143.35606630700636\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.23232102394104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021366065368056297\n",
      "          model: {}\n",
      "          policy_loss: -0.027586355805397034\n",
      "          total_loss: 4.0263166427612305\n",
      "          vf_explained_var: 0.7089212536811829\n",
      "          vf_loss: 4.0496296882629395\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.111111111111114\n",
      "    ram_util_percent: 23.0\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0900704092969982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3233846751250379\n",
      "    mean_inference_ms: 0.5206091048458535\n",
      "    mean_raw_obs_processing_ms: 0.0709373476165964\n",
      "  time_since_restore: 954.3763558864594\n",
      "  time_this_iter_s: 6.246694564819336\n",
      "  time_total_s: 954.3763558864594\n",
      "  timers:\n",
      "    learn_throughput: 856.058\n",
      "    learn_time_ms: 4672.58\n",
      "    load_throughput: 1544621.56\n",
      "    load_time_ms: 2.59\n",
      "    sample_throughput: 117.577\n",
      "    sample_time_ms: 34020.259\n",
      "    update_time_ms: 1.82\n",
      "  timestamp: 1671817446\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:08 (running for 00:16:08.54)<br>Memory usage on this node: 7.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         954.376</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> 143.356</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 144.34756540276757\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 296\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1035417318344116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02170182764530182\n",
      "          model: {}\n",
      "          policy_loss: -0.02481427788734436\n",
      "          total_loss: 2.2184224128723145\n",
      "          vf_explained_var: 0.8036614656448364\n",
      "          vf_loss: 2.238896608352661\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.666666666666664\n",
      "    ram_util_percent: 22.84444444444444\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09020015384928276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32380627583568466\n",
      "    mean_inference_ms: 0.5213295265430471\n",
      "    mean_raw_obs_processing_ms: 0.07105982352020582\n",
      "  time_since_restore: 960.6514632701874\n",
      "  time_this_iter_s: 6.275107383728027\n",
      "  time_total_s: 960.6514632701874\n",
      "  timers:\n",
      "    learn_throughput: 872.374\n",
      "    learn_time_ms: 4585.19\n",
      "    load_throughput: 1576405.987\n",
      "    load_time_ms: 2.537\n",
      "    sample_throughput: 118.322\n",
      "    sample_time_ms: 33805.913\n",
      "    update_time_ms: 1.813\n",
      "  timestamp: 1671817452\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:13 (running for 00:16:13.83)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         960.651</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 144.348</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:18 (running for 00:16:18.84)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         960.651</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 144.348</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 144.98443563669466\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 298\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0951378345489502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02024080418050289\n",
      "          model: {}\n",
      "          policy_loss: -0.025222396478056908\n",
      "          total_loss: 3.0589122772216797\n",
      "          vf_explained_var: 0.7922682166099548\n",
      "          vf_loss: 3.0800864696502686\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.94444444444444\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09032881258554226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32422283335951163\n",
      "    mean_inference_ms: 0.5220433053331868\n",
      "    mean_raw_obs_processing_ms: 0.07118127468231492\n",
      "  time_since_restore: 966.8978424072266\n",
      "  time_this_iter_s: 6.246379137039185\n",
      "  time_total_s: 966.8978424072266\n",
      "  timers:\n",
      "    learn_throughput: 885.83\n",
      "    learn_time_ms: 4515.538\n",
      "    load_throughput: 1568506.493\n",
      "    load_time_ms: 2.55\n",
      "    sample_throughput: 118.862\n",
      "    sample_time_ms: 33652.458\n",
      "    update_time_ms: 1.766\n",
      "  timestamp: 1671817459\n",
      "  timesteps_since_restore: 428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:24 (running for 00:16:24.13)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         966.898</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> 144.984</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-25\n",
      "  done: false\n",
      "  episode_len_mean: 1592.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 146.18773085577826\n",
      "  episode_reward_min: -23.25915887827422\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 301\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1457833051681519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023061752319335938\n",
      "          model: {}\n",
      "          policy_loss: -0.027643360197544098\n",
      "          total_loss: 3.659602642059326\n",
      "          vf_explained_var: 0.7275583744049072\n",
      "          vf_loss: 3.682633399963379\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.611111111111114\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09051873769819814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.324840001844922\n",
      "    mean_inference_ms: 0.5231028643585358\n",
      "    mean_raw_obs_processing_ms: 0.07136114429351587\n",
      "  time_since_restore: 973.1349663734436\n",
      "  time_this_iter_s: 6.237123966217041\n",
      "  time_total_s: 973.1349663734436\n",
      "  timers:\n",
      "    learn_throughput: 902.907\n",
      "    learn_time_ms: 4430.133\n",
      "    load_throughput: 1590875.696\n",
      "    load_time_ms: 2.514\n",
      "    sample_throughput: 119.486\n",
      "    sample_time_ms: 33476.615\n",
      "    update_time_ms: 1.751\n",
      "  timestamp: 1671817465\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:29 (running for 00:16:29.36)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         973.135</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> 146.188</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -23.2592</td><td style=\"text-align: right;\">            1592.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-31\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 145.6774394291549\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 305\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1541049480438232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01772129535675049\n",
      "          model: {}\n",
      "          policy_loss: -0.03146097809076309\n",
      "          total_loss: 114.98363494873047\n",
      "          vf_explained_var: 0.6160273551940918\n",
      "          vf_loss: 115.01155090332031\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.37777777777778\n",
      "    ram_util_percent: 22.733333333333334\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09077080905333097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3256594545503553\n",
      "    mean_inference_ms: 0.5244998117267435\n",
      "    mean_raw_obs_processing_ms: 0.07160029896121685\n",
      "  time_since_restore: 979.3288073539734\n",
      "  time_this_iter_s: 6.193840980529785\n",
      "  time_total_s: 979.3288073539734\n",
      "  timers:\n",
      "    learn_throughput: 916.899\n",
      "    learn_time_ms: 4362.531\n",
      "    load_throughput: 1594171.093\n",
      "    load_time_ms: 2.509\n",
      "    sample_throughput: 120.022\n",
      "    sample_time_ms: 33327.279\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1671817471\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:34 (running for 00:16:34.60)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         979.329</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> 145.677</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-37\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 146.07965254063345\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 307\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.186484694480896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023413734510540962\n",
      "          model: {}\n",
      "          policy_loss: -0.02492360770702362\n",
      "          total_loss: 2.538810968399048\n",
      "          vf_explained_var: 0.6059569120407104\n",
      "          vf_loss: 2.559051752090454\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.17777777777778\n",
      "    ram_util_percent: 22.73333333333333\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090277159631871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32605274328378103\n",
      "    mean_inference_ms: 0.5251468320386303\n",
      "    mean_raw_obs_processing_ms: 0.07172160965015136\n",
      "  time_since_restore: 985.7205014228821\n",
      "  time_this_iter_s: 6.391694068908691\n",
      "  time_total_s: 985.7205014228821\n",
      "  timers:\n",
      "    learn_throughput: 931.39\n",
      "    learn_time_ms: 4294.657\n",
      "    load_throughput: 1597662.721\n",
      "    load_time_ms: 2.504\n",
      "    sample_throughput: 120.65\n",
      "    sample_time_ms: 33153.754\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1671817477\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:40 (running for 00:16:39.99)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         985.721</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">  146.08</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 146.78379002423978\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 309\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1778619289398193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02176639623939991\n",
      "          model: {}\n",
      "          policy_loss: -0.029138732701539993\n",
      "          total_loss: 3.6291630268096924\n",
      "          vf_explained_var: 0.7502771019935608\n",
      "          vf_loss: 3.6539487838745117\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.93333333333333\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09102685487306911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32645824021680875\n",
      "    mean_inference_ms: 0.5258349804835548\n",
      "    mean_raw_obs_processing_ms: 0.07183959048527493\n",
      "  time_since_restore: 992.0048546791077\n",
      "  time_this_iter_s: 6.284353256225586\n",
      "  time_total_s: 992.0048546791077\n",
      "  timers:\n",
      "    learn_throughput: 931.901\n",
      "    learn_time_ms: 4292.301\n",
      "    load_throughput: 3784789.749\n",
      "    load_time_ms: 1.057\n",
      "    sample_throughput: 631.625\n",
      "    sample_time_ms: 6332.872\n",
      "    update_time_ms: 1.695\n",
      "  timestamp: 1671817484\n",
      "  timesteps_since_restore: 444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:45 (running for 00:16:45.29)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         992.005</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> 146.784</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:50 (running for 00:16:50.29)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         992.005</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> 146.784</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 147.34300151162267\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 311\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1291580200195312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022594213485717773\n",
      "          model: {}\n",
      "          policy_loss: -0.02546831965446472\n",
      "          total_loss: 3.555968761444092\n",
      "          vf_explained_var: 0.7942007184028625\n",
      "          vf_loss: 3.576918363571167\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.511111111111106\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09114311601783523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32687605921838275\n",
      "    mean_inference_ms: 0.526564039707162\n",
      "    mean_raw_obs_processing_ms: 0.07195335912153229\n",
      "  time_since_restore: 998.2766234874725\n",
      "  time_this_iter_s: 6.271768808364868\n",
      "  time_total_s: 998.2766234874725\n",
      "  timers:\n",
      "    learn_throughput: 936.031\n",
      "    learn_time_ms: 4273.36\n",
      "    load_throughput: 3721819.069\n",
      "    load_time_ms: 1.075\n",
      "    sample_throughput: 633.857\n",
      "    sample_time_ms: 6310.575\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1671817490\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:44:55 (running for 00:16:55.62)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         998.277</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> 147.343</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 170.75916797824772\n",
      "  episode_reward_mean: 148.3552731192055\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 315\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.189015507698059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021781712770462036\n",
      "          model: {}\n",
      "          policy_loss: -0.044324345886707306\n",
      "          total_loss: 4.983988285064697\n",
      "          vf_explained_var: 0.7012355327606201\n",
      "          vf_loss: 5.023955345153809\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.27777777777778\n",
      "    ram_util_percent: 22.744444444444444\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09138635774110743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32767187202735665\n",
      "    mean_inference_ms: 0.5279110593543956\n",
      "    mean_raw_obs_processing_ms: 0.07218525392297988\n",
      "  time_since_restore: 1004.4570233821869\n",
      "  time_this_iter_s: 6.1803998947143555\n",
      "  time_total_s: 1004.4570233821869\n",
      "  timers:\n",
      "    learn_throughput: 937.727\n",
      "    learn_time_ms: 4265.634\n",
      "    load_throughput: 3714238.654\n",
      "    load_time_ms: 1.077\n",
      "    sample_throughput: 636.637\n",
      "    sample_time_ms: 6283.019\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1671817496\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:00 (running for 00:17:00.79)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1004.46</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\"> 148.355</td><td style=\"text-align: right;\">             170.759</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 174.62091577386317\n",
      "  episode_reward_mean: 148.89910697797868\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 317\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1468409299850464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022562937811017036\n",
      "          model: {}\n",
      "          policy_loss: -0.03289782255887985\n",
      "          total_loss: 2.770758628845215\n",
      "          vf_explained_var: 0.7525264620780945\n",
      "          vf_loss: 2.7991440296173096\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.96666666666667\n",
      "    ram_util_percent: 22.73333333333333\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09151387479774237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.3280522493740423\n",
      "    mean_inference_ms: 0.5285316295685581\n",
      "    mean_raw_obs_processing_ms: 0.07230284560248164\n",
      "  time_since_restore: 1010.7049379348755\n",
      "  time_this_iter_s: 6.247914552688599\n",
      "  time_total_s: 1010.7049379348755\n",
      "  timers:\n",
      "    learn_throughput: 937.389\n",
      "    learn_time_ms: 4267.174\n",
      "    load_throughput: 3732002.224\n",
      "    load_time_ms: 1.072\n",
      "    sample_throughput: 637.226\n",
      "    sample_time_ms: 6277.209\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1671817503\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:06 (running for 00:17:06.09)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          1010.7</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\"> 148.899</td><td style=\"text-align: right;\">             174.621</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 174.8014450286627\n",
      "  episode_reward_mean: 149.37809416413458\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 319\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1646305322647095\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020159201696515083\n",
      "          model: {}\n",
      "          policy_loss: -0.026339324191212654\n",
      "          total_loss: 2.694669246673584\n",
      "          vf_explained_var: 0.8167151212692261\n",
      "          vf_loss: 2.7169766426086426\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.55555555555556\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09163301252621414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32844254209363477\n",
      "    mean_inference_ms: 0.5291903896418679\n",
      "    mean_raw_obs_processing_ms: 0.07241661089455582\n",
      "  time_since_restore: 1016.9245946407318\n",
      "  time_this_iter_s: 6.219656705856323\n",
      "  time_total_s: 1016.9245946407318\n",
      "  timers:\n",
      "    learn_throughput: 937.757\n",
      "    learn_time_ms: 4265.497\n",
      "    load_throughput: 3692169.014\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 637.205\n",
      "    sample_time_ms: 6277.411\n",
      "    update_time_ms: 1.705\n",
      "  timestamp: 1671817509\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:11 (running for 00:17:11.30)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1016.92</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> 149.378</td><td style=\"text-align: right;\">             174.801</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 1582.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 180.52200362931202\n",
      "  episode_reward_mean: 149.9950180276963\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 321\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1046898365020752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022648053243756294\n",
      "          model: {}\n",
      "          policy_loss: -0.022482506930828094\n",
      "          total_loss: 2.7533960342407227\n",
      "          vf_explained_var: 0.8047184944152832\n",
      "          vf_loss: 2.7713489532470703\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.599999999999994\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09174381092156567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.32884314922374697\n",
      "    mean_inference_ms: 0.5298899893471947\n",
      "    mean_raw_obs_processing_ms: 0.072526161676208\n",
      "  time_since_restore: 1023.1794543266296\n",
      "  time_this_iter_s: 6.254859685897827\n",
      "  time_total_s: 1023.1794543266296\n",
      "  timers:\n",
      "    learn_throughput: 937.911\n",
      "    learn_time_ms: 4264.798\n",
      "    load_throughput: 3692250.27\n",
      "    load_time_ms: 1.083\n",
      "    sample_throughput: 637.499\n",
      "    sample_time_ms: 6274.52\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1671817515\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:16 (running for 00:17:16.57)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1023.18</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> 149.995</td><td style=\"text-align: right;\">             180.522</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:21 (running for 00:17:21.57)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1023.18</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> 149.995</td><td style=\"text-align: right;\">             180.522</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1582.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v3_25b72_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-12-23_17-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 1590.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 180.52200362931202\n",
      "  episode_reward_mean: 152.88540523813523\n",
      "  episode_reward_min: -40.69877418219788\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 325\n",
      "  experiment_id: 80c501b76ccb4ec99d8dc8072c7f6825\n",
      "  hostname: dl\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1084564924240112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01912125013768673\n",
      "          model: {}\n",
      "          policy_loss: -0.03804410621523857\n",
      "          total_loss: 4.275332450866699\n",
      "          vf_explained_var: 0.7723701596260071\n",
      "          vf_loss: 4.30955171585083\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.98\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.55555555555556\n",
      "    ram_util_percent: 22.7\n",
      "  pid: 158805\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09198532311393672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.329595198392306\n",
      "    mean_inference_ms: 0.5311423181262364\n",
      "    mean_raw_obs_processing_ms: 0.0727526634372275\n",
      "  time_since_restore: 1029.4468221664429\n",
      "  time_this_iter_s: 6.267367839813232\n",
      "  time_total_s: 1029.4468221664429\n",
      "  timers:\n",
      "    learn_throughput: 937.504\n",
      "    learn_time_ms: 4266.65\n",
      "    load_throughput: 3745332.292\n",
      "    load_time_ms: 1.068\n",
      "    sample_throughput: 637.537\n",
      "    sample_time_ms: 6274.141\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1671817521\n",
      "  timesteps_since_restore: 468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 25b72_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:45:25,900\tWARNING tune.py:595 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-12-23 17:45:25 (running for 00:17:25.89)<br>Memory usage on this node: 7.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 2.0/2 GPUs, 0.0/15.85 GiB heap, 0.0/7.93 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/oscar/msc/fastdeeprl/16_solving_rl_problems_with_rllib/exercises/walker_v3/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v3_25b72_00000</td><td>RUNNING </td><td>192.168.0.98:158805</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1029.45</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> 152.885</td><td style=\"text-align: right;\">             180.522</td><td style=\"text-align: right;\">            -40.6988</td><td style=\"text-align: right;\">           1590.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:45:26,121\tERROR tune.py:635 -- Trials did not complete: [PPO_BipedalWalker-v3_25b72_00000]\n",
      "2022-12-23 17:45:26,121\tINFO tune.py:639 -- Total run time: 1046.11 seconds (1045.89 seconds for the tuning loop).\n",
      "2022-12-23 17:45:26,122\tWARNING tune.py:643 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f35f90ad490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the experiment runner below\n",
    "from ray import tune\n",
    "\n",
    "# Run the experiment by filling the blank with the required configuration\n",
    "tune.run(\n",
    "    \"PPO\", \n",
    "    config={\n",
    "        \"env\": \"BipedalWalker-v3\", \n",
    "        \"num_gpus\": 2,\n",
    "        \"evaluation_interval\": 100,    # num of training iter between evaluations\n",
    "        \"evaluation_num_episodes\": 100,\n",
    "    }, \n",
    "    local_dir=\"walker_v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0732355-ce71-4b72-98b8-73230fd224f9",
   "metadata": {},
   "source": [
    "If the experiment is running and producing the expected output (like we saw in `CartPole-v1`), then well done! The robot has started learning. \n",
    "\n",
    "How can we see if it's learning? You can move on to the next video, where we will discuss how to visualize the results from a running experiment in real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdeeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c71cf0cfca1a30a715643409ef6f02fe6cf59ad20fff67f74f909906b1eae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
